{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   4%|▍         | 1/23 [00:19<07:15, 19.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 4000.1hsghhdbk103123.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   9%|▊         | 2/23 [00:20<02:55,  8.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed CCL_BuyersGuide.pdf\n",
      "Successfully processed renting-vs-owning.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  22%|██▏       | 5/23 [00:20<00:41,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed RS20530.pdf\n",
      "Successfully processed FHA-Reference-Guide-2023.pdf\n",
      "Successfully processed ort-ss-realestatedictionary.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  30%|███       | 7/23 [00:20<00:20,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed TJC_ebook_fha-homeloan.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  35%|███▍      | 8/23 [00:21<00:17,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed GeneralGlossary.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  39%|███▉      | 9/23 [00:22<00:12,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed GLOSSARY_OF_REAL_ESTATE_TERMS.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  43%|████▎     | 10/23 [00:22<00:11,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed Home_Buyers_Guide.pdf\n",
      "Successfully processed consumer-guide-buying-your-first-home-2024-11-05.pdf\n",
      "Successfully processed FHA_loan_guidelines.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  57%|█████▋    | 13/23 [00:23<00:04,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 2024_Zillow_Rent-vs-Buy.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  61%|██████    | 14/23 [00:23<00:04,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed guide_firsttimehomebuying-2.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  65%|██████▌   | 15/23 [00:24<00:03,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 1507.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  70%|██████▉   | 16/23 [00:25<00:04,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed HL_Buyers_Guide_FINAL_March2019.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  78%|███████▊  | 18/23 [00:25<00:02,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed NAHREP-Glossary-of-Real-Estate-Industry-Terms.pdf\n",
      "Successfully processed naiop-2024-terms-and-definitions.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  87%|████████▋ | 20/23 [00:26<00:00,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed First-TIme-HomeBuyer-Guide.pdf\n",
      "Successfully processed realestateglossary.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  91%|█████████▏| 21/23 [00:26<00:00,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed home-buyers-guide-1.pdf\n",
      "Successfully processed renting-vs-buying-study-press-release.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 23/23 [00:26<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed First-TIme-HomeBuyer-Guide-2.pdf\n",
      "Processed 23 PDF files and saved to extracted_pdf_texts.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text() + \"\\n\\n\"\n",
    "        return text\n",
    "\n",
    "def process_pdfs(directory):\n",
    "    \"\"\"Process all PDFs in the given directory.\"\"\"\n",
    "    results = []\n",
    "    pdf_files = [f for f in os.listdir(directory) if f.lower().endswith('.pdf')]\n",
    "    \n",
    "    for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        file_path = os.path.join(directory, pdf_file)\n",
    "        try:\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "            results.append({\n",
    "                'filename': pdf_file,\n",
    "                'text': text,\n",
    "                'size': os.path.getsize(file_path)\n",
    "            })\n",
    "            print(f\"Successfully processed {pdf_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_file}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir = \"data\"\n",
    "    results_df = process_pdfs(data_dir)\n",
    "    results_df.to_pickle(\"extracted_pdf_texts.pkl\")\n",
    "    print(f\"Processed {len(results_df)} PDF files and saved to extracted_pdf_texts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from extracted_pdf_texts.pkl...\n",
      "\n",
      "Dataset contains 23 documents\n",
      "Columns: ['filename', 'text', 'size']\n",
      "\n",
      "Documents with empty text: 0\n",
      "\n",
      "Text length statistics:\n",
      "count    2.300000e+01\n",
      "mean     2.255588e+05\n",
      "std      8.347313e+05\n",
      "min      2.083000e+03\n",
      "25%      2.211300e+04\n",
      "50%      3.974200e+04\n",
      "75%      8.795000e+04\n",
      "max      4.047940e+06\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "3 random samples (first 500 chars):\n",
      "\n",
      "Sample 1 from 'CCL_BuyersGuide.pdf':\n",
      "--------------------------------------------------------------------------------\n",
      "Home \n",
      "Buyer’s \n",
      "Guide\n",
      "\n",
      "1\n",
      "\n",
      "2About Us\n",
      "Corcoran Classic Living is a top- performing \n",
      "residential and commercial real estate firm \n",
      "serving greater Athens, GA. Our agents \n",
      "are socially-minded and deeply entrenched \n",
      "in their communities, backed by a top-\n",
      "notch support staff, global connections, \n",
      "and innovative technologies that ensure a \n",
      "seamless experience. Our mission is to treat \n",
      "our customers and clients as the lifeblood of \n",
      "our business, keep their satisfaction our top \n",
      "priority, and strive for a ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 2 from 'HL_Buyers_Guide_FINAL_March2019.pdf':\n",
      "--------------------------------------------------------------------------------\n",
      "OPEN\n",
      "???\n",
      "FOR\n",
      "SALEA Step-by-Step\n",
      "Guide to \n",
      "Home Buying\n",
      "\n",
      "\n",
      "© Copyright 2019 NATIONAL ASSOCIATION OF REALTORS®HouseLogic.com\n",
      "2A STEP-BY-STEP GUIDE TO HOME BUYING\n",
      "Table of Contents\n",
      "3\n",
      "6\n",
      "9\n",
      "14\n",
      "16\n",
      "21\n",
      "24\n",
      "29\n",
      "33\n",
      "38\n",
      "42\n",
      "47\n",
      "50\n",
      "\n",
      "HouseLogic.com\n",
      "3THE EVERYTHING GUIDE TO\n",
      "Buying Your First Home\n",
      "How to find exactly what you want, and how to work with the experts who’ll help you get it.\n",
      "It’s a big move, literally and figuratively. Buying a house requires a serious amount of money and time. The \n",
      "journey isn’t always e...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 3 from 'renting-vs-buying-study-press-release.pdf':\n",
      "--------------------------------------------------------------------------------\n",
      "Renting\n",
      "is\n",
      "Now\n",
      "Cheaper\n",
      "Than\n",
      "Buying\n",
      "in\n",
      "All\n",
      "50\n",
      "of\n",
      "the\n",
      "Largest\n",
      "U.S.\n",
      "Metros\n",
      "Metros\n",
      "in\n",
      "the\n",
      "West\n",
      "have\n",
      "the\n",
      "biggest\n",
      "differences\n",
      "between\n",
      "renting\n",
      "and\n",
      "buying\n",
      "costs\n",
      "NEW\n",
      "YORK\n",
      "-\n",
      "April\n",
      "29,\n",
      "2024\n",
      "-\n",
      "Renting\n",
      "a\n",
      "typical\n",
      "home\n",
      "is\n",
      "now\n",
      "cheaper\n",
      "than\n",
      "buying\n",
      "one\n",
      "in\n",
      "all\n",
      "50\n",
      "of\n",
      "the\n",
      "largest\n",
      "U.S.\n",
      "metros\n",
      "in\n",
      "2024,\n",
      "according\n",
      "to\n",
      "a\n",
      "new\n",
      "Bankrate\n",
      "study .\n",
      "Bankrate\n",
      "analyzed\n",
      "typical\n",
      "monthly\n",
      "mortgage\n",
      "payments\n",
      "and\n",
      "typical\n",
      "monthly\n",
      "rent\n",
      "for\n",
      "all\n",
      "homes\n",
      "across\n",
      "all\n",
      "50\n",
      "of\n",
      "the\n",
      "largest\n",
      "metro\n",
      "statistical\n",
      "areas\n",
      "(MSAs)\n",
      "in\n",
      "the\n",
      "United\n",
      "States\n",
      "to\n",
      "compare\n",
      "...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Checking for potential issues:\n",
      "Documents with no spaces (potential OCR issues): 0\n",
      "Documents with non-ASCII characters: 23\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "def check_extracted_data(pkl_path, num_samples=3, sample_length=500):\n",
    "    \"\"\"\n",
    "    Examine the extracted PDF data to check its quality.\n",
    "    \n",
    "    Args:\n",
    "        pkl_path: Path to the pickle file with extracted text\n",
    "        num_samples: Number of random samples to display\n",
    "        sample_length: Number of characters to display from each sample\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    if not os.path.exists(pkl_path):\n",
    "        print(f\"Error: {pkl_path} does not exist.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Loading data from {pkl_path}...\")\n",
    "    df = pd.read_pickle(pkl_path)\n",
    "    \n",
    "    # Print basic information\n",
    "    print(f\"\\nDataset contains {len(df)} documents\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Check for empty text\n",
    "    empty_texts = df[df['text'].str.strip() == ''].shape[0]\n",
    "    print(f\"\\nDocuments with empty text: {empty_texts}\")\n",
    "    \n",
    "    # Check text lengths\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    print(f\"\\nText length statistics:\")\n",
    "    print(df['text_length'].describe())\n",
    "    \n",
    "    # Show some random samples\n",
    "    print(f\"\\n{num_samples} random samples (first {sample_length} chars):\")\n",
    "    sample_indices = random.sample(range(len(df)), min(num_samples, len(df)))\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        doc = df.iloc[idx]\n",
    "        print(f\"\\nSample {i+1} from '{doc['filename']}':\")\n",
    "        print(\"-\" * 80)\n",
    "        print(doc['text'][:sample_length] + \"...\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Check for common issues\n",
    "    print(\"\\nChecking for potential issues:\")\n",
    "    \n",
    "    # Missing spaces between words (possible OCR issue)\n",
    "    no_spaces = df[~df['text'].str.contains(' ', regex=False)].shape[0]\n",
    "    print(f\"Documents with no spaces (potential OCR issues): {no_spaces}\")\n",
    "    \n",
    "    # Unusual characters (possible encoding issues)\n",
    "    unusual_chars = df[df['text'].str.contains('[^\\x00-\\x7F]', regex=True)].shape[0]\n",
    "    print(f\"Documents with non-ASCII characters: {unusual_chars}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_extracted_data(\"extracted_pdf_texts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from extracted_pdf_texts.pkl...\n",
      "Cleaning text data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 58.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking documents into smaller pieces...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking documents:   0%|          | 0/23 [06:46<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text for NLP tasks.\"\"\"\n",
    "    # Replace common non-ASCII characters \n",
    "    text = text.replace('–', '-').replace('—', '-').replace(''', \"'\").replace(''', \"'\")\n",
    "    text = text.replace('\"', '\"').replace('\"', '\"').replace('…', '...')\n",
    "    \n",
    "    # Normalize unicode characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove excessive newlines\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    # Clean up page numbers and headers/footers (common in PDFs)\n",
    "    text = re.sub(r'\\n\\s*\\d+\\s*\\n', '\\n', text)  # Standalone page numbers\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=100):\n",
    "    \"\"\"Split text into overlapping chunks of approximately chunk_size characters.\"\"\"\n",
    "    if len(text) <= chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    # Add timeout protection\n",
    "    max_iterations = (len(text) // (chunk_size - overlap)) * 2  # Generous upper bound\n",
    "    iteration = 0\n",
    "    \n",
    "    while start < len(text) and iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        \n",
    "        # Limit the search window for breaking points to improve performance\n",
    "        search_start = max(start, end - 200)\n",
    "        \n",
    "        # Try to find a good breaking point (end of sentence or paragraph)\n",
    "        if end < len(text):\n",
    "            # Look for paragraph break first (limit search range)\n",
    "            paragraph_break = text.rfind('\\n\\n', search_start, end)\n",
    "            if paragraph_break != -1:\n",
    "                end = paragraph_break\n",
    "            else:\n",
    "                # Look for sentence break (use a simpler, faster approach)\n",
    "                for marker in ['. ', '! ', '? ']:\n",
    "                    sentence_break = text.rfind(marker, search_start, end)\n",
    "                    if sentence_break != -1:\n",
    "                        end = sentence_break + 2  # +2 to include the punctuation and space\n",
    "                        break\n",
    "        \n",
    "        # Make sure we're making progress\n",
    "        if end <= start:\n",
    "            end = start + chunk_size  # Force progress if no break point found\n",
    "            \n",
    "        chunks.append(text[start:end].strip())\n",
    "        start = end - overlap  # Create overlap between chunks\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_text_data(pkl_path, output_path=None, chunk_size=1500):\n",
    "    \"\"\"Clean, preprocess and chunk text data from PDFs.\"\"\"\n",
    "    # Load the data\n",
    "    print(f\"Loading data from {pkl_path}...\")\n",
    "    df = pd.read_pickle(pkl_path)\n",
    "    \n",
    "    # Clean texts\n",
    "    print(\"Cleaning text data...\")\n",
    "    df['cleaned_text'] = df['text'].progress_apply(clean_text)\n",
    "    \n",
    "    # Chunk texts\n",
    "    print(\"Chunking documents into smaller pieces...\")\n",
    "    all_chunks = []\n",
    "    \n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Chunking documents\"):\n",
    "        try:\n",
    "            # Skip extremely large docs or process them differently\n",
    "            if len(row['cleaned_text']) > 1_000_000:  # 1 million chars\n",
    "                print(f\"⚠️ Large document detected: {row['filename']} ({len(row['cleaned_text'])} chars)\")\n",
    "                # Process large documents in a simpler way (just divide by size)\n",
    "                simple_chunks = [row['cleaned_text'][j:j+chunk_size] \n",
    "                               for j in range(0, len(row['cleaned_text']), chunk_size)]\n",
    "                for j, chunk in enumerate(simple_chunks):\n",
    "                    all_chunks.append({\n",
    "                        'source_file': row['filename'],\n",
    "                        'chunk_id': f\"{row['filename']}_simple_{j}\",\n",
    "                        'text': chunk.strip()\n",
    "                    })\n",
    "                continue\n",
    "            \n",
    "            # Regular chunking for normal sized documents\n",
    "            chunks = chunk_text(row['cleaned_text'], chunk_size=chunk_size)\n",
    "            for j, chunk in enumerate(chunks):\n",
    "                all_chunks.append({\n",
    "                    'source_file': row['filename'],\n",
    "                    'chunk_id': f\"{row['filename']}_{j}\",\n",
    "                    'text': chunk\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['filename']}: {e}\")\n",
    "    \n",
    "    chunks_df = pd.DataFrame(all_chunks)\n",
    "    print(f\"Created {len(chunks_df)} chunks from {len(df)} documents\")\n",
    "    \n",
    "    # Save the processed data\n",
    "    if output_path:\n",
    "        chunks_df.to_pickle(output_path)\n",
    "        print(f\"Saved processed chunks to {output_path}\")\n",
    "    \n",
    "    return chunks_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Add tqdm to pandas operations\n",
    "    tqdm.pandas()\n",
    "    \n",
    "    # Process the data\n",
    "    processed_df = process_text_data(\"extracted_pdf_texts.pkl\", \"processed_chunks.pkl\")\n",
    "    \n",
    "    # Display some statistics\n",
    "    print(\"\\nChunk length statistics:\")\n",
    "    processed_df['text_length'] = processed_df['text'].str.len()\n",
    "    print(processed_df['text_length'].describe())\n",
    "    \n",
    "    # Print a few sample chunks\n",
    "    print(\"\\nSample chunks:\")\n",
    "    for i in range(min(3, len(processed_df))):\n",
    "        print(f\"\\nChunk {i+1} from {processed_df.iloc[i]['source_file']}:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(processed_df.iloc[i]['text'][:300] + \"...\" if len(processed_df.iloc[i]['text']) > 300 else processed_df.iloc[i]['text'])\n",
    "        print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
