{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   4%|▍         | 1/23 [00:19<07:15, 19.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 4000.1hsghhdbk103123.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   9%|▊         | 2/23 [00:20<02:55,  8.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed CCL_BuyersGuide.pdf\n",
      "Successfully processed renting-vs-owning.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  22%|██▏       | 5/23 [00:20<00:41,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed RS20530.pdf\n",
      "Successfully processed FHA-Reference-Guide-2023.pdf\n",
      "Successfully processed ort-ss-realestatedictionary.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  30%|███       | 7/23 [00:20<00:20,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed TJC_ebook_fha-homeloan.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  35%|███▍      | 8/23 [00:21<00:17,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed GeneralGlossary.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  39%|███▉      | 9/23 [00:22<00:12,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed GLOSSARY_OF_REAL_ESTATE_TERMS.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  43%|████▎     | 10/23 [00:22<00:11,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed Home_Buyers_Guide.pdf\n",
      "Successfully processed consumer-guide-buying-your-first-home-2024-11-05.pdf\n",
      "Successfully processed FHA_loan_guidelines.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  57%|█████▋    | 13/23 [00:23<00:04,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 2024_Zillow_Rent-vs-Buy.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  61%|██████    | 14/23 [00:23<00:04,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed guide_firsttimehomebuying-2.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  65%|██████▌   | 15/23 [00:24<00:03,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 1507.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  70%|██████▉   | 16/23 [00:25<00:04,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed HL_Buyers_Guide_FINAL_March2019.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  78%|███████▊  | 18/23 [00:25<00:02,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed NAHREP-Glossary-of-Real-Estate-Industry-Terms.pdf\n",
      "Successfully processed naiop-2024-terms-and-definitions.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  87%|████████▋ | 20/23 [00:26<00:00,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed First-TIme-HomeBuyer-Guide.pdf\n",
      "Successfully processed realestateglossary.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  91%|█████████▏| 21/23 [00:26<00:00,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed home-buyers-guide-1.pdf\n",
      "Successfully processed renting-vs-buying-study-press-release.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 23/23 [00:26<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed First-TIme-HomeBuyer-Guide-2.pdf\n",
      "Processed 23 PDF files and saved to extracted_pdf_texts.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text() + \"\\n\\n\"\n",
    "        return text\n",
    "\n",
    "def process_pdfs(directory):\n",
    "    \"\"\"Process all PDFs in the given directory.\"\"\"\n",
    "    results = []\n",
    "    pdf_files = [f for f in os.listdir(directory) if f.lower().endswith('.pdf')]\n",
    "    \n",
    "    for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        file_path = os.path.join(directory, pdf_file)\n",
    "        try:\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "            results.append({\n",
    "                'filename': pdf_file,\n",
    "                'text': text,\n",
    "                'size': os.path.getsize(file_path)\n",
    "            })\n",
    "            print(f\"Successfully processed {pdf_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_file}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir = \"data\"\n",
    "    results_df = process_pdfs(data_dir)\n",
    "    results_df.to_pickle(\"extracted_pdf_texts.pkl\")\n",
    "    print(f\"Processed {len(results_df)} PDF files and saved to extracted_pdf_texts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from extracted_pdf_texts.pkl...\n",
      "\n",
      "Dataset contains 23 documents\n",
      "Columns: ['filename', 'text', 'size']\n",
      "\n",
      "Documents with empty text: 0\n",
      "\n",
      "Text length statistics:\n",
      "count    2.300000e+01\n",
      "mean     2.255588e+05\n",
      "std      8.347313e+05\n",
      "min      2.083000e+03\n",
      "25%      2.211300e+04\n",
      "50%      3.974200e+04\n",
      "75%      8.795000e+04\n",
      "max      4.047940e+06\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "3 random samples (first 500 chars):\n",
      "\n",
      "Sample 1 from 'CCL_BuyersGuide.pdf':\n",
      "--------------------------------------------------------------------------------\n",
      "Home \n",
      "Buyer’s \n",
      "Guide\n",
      "\n",
      "1\n",
      "\n",
      "2About Us\n",
      "Corcoran Classic Living is a top- performing \n",
      "residential and commercial real estate firm \n",
      "serving greater Athens, GA. Our agents \n",
      "are socially-minded and deeply entrenched \n",
      "in their communities, backed by a top-\n",
      "notch support staff, global connections, \n",
      "and innovative technologies that ensure a \n",
      "seamless experience. Our mission is to treat \n",
      "our customers and clients as the lifeblood of \n",
      "our business, keep their satisfaction our top \n",
      "priority, and strive for a ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 2 from 'HL_Buyers_Guide_FINAL_March2019.pdf':\n",
      "--------------------------------------------------------------------------------\n",
      "OPEN\n",
      "???\n",
      "FOR\n",
      "SALEA Step-by-Step\n",
      "Guide to \n",
      "Home Buying\n",
      "\n",
      "\n",
      "© Copyright 2019 NATIONAL ASSOCIATION OF REALTORS®HouseLogic.com\n",
      "2A STEP-BY-STEP GUIDE TO HOME BUYING\n",
      "Table of Contents\n",
      "3\n",
      "6\n",
      "9\n",
      "14\n",
      "16\n",
      "21\n",
      "24\n",
      "29\n",
      "33\n",
      "38\n",
      "42\n",
      "47\n",
      "50\n",
      "\n",
      "HouseLogic.com\n",
      "3THE EVERYTHING GUIDE TO\n",
      "Buying Your First Home\n",
      "How to find exactly what you want, and how to work with the experts who’ll help you get it.\n",
      "It’s a big move, literally and figuratively. Buying a house requires a serious amount of money and time. The \n",
      "journey isn’t always e...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 3 from 'renting-vs-buying-study-press-release.pdf':\n",
      "--------------------------------------------------------------------------------\n",
      "Renting\n",
      "is\n",
      "Now\n",
      "Cheaper\n",
      "Than\n",
      "Buying\n",
      "in\n",
      "All\n",
      "50\n",
      "of\n",
      "the\n",
      "Largest\n",
      "U.S.\n",
      "Metros\n",
      "Metros\n",
      "in\n",
      "the\n",
      "West\n",
      "have\n",
      "the\n",
      "biggest\n",
      "differences\n",
      "between\n",
      "renting\n",
      "and\n",
      "buying\n",
      "costs\n",
      "NEW\n",
      "YORK\n",
      "-\n",
      "April\n",
      "29,\n",
      "2024\n",
      "-\n",
      "Renting\n",
      "a\n",
      "typical\n",
      "home\n",
      "is\n",
      "now\n",
      "cheaper\n",
      "than\n",
      "buying\n",
      "one\n",
      "in\n",
      "all\n",
      "50\n",
      "of\n",
      "the\n",
      "largest\n",
      "U.S.\n",
      "metros\n",
      "in\n",
      "2024,\n",
      "according\n",
      "to\n",
      "a\n",
      "new\n",
      "Bankrate\n",
      "study .\n",
      "Bankrate\n",
      "analyzed\n",
      "typical\n",
      "monthly\n",
      "mortgage\n",
      "payments\n",
      "and\n",
      "typical\n",
      "monthly\n",
      "rent\n",
      "for\n",
      "all\n",
      "homes\n",
      "across\n",
      "all\n",
      "50\n",
      "of\n",
      "the\n",
      "largest\n",
      "metro\n",
      "statistical\n",
      "areas\n",
      "(MSAs)\n",
      "in\n",
      "the\n",
      "United\n",
      "States\n",
      "to\n",
      "compare\n",
      "...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Checking for potential issues:\n",
      "Documents with no spaces (potential OCR issues): 0\n",
      "Documents with non-ASCII characters: 23\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "def check_extracted_data(pkl_path, num_samples=3, sample_length=500):\n",
    "    \"\"\"\n",
    "    Examine the extracted PDF data to check its quality.\n",
    "    \n",
    "    Args:\n",
    "        pkl_path: Path to the pickle file with extracted text\n",
    "        num_samples: Number of random samples to display\n",
    "        sample_length: Number of characters to display from each sample\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    if not os.path.exists(pkl_path):\n",
    "        print(f\"Error: {pkl_path} does not exist.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Loading data from {pkl_path}...\")\n",
    "    df = pd.read_pickle(pkl_path)\n",
    "    \n",
    "    # Print basic information\n",
    "    print(f\"\\nDataset contains {len(df)} documents\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Check for empty text\n",
    "    empty_texts = df[df['text'].str.strip() == ''].shape[0]\n",
    "    print(f\"\\nDocuments with empty text: {empty_texts}\")\n",
    "    \n",
    "    # Check text lengths\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    print(f\"\\nText length statistics:\")\n",
    "    print(df['text_length'].describe())\n",
    "    \n",
    "    # Show some random samples\n",
    "    print(f\"\\n{num_samples} random samples (first {sample_length} chars):\")\n",
    "    sample_indices = random.sample(range(len(df)), min(num_samples, len(df)))\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        doc = df.iloc[idx]\n",
    "        print(f\"\\nSample {i+1} from '{doc['filename']}':\")\n",
    "        print(\"-\" * 80)\n",
    "        print(doc['text'][:sample_length] + \"...\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Check for common issues\n",
    "    print(\"\\nChecking for potential issues:\")\n",
    "    \n",
    "    # Missing spaces between words (possible OCR issue)\n",
    "    no_spaces = df[~df['text'].str.contains(' ', regex=False)].shape[0]\n",
    "    print(f\"Documents with no spaces (potential OCR issues): {no_spaces}\")\n",
    "    \n",
    "    # Unusual characters (possible encoding issues)\n",
    "    unusual_chars = df[df['text'].str.contains('[^\\x00-\\x7F]', regex=True)].shape[0]\n",
    "    print(f\"Documents with non-ASCII characters: {unusual_chars}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_extracted_data(\"extracted_pdf_texts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from extracted_pdf_texts.pkl...\n",
      "Cleaning text data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 116.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking documents into smaller pieces...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking documents: 100%|██████████| 23/23 [00:00<00:00, 1913.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Large document detected: 4000.1hsghhdbk103123.pdf (3893681 chars)\n",
      "Created 4138 chunks from 23 documents\n",
      "Saved processed chunks to processed_chunks.pkl\n",
      "\n",
      "Chunk length statistics:\n",
      "count    4138.000000\n",
      "mean     1241.202272\n",
      "std       527.984130\n",
      "min        99.000000\n",
      "25%      1428.000000\n",
      "50%      1499.000000\n",
      "75%      1500.000000\n",
      "max      1500.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Sample chunks:\n",
      "\n",
      "Chunk 1 from 4000.1hsghhdbk103123.pdf:\n",
      "--------------------------------------------------------------------------------\n",
      "Special Attention of: Transmittal: Handbook 4000.1 All FHA -Approved Mortgagees Issued: October 31, 2023 All Direct Endorsement Underwriters Effective Date: April 29, 2024 All Eligible Submission Sources for Condominium Project Approvals All FHA Roster Appraisers All FHA -Approved 203(k) Consultants...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Chunk 2 from 4000.1hsghhdbk103123.pdf:\n",
      "--------------------------------------------------------------------------------\n",
      "nt of the Appraisal II.D.3 Acceptable Appraisal Reporting Forms and Protocols 2 3. Implementation : These updates do not impact previously announced effective dates for Handbook 4000.1. Below is a list of effective dates for the Handbook changes: Changes identified in previously published Mortgagee ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Chunk 3 from 4000.1hsghhdbk103123.pdf:\n",
      "--------------------------------------------------------------------------------\n",
      "led or superseded. All superseded or canceled policy documents will co ntinue to be available for informational purposes only on HUDs website. Policy documents that have been superseded in full by the Handbook can always be found on HUDs Client Information Policy Systems (HUDCLIPS) web pages, access...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text for NLP tasks.\"\"\"\n",
    "    # Replace common non-ASCII characters \n",
    "    text = text.replace('–', '-').replace('—', '-').replace(''', \"'\").replace(''', \"'\")\n",
    "    text = text.replace('\"', '\"').replace('\"', '\"').replace('…', '...')\n",
    "    \n",
    "    # Normalize unicode characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove excessive newlines\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    # Clean up page numbers and headers/footers (common in PDFs)\n",
    "    text = re.sub(r'\\n\\s*\\d+\\s*\\n', '\\n', text)  # Standalone page numbers\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=100):\n",
    "    \"\"\"Split text into overlapping chunks of approximately chunk_size characters.\"\"\"\n",
    "    if len(text) <= chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    # Add timeout protection\n",
    "    max_iterations = (len(text) // (chunk_size - overlap)) * 2  # Generous upper bound\n",
    "    iteration = 0\n",
    "    \n",
    "    while start < len(text) and iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        \n",
    "        # Limit the search window for breaking points to improve performance\n",
    "        search_start = max(start, end - 200)\n",
    "        \n",
    "        # Try to find a good breaking point (end of sentence or paragraph)\n",
    "        if end < len(text):\n",
    "            # Look for paragraph break first (limit search range)\n",
    "            paragraph_break = text.rfind('\\n\\n', search_start, end)\n",
    "            if paragraph_break != -1:\n",
    "                end = paragraph_break\n",
    "            else:\n",
    "                # Look for sentence break (use a simpler, faster approach)\n",
    "                for marker in ['. ', '! ', '? ']:\n",
    "                    sentence_break = text.rfind(marker, search_start, end)\n",
    "                    if sentence_break != -1:\n",
    "                        end = sentence_break + 2  # +2 to include the punctuation and space\n",
    "                        break\n",
    "        \n",
    "        # Make sure we're making progress\n",
    "        if end <= start:\n",
    "            end = start + chunk_size  # Force progress if no break point found\n",
    "            \n",
    "        chunks.append(text[start:end].strip())\n",
    "        start = end - overlap  # Create overlap between chunks\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_text_data(pkl_path, output_path=None, chunk_size=1500):\n",
    "    \"\"\"Clean, preprocess and chunk text data from PDFs.\"\"\"\n",
    "    # Load the data\n",
    "    print(f\"Loading data from {pkl_path}...\")\n",
    "    df = pd.read_pickle(pkl_path)\n",
    "    \n",
    "    # Clean texts\n",
    "    print(\"Cleaning text data...\")\n",
    "    df['cleaned_text'] = df['text'].progress_apply(clean_text)\n",
    "    \n",
    "    # Chunk texts\n",
    "    print(\"Chunking documents into smaller pieces...\")\n",
    "    all_chunks = []\n",
    "    \n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Chunking documents\"):\n",
    "        try:\n",
    "            # Skip extremely large docs or process them differently\n",
    "            if len(row['cleaned_text']) > 1_000_000:  # 1 million chars\n",
    "                print(f\"⚠️ Large document detected: {row['filename']} ({len(row['cleaned_text'])} chars)\")\n",
    "                # Process large documents in a simpler way (just divide by size)\n",
    "                simple_chunks = [row['cleaned_text'][j:j+chunk_size] \n",
    "                               for j in range(0, len(row['cleaned_text']), chunk_size)]\n",
    "                for j, chunk in enumerate(simple_chunks):\n",
    "                    all_chunks.append({\n",
    "                        'source_file': row['filename'],\n",
    "                        'chunk_id': f\"{row['filename']}_simple_{j}\",\n",
    "                        'text': chunk.strip()\n",
    "                    })\n",
    "                continue\n",
    "            \n",
    "            # Regular chunking for normal sized documents\n",
    "            chunks = chunk_text(row['cleaned_text'], chunk_size=chunk_size)\n",
    "            for j, chunk in enumerate(chunks):\n",
    "                all_chunks.append({\n",
    "                    'source_file': row['filename'],\n",
    "                    'chunk_id': f\"{row['filename']}_{j}\",\n",
    "                    'text': chunk\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['filename']}: {e}\")\n",
    "    \n",
    "    chunks_df = pd.DataFrame(all_chunks)\n",
    "    print(f\"Created {len(chunks_df)} chunks from {len(df)} documents\")\n",
    "    \n",
    "    # Save the processed data\n",
    "    if output_path:\n",
    "        chunks_df.to_pickle(output_path)\n",
    "        print(f\"Saved processed chunks to {output_path}\")\n",
    "    \n",
    "    return chunks_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Add tqdm to pandas operations\n",
    "    tqdm.pandas()\n",
    "    \n",
    "    # Process the data\n",
    "    processed_df = process_text_data(\"extracted_pdf_texts.pkl\", \"processed_chunks.pkl\")\n",
    "    \n",
    "    # Display some statistics\n",
    "    print(\"\\nChunk length statistics:\")\n",
    "    processed_df['text_length'] = processed_df['text'].str.len()\n",
    "    print(processed_df['text_length'].describe())\n",
    "    \n",
    "    # Print a few sample chunks\n",
    "    print(\"\\nSample chunks:\")\n",
    "    for i in range(min(3, len(processed_df))):\n",
    "        print(f\"\\nChunk {i+1} from {processed_df.iloc[i]['source_file']}:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(processed_df.iloc[i]['text'][:300] + \"...\" if len(processed_df.iloc[i]['text']) > 300 else processed_df.iloc[i]['text'])\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4ff9f0a5f54aed8168e61601b470ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820e4738472c49009f2b3e8483544a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e3fc98123a4b9b9c1a6f256ea08ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa899093f574f97a6594d968e40030d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885b4fff858c4048bbbd4664c5b468ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7821b761525c438b91e74d821c480120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9c69683ae84a0596041c4f720cc448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1be03e2df04832849a571c92827ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217de2bca1b147e0afa7f4ca431e4095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0835c269a9ff44038459a4df778a2387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af686cc0a2014b5faddf04dbcfa8d094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chunks from processed_chunks.pkl...\n",
      "Loaded 4138 chunks\n",
      "Creating embeddings using all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9981337d0bdf4e35ba085c4f5e99d07d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating embeddings:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings to embeddings.pkl\n",
      "Building FAISS index...\n",
      "Built index with 4138 vectors\n",
      "\n",
      "Testing retrieval with sample queries:\n",
      "\n",
      "Query: What is an FHA loan?\n",
      "Result 1 from FHA-Reference-Guide-2023.pdf:\n",
      "Distance: 0.6199\n",
      "Text snippet: alify for than conventional mortgages. FHA loans are insured by the Federal Housing Administration. FHA does not lend money or issue cre dit, so the f...\n",
      "Result 2 from RS20530.pdf:\n",
      "Distance: 0.6410\n",
      "Text snippet: ....... ................................ ................................ ......................... 19 FHA-Insured Home Loans: An Overview Congression...\n",
      "\n",
      "Query: Should I rent or buy a house?\n",
      "Result 1 from CCL_BuyersGuide.pdf:\n",
      "Distance: 0.7296\n",
      "Text snippet: pected to rise 10 to 15 percent over the next decade, creating a continued high demand for housing. EQUITY Money paid for rent is money that youll nev...\n",
      "Result 2 from HL_Buyers_Guide_FINAL_March2019.pdf:\n",
      "Distance: 0.7623\n",
      "Text snippet: heres one reason you feel speaks especially to you, circle it with some hearts. Yknow. If you want. Any other reasons?Because Ive always wanted to own...\n",
      "\n",
      "Query: What are closing costs?\n",
      "Result 1 from Home_Buyers_Guide.pdf:\n",
      "Distance: 0.7689\n",
      "Text snippet: rices of similar properties. 8A.Home .Buyers .Glossary H O M E B U Y E R S G U I D E R E A LT O R . C O M | T O P P R O D U C E R | S T E P - B Y- S T...\n",
      "Result 2 from realestateglossary.pdf:\n",
      "Distance: 0.8004\n",
      "Text snippet: torneys. Closing Costs: The upfront fees charged in connection with a mortgage loan trans - action. Money paid by a buyer (and/or seller or other thir...\n",
      "\n",
      "Query: How do I get pre-approved for a mortgage?\n",
      "Result 1 from Home_Buyers_Guide.pdf:\n",
      "Distance: 0.6360\n",
      "Text snippet: ion the lender will provide you with a document that details how much you can borrow to buy a home. You may want to consider looking online to see wha...\n",
      "Result 2 from HL_Buyers_Guide_FINAL_March2019.pdf:\n",
      "Distance: 0.6367\n",
      "Text snippet: an effectively size up your loan options and decide which lender is best for you - and your future. (If you need help navigating the details, the Cons...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# First, install required packages\n",
    "# !pip install sentence-transformers faiss-cpu\n",
    "\n",
    "class RealEstateRAG:\n",
    "    def __init__(self, chunks_path=\"processed_chunks.pkl\", embedding_model=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system with document chunks and embedding model.\n",
    "        \n",
    "        Args:\n",
    "            chunks_path: Path to the pickle file with processed chunks\n",
    "            embedding_model: SentenceTransformer model to use for embeddings\n",
    "        \"\"\"\n",
    "        self.chunks_path = chunks_path\n",
    "        self.embedding_model_name = embedding_model\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.index = None\n",
    "        self.chunks_df = None\n",
    "        self.embeddings = None\n",
    "        \n",
    "        # Load chunks if they exist\n",
    "        if os.path.exists(chunks_path):\n",
    "            self.load_chunks()\n",
    "    \n",
    "    def load_chunks(self):\n",
    "        \"\"\"Load document chunks from pickle file.\"\"\"\n",
    "        print(f\"Loading chunks from {self.chunks_path}...\")\n",
    "        self.chunks_df = pd.read_pickle(self.chunks_path)\n",
    "        print(f\"Loaded {len(self.chunks_df)} chunks\")\n",
    "    \n",
    "    def create_embeddings(self, save_path=\"embeddings.pkl\"):\n",
    "        \"\"\"Create embeddings for all chunks.\"\"\"\n",
    "        if self.chunks_df is None:\n",
    "            self.load_chunks()\n",
    "        \n",
    "        print(f\"Creating embeddings using {self.embedding_model_name}...\")\n",
    "        texts = self.chunks_df['text'].tolist()\n",
    "        \n",
    "        # Embed in batches to avoid memory issues\n",
    "        batch_size = 64\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Creating embeddings\"):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            batch_embeddings = self.embedding_model.encode(batch_texts)\n",
    "            embeddings.append(batch_embeddings)\n",
    "        \n",
    "        self.embeddings = np.vstack(embeddings)\n",
    "        \n",
    "        # Save embeddings\n",
    "        if save_path:\n",
    "            with open(save_path, 'wb') as f:\n",
    "                pickle.dump(self.embeddings, f)\n",
    "            print(f\"Saved embeddings to {save_path}\")\n",
    "        \n",
    "        return self.embeddings\n",
    "    \n",
    "    def load_embeddings(self, embeddings_path=\"embeddings.pkl\"):\n",
    "        \"\"\"Load pre-computed embeddings.\"\"\"\n",
    "        if os.path.exists(embeddings_path):\n",
    "            print(f\"Loading embeddings from {embeddings_path}...\")\n",
    "            with open(embeddings_path, 'rb') as f:\n",
    "                self.embeddings = pickle.load(f)\n",
    "            print(f\"Loaded embeddings with shape {self.embeddings.shape}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Embeddings file {embeddings_path} not found.\")\n",
    "            return False\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"Build a FAISS index for fast similarity search.\"\"\"\n",
    "        if self.embeddings is None:\n",
    "            if not self.load_embeddings():\n",
    "                self.create_embeddings()\n",
    "        \n",
    "        print(\"Building FAISS index...\")\n",
    "        dimension = self.embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "        self.index.add(self.embeddings.astype('float32'))\n",
    "        print(f\"Built index with {self.index.ntotal} vectors\")\n",
    "    \n",
    "    def search(self, query, k=5):\n",
    "        \"\"\"\n",
    "        Search for chunks most similar to the query.\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            k: Number of results to return\n",
    "        \n",
    "        Returns:\n",
    "            List of dictionaries with chunk text and metadata\n",
    "        \"\"\"\n",
    "        if self.index is None:\n",
    "            self.build_index()\n",
    "        \n",
    "        # Embed the query\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        # Search the index\n",
    "        distances, indices = self.index.search(query_embedding.astype('float32'), k)\n",
    "        \n",
    "        # Get the results\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx < len(self.chunks_df):  # Ensure index is valid\n",
    "                chunk = self.chunks_df.iloc[idx]\n",
    "                results.append({\n",
    "                    'chunk_id': chunk['chunk_id'],\n",
    "                    'source_file': chunk['source_file'],\n",
    "                    'text': chunk['text'],\n",
    "                    'distance': distances[0][i]\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def answer_question(self, question, k=5):\n",
    "        \"\"\"\n",
    "        Answer a question using RAG.\n",
    "        \n",
    "        Args:\n",
    "            question: The question to answer\n",
    "            k: Number of chunks to retrieve\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with retrieved context and sample answer\n",
    "        \"\"\"\n",
    "        # Retrieve relevant chunks\n",
    "        relevant_chunks = self.search(question, k=k)\n",
    "        \n",
    "        # Combine context\n",
    "        context = \"\\n\\n\".join([f\"From {chunk['source_file']}:\\n{chunk['text']}\" \n",
    "                              for chunk in relevant_chunks])\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'retrieved_chunks': relevant_chunks,\n",
    "            'context': context\n",
    "        }\n",
    "\n",
    "# Initialize and run\n",
    "if __name__ == \"__main__\":\n",
    "    rag = RealEstateRAG()\n",
    "    \n",
    "    # Check if embeddings exist, if not create them\n",
    "    if not os.path.exists(\"embeddings.pkl\"):\n",
    "        rag.create_embeddings()\n",
    "    else:\n",
    "        rag.load_embeddings()\n",
    "    \n",
    "    # Build search index\n",
    "    rag.build_index()\n",
    "    \n",
    "    # Test the search\n",
    "    test_queries = [\n",
    "        \"What is an FHA loan?\",\n",
    "        \"Should I rent or buy a house?\",\n",
    "        \"What are closing costs?\",\n",
    "        \"How do I get pre-approved for a mortgage?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTesting retrieval with sample queries:\")\n",
    "    for query in test_queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        results = rag.search(query, k=2)\n",
    "        for i, result in enumerate(results):\n",
    "            print(f\"Result {i+1} from {result['source_file']}:\")\n",
    "            print(f\"Distance: {result['distance']:.4f}\")\n",
    "            print(f\"Text snippet: {result['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealEstateChatbot:\n",
    "    def __init__(self, rag_system):\n",
    "        \"\"\"\n",
    "        Initialize the chatbot with a RAG system.\n",
    "        \n",
    "        Args:\n",
    "            rag_system: An initialized RealEstateRAG object\n",
    "        \"\"\"\n",
    "        self.rag = rag_system\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def answer(self, query, k=5, show_context=False):\n",
    "        \"\"\"\n",
    "        Answer a user question by retrieving relevant context.\n",
    "        \n",
    "        Args:\n",
    "            query: The user's question\n",
    "            k: Number of contexts to retrieve\n",
    "            show_context: Whether to display the retrieved context\n",
    "        \"\"\"\n",
    "        # Save the user's query to conversation history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": query})\n",
    "        \n",
    "        # Retrieve relevant information\n",
    "        retrieval_results = self.rag.answer_question(query, k=k)\n",
    "        context = retrieval_results['context']\n",
    "        \n",
    "        # Show the retrieved context if requested\n",
    "        if show_context:\n",
    "            print(\"RETRIEVED CONTEXT:\")\n",
    "            print(\"-\" * 80)\n",
    "            print(context)\n",
    "            print(\"-\" * 80)\n",
    "            print()\n",
    "        \n",
    "        # Get sources for citation\n",
    "        sources = []\n",
    "        for chunk in retrieval_results['retrieved_chunks']:\n",
    "            source = chunk['source_file']\n",
    "            if source not in sources:\n",
    "                sources.append(source)\n",
    "        \n",
    "        # Format response with retrieved information\n",
    "        response = \"Based on the retrieved documents:\\n\\n\"\n",
    "        for i, chunk in enumerate(retrieval_results['retrieved_chunks']):\n",
    "            response += f\"From {chunk['source_file']}:\\n\"\n",
    "            response += f\"{chunk['text'][:300]}...\\n\\n\"\n",
    "        \n",
    "        response += \"\\nSources: \" + \", \".join(sources)\n",
    "        \n",
    "        # Add the response to conversation history\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def chat(self):\n",
    "        \"\"\"Start an interactive chat session.\"\"\"\n",
    "        print(\"Welcome to RealEstateGPT! Ask me anything about real estate.\")\n",
    "        print(\"Type 'exit' to end the conversation.\\n\")\n",
    "        \n",
    "        while True:\n",
    "            query = input(\"You: \")\n",
    "            if query.lower() in ['exit', 'quit', 'bye']:\n",
    "                print(\"RealEstateGPT: Goodbye! Hope I was helpful.\")\n",
    "                break\n",
    "            \n",
    "            answer = self.answer(query, show_context=False)\n",
    "            print(f\"\\nRealEstateGPT: {answer}\\n\")\n",
    "\n",
    "# Function for Jupyter Notebook interface\n",
    "def create_chatbot_interface():\n",
    "    try:\n",
    "        import ipywidgets as widgets\n",
    "        from IPython.display import display, clear_output, Markdown\n",
    "        \n",
    "        # Initialize the chatbot\n",
    "        rag = RealEstateRAG()\n",
    "        rag.load_embeddings()\n",
    "        rag.build_index()\n",
    "        chatbot = RealEstateChatbot(rag)\n",
    "        \n",
    "        # Create widgets\n",
    "        output = widgets.Output()\n",
    "        text_input = widgets.Text(\n",
    "            placeholder='Type your real estate question here...',\n",
    "            layout=widgets.Layout(width='80%')\n",
    "        )\n",
    "        context_checkbox = widgets.Checkbox(\n",
    "            value=False,\n",
    "            description='Show retrieved context',\n",
    "            disabled=False\n",
    "        )\n",
    "        send_button = widgets.Button(\n",
    "            description='Send',\n",
    "            button_style='primary',\n",
    "            tooltip='Send your question'\n",
    "        )\n",
    "        clear_button = widgets.Button(\n",
    "            description='Clear',\n",
    "            tooltip='Clear the conversation'\n",
    "        )\n",
    "        \n",
    "        # Layout\n",
    "        input_box = widgets.HBox([text_input, send_button, clear_button])\n",
    "        display(widgets.VBox([context_checkbox, input_box, output]))\n",
    "        \n",
    "        def on_send_button_clicked(b):\n",
    "            with output:\n",
    "                query = text_input.value\n",
    "                if query.strip() == \"\":\n",
    "                    return\n",
    "                \n",
    "                # Display user question\n",
    "                display(Markdown(f\"**You:** {query}\"))\n",
    "                \n",
    "                # Get and display answer\n",
    "                answer = chatbot.answer(query, show_context=context_checkbox.value)\n",
    "                display(Markdown(f\"**RealEstateGPT:** {answer}\"))\n",
    "                \n",
    "                # Clear input field\n",
    "                text_input.value = \"\"\n",
    "        \n",
    "        def on_clear_button_clicked(b):\n",
    "            with output:\n",
    "                clear_output()\n",
    "                chatbot.conversation_history = []\n",
    "        \n",
    "        # Connect events\n",
    "        send_button.on_click(on_send_button_clicked)\n",
    "        clear_button.on_click(on_clear_button_clicked)\n",
    "        \n",
    "        # Also submit on enter key\n",
    "        text_input.on_submit(lambda x: on_send_button_clicked(None))\n",
    "        \n",
    "        return chatbot\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"ipywidgets not available. Use chatbot.chat() instead.\")\n",
    "        return None\n",
    "\n",
    "# Initialize and run immediately\n",
    "print(\"Initializing Real Estate Chatbot...\")\n",
    "rag = RealEstateRAG()\n",
    "rag.load_embeddings()\n",
    "rag.build_index()\n",
    "chatbot = RealEstateChatbot(rag)\n",
    "\n",
    "# Demo section\n",
    "print(\"\\n=== Demo: Ask a few sample questions ===\\n\")\n",
    "\n",
    "demo_questions = [\n",
    "    \"What are the advantages of FHA loans for first-time homebuyers?\",\n",
    "    \"Is it better to rent or buy in 2024?\",\n",
    "    \"What should I know about closing costs?\"\n",
    "]\n",
    "\n",
    "for question in demo_questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    answer = chatbot.answer(question, k=3)\n",
    "    print(f\"Answer: {answer}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "print(\"\\n=== Interactive Mode ===\\n\")\n",
    "# For Jupyter Notebook interface\n",
    "create_chatbot_interface()\n",
    "\n",
    "print(\"Or start a command-line chat with: chatbot.chat()\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
