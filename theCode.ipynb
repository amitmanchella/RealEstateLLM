{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T22:05:08.312728Z",
     "start_time": "2025-04-21T22:05:08.306013Z"
    }
   },
   "source": [
    "#Install packages if necessary\n",
    "#!pip install pandas numpy faiss-cpu PyPDF2 sentence-transformers python-docx tqdm ipywidgets"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T22:22:00.282012Z",
     "start_time": "2025-04-21T22:19:18.989264Z"
    }
   },
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text() + \"\\n\\n\"\n",
    "        return text\n",
    "\n",
    "def process_pdfs(directory):\n",
    "    \"\"\"Process all PDFs in the given directory.\"\"\"\n",
    "    results = []\n",
    "    pdf_files = [f for f in os.listdir(directory) if f.lower().endswith('.pdf')]\n",
    "    \n",
    "    for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        file_path = os.path.join(directory, pdf_file)\n",
    "        try:\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "            results.append({\n",
    "                'filename': pdf_file,\n",
    "                'text': text,\n",
    "                'size': os.path.getsize(file_path)\n",
    "            })\n",
    "            print(f\"Successfully processed {pdf_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_file}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir = \"data\"\n",
    "    results_df = process_pdfs(data_dir)\n",
    "    results_df.to_pickle(\"extracted_pdf_texts.pkl\")\n",
    "    print(f\"Processed {len(results_df)} PDF files and saved to extracted_pdf_texts.pkl\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   4%|▍         | 1/23 [00:01<00:26,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 1507.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   9%|▊         | 2/23 [00:01<00:16,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 2024_Zillow_Rent-vs-Buy.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  13%|█▎        | 3/23 [02:05<19:01, 57.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 4000.1hsghhdbk103123.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  17%|█▋        | 4/23 [02:07<11:13, 35.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed CCL_BuyersGuide.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  22%|██▏       | 5/23 [02:08<06:49, 22.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed consumer-guide-buying-your-first-home-2024-11-05.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  26%|██▌       | 6/23 [02:08<04:19, 15.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed FHA-Reference-Guide-2023.pdf\n",
      "Successfully processed FHA_loan_guidelines.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  35%|███▍      | 8/23 [02:10<02:00,  8.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed First-TIme-HomeBuyer-Guide-2.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  39%|███▉      | 9/23 [02:11<01:28,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed First-TIme-HomeBuyer-Guide.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  43%|████▎     | 10/23 [02:16<01:16,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed GeneralGlossary.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  48%|████▊     | 11/23 [02:17<00:56,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed GLOSSARY_OF_REAL_ESTATE_TERMS.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  52%|█████▏    | 12/23 [02:20<00:44,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed guide_firsttimehomebuying-2.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  57%|█████▋    | 13/23 [02:25<00:42,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed HL_Buyers_Guide_FINAL_March2019.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  61%|██████    | 14/23 [02:27<00:33,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed home-buyers-guide-1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  65%|██████▌   | 15/23 [02:32<00:32,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed Home_Buyers_Guide.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  70%|██████▉   | 16/23 [02:35<00:27,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed NAHREP-Glossary-of-Real-Estate-Industry-Terms.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  74%|███████▍  | 17/23 [02:37<00:18,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed naiop-2024-terms-and-definitions.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  78%|███████▊  | 18/23 [02:37<00:11,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed ort-ss-realestatedictionary.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  83%|████████▎ | 19/23 [02:38<00:07,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed realestateglossary.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  91%|█████████▏| 21/23 [02:38<00:01,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed renting-vs-buying-study-press-release.pdf\n",
      "Successfully processed renting-vs-owning.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  96%|█████████▌| 22/23 [02:40<00:01,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed RS20530.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 23/23 [02:41<00:00,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed TJC_ebook_fha-homeloan.pdf\n",
      "Processed 23 PDF files and saved to extracted_pdf_texts.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T22:32:49.353094Z",
     "start_time": "2025-04-21T22:32:49.311376Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "def check_extracted_data(pkl_path, num_samples=3, sample_length=500):\n",
    "    \"\"\"\n",
    "    Examine the extracted PDF data to check its quality.\n",
    "    \n",
    "    Args:\n",
    "        pkl_path: Path to the pickle file with extracted text\n",
    "        num_samples: Number of random samples to display\n",
    "        sample_length: Number of characters to display from each sample\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    if not os.path.exists(pkl_path):\n",
    "        print(f\"Error: {pkl_path} does not exist.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Loading data from {pkl_path}...\")\n",
    "    df = pd.read_pickle(pkl_path)\n",
    "    \n",
    "    # Print basic information\n",
    "    print(f\"\\nDataset contains {len(df)} documents\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Check for empty text\n",
    "    empty_texts = df[df['text'].str.strip() == ''].shape[0]\n",
    "    print(f\"\\nDocuments with empty text: {empty_texts}\")\n",
    "    \n",
    "    # Check text lengths\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    print(f\"\\nText length statistics:\")\n",
    "    print(df['text_length'].describe())\n",
    "    \n",
    "    # Show some random samples\n",
    "    print(f\"\\n{num_samples} random samples (first {sample_length} chars):\")\n",
    "    sample_indices = random.sample(range(len(df)), min(num_samples, len(df)))\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        doc = df.iloc[idx]\n",
    "        print(f\"\\nSample {i+1} from '{doc['filename']}':\")\n",
    "        print(\"-\" * 80)\n",
    "        print(doc['text'][:sample_length] + \"...\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Check for common issues\n",
    "    print(\"\\nChecking for potential issues:\")\n",
    "    \n",
    "    # Missing spaces between words (possible OCR issue)\n",
    "    no_spaces = df[~df['text'].str.contains(' ', regex=False)].shape[0]\n",
    "    print(f\"Documents with no spaces (potential OCR issues): {no_spaces}\")\n",
    "    \n",
    "    # Unusual characters (possible encoding issues)\n",
    "    unusual_chars = df[df['text'].str.contains('[^\\x00-\\x7F]', regex=True)].shape[0]\n",
    "    print(f\"Documents with non-ASCII characters: {unusual_chars}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_extracted_data(\"extracted_pdf_texts.pkl\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from extracted_pdf_texts.pkl...\n",
      "\n",
      "Dataset contains 23 documents\n",
      "Columns: ['filename', 'text', 'size']\n",
      "\n",
      "Documents with empty text: 0\n",
      "\n",
      "Text length statistics:\n",
      "count    2.300000e+01\n",
      "mean     2.255588e+05\n",
      "std      8.347313e+05\n",
      "min      2.083000e+03\n",
      "25%      2.211300e+04\n",
      "50%      3.974200e+04\n",
      "75%      8.795000e+04\n",
      "max      4.047940e+06\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "3 random samples (first 500 chars):\n",
      "\n",
      "Sample 1 from 'TJC_ebook_fha-homeloan.pdf':\n",
      "--------------------------------------------------------------------------------\n",
      "SIMPLE STEPS TO AN \n",
      "FHA \n",
      "HOME LOAN\n",
      "Written by: Alvaro R. MoreiraMoreir a Team Mortgage’s\n",
      "\n",
      "\n",
      "CONTENTS\n",
      "Introduction ................................................................................................................................  3 \n",
      "Chapter 1:   Choosing the Right FHA Professional .....................................................................  4\n",
      "Chapter 2:  What Y ou Need To Know About FHA Home Loans ...................................................  6\n",
      "Chapter 3:  The Truth...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 2 from '4000.1hsghhdbk103123.pdf':\n",
      "--------------------------------------------------------------------------------\n",
      " \n",
      " \n",
      "Special Attention of:      Transmittal:  Handbook 4000.1  \n",
      "All FHA -Approved Mortgagees     Issued:  October 31, 2023  \n",
      "All Direct Endorsement Underwriters     Effective Date:  April 29, 2024  \n",
      "All Eligible Submission Sources  \n",
      "for Condominium Project Approvals  \n",
      "All FHA Roster Appraisers  \n",
      "All FHA -Approved 203(k) Consultants  \n",
      "All FHA -Approved Title I Lenders  \n",
      "All HUD -Approved Housing Counselors  \n",
      "All HUD -Approved Nonprofit Organizations  \n",
      "All Governmental Entity Participants  \n",
      "All Rea...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 3 from 'FHA_loan_guidelines.pdf':\n",
      "--------------------------------------------------------------------------------\n",
      "FHA LOANS:  CREDIT GUIDELINES\n",
      "HUD requires a borrower to demonstrate a good to excellent repayment history of all\n",
      "debs.  This history serves as the most useful guide in determining a borrower’s\n",
      "willingness to repay credit obligations and serves as a model in predicating his/hers\n",
      "future actions.\n",
      "A borrower who has made payments on previous or current credit obligations (such as\n",
      "credit cards, student loans, etc.) in a timely manner represent a reduced risk to HUD.\n",
      "Conversely, if the credit history...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Checking for potential issues:\n",
      "Documents with no spaces (potential OCR issues): 0\n",
      "Documents with non-ASCII characters: 23\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T22:32:52.680770Z",
     "start_time": "2025-04-21T22:32:52.216162Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text for NLP tasks.\"\"\"\n",
    "    # Replace common non-ASCII characters \n",
    "    text = text.replace('–', '-').replace('—', '-').replace(''', \"'\").replace(''', \"'\")\n",
    "    text = text.replace('\"', '\"').replace('\"', '\"').replace('…', '...')\n",
    "    \n",
    "    # Normalize unicode characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove excessive newlines\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    # Clean up page numbers and headers/footers (common in PDFs)\n",
    "    text = re.sub(r'\\n\\s*\\d+\\s*\\n', '\\n', text)  # Standalone page numbers\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=100):\n",
    "    \"\"\"Split text into overlapping chunks of approximately chunk_size characters.\"\"\"\n",
    "    if len(text) <= chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    # Add timeout protection\n",
    "    max_iterations = (len(text) // (chunk_size - overlap)) * 2  # Generous upper bound\n",
    "    iteration = 0\n",
    "    \n",
    "    while start < len(text) and iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        \n",
    "        # Limit the search window for breaking points to improve performance\n",
    "        search_start = max(start, end - 200)\n",
    "        \n",
    "        # Try to find a good breaking point (end of sentence or paragraph)\n",
    "        if end < len(text):\n",
    "            # Look for paragraph break first (limit search range)\n",
    "            paragraph_break = text.rfind('\\n\\n', search_start, end)\n",
    "            if paragraph_break != -1:\n",
    "                end = paragraph_break\n",
    "            else:\n",
    "                # Look for sentence break (use a simpler, faster approach)\n",
    "                for marker in ['. ', '! ', '? ']:\n",
    "                    sentence_break = text.rfind(marker, search_start, end)\n",
    "                    if sentence_break != -1:\n",
    "                        end = sentence_break + 2  # +2 to include the punctuation and space\n",
    "                        break\n",
    "        \n",
    "        # Make sure we're making progress\n",
    "        if end <= start:\n",
    "            end = start + chunk_size  # Force progress if no break point found\n",
    "            \n",
    "        chunks.append(text[start:end].strip())\n",
    "        start = end - overlap  # Create overlap between chunks\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_text_data(pkl_path, output_path=None, chunk_size=1500):\n",
    "    \"\"\"Clean, preprocess and chunk text data from PDFs.\"\"\"\n",
    "    # Load the data\n",
    "    print(f\"Loading data from {pkl_path}...\")\n",
    "    df = pd.read_pickle(pkl_path)\n",
    "    \n",
    "    # Clean texts\n",
    "    print(\"Cleaning text data...\")\n",
    "    df['cleaned_text'] = df['text'].progress_apply(clean_text)\n",
    "    \n",
    "    # Chunk texts\n",
    "    print(\"Chunking documents into smaller pieces...\")\n",
    "    all_chunks = []\n",
    "    \n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Chunking documents\"):\n",
    "        try:\n",
    "            # Skip extremely large docs or process them differently\n",
    "            if len(row['cleaned_text']) > 1_000_000:  # 1 million chars\n",
    "                print(f\"⚠️ Large document detected: {row['filename']} ({len(row['cleaned_text'])} chars)\")\n",
    "                # Process large documents in a simpler way (just divide by size)\n",
    "                simple_chunks = [row['cleaned_text'][j:j+chunk_size] \n",
    "                               for j in range(0, len(row['cleaned_text']), chunk_size)]\n",
    "                for j, chunk in enumerate(simple_chunks):\n",
    "                    all_chunks.append({\n",
    "                        'source_file': row['filename'],\n",
    "                        'chunk_id': f\"{row['filename']}_simple_{j}\",\n",
    "                        'text': chunk.strip()\n",
    "                    })\n",
    "                continue\n",
    "            \n",
    "            # Regular chunking for normal sized documents\n",
    "            chunks = chunk_text(row['cleaned_text'], chunk_size=chunk_size)\n",
    "            for j, chunk in enumerate(chunks):\n",
    "                all_chunks.append({\n",
    "                    'source_file': row['filename'],\n",
    "                    'chunk_id': f\"{row['filename']}_{j}\",\n",
    "                    'text': chunk\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['filename']}: {e}\")\n",
    "    \n",
    "    chunks_df = pd.DataFrame(all_chunks)\n",
    "    print(f\"Created {len(chunks_df)} chunks from {len(df)} documents\")\n",
    "    \n",
    "    # Save the processed data\n",
    "    if output_path:\n",
    "        chunks_df.to_pickle(output_path)\n",
    "        print(f\"Saved processed chunks to {output_path}\")\n",
    "    \n",
    "    return chunks_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Add tqdm to pandas operations\n",
    "    tqdm.pandas()\n",
    "    \n",
    "    # Process the data\n",
    "    processed_df = process_text_data(\"extracted_pdf_texts.pkl\", \"processed_chunks.pkl\")\n",
    "    \n",
    "    # Display some statistics\n",
    "    print(\"\\nChunk length statistics:\")\n",
    "    processed_df['text_length'] = processed_df['text'].str.len()\n",
    "    print(processed_df['text_length'].describe())\n",
    "    \n",
    "    # Print a few sample chunks\n",
    "    print(\"\\nSample chunks:\")\n",
    "    for i in range(min(3, len(processed_df))):\n",
    "        print(f\"\\nChunk {i+1} from {processed_df.iloc[i]['source_file']}:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(processed_df.iloc[i]['text'][:300] + \"...\" if len(processed_df.iloc[i]['text']) > 300 else processed_df.iloc[i]['text'])\n",
    "        print(\"-\" * 80)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from extracted_pdf_texts.pkl...\n",
      "Cleaning text data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 61.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking documents into smaller pieces...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking documents: 100%|██████████| 23/23 [00:00<00:00, 724.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Large document detected: 4000.1hsghhdbk103123.pdf (3893681 chars)\n",
      "Created 4138 chunks from 23 documents\n",
      "Saved processed chunks to processed_chunks.pkl\n",
      "\n",
      "Chunk length statistics:\n",
      "count    4138.000000\n",
      "mean     1241.202272\n",
      "std       527.984130\n",
      "min        99.000000\n",
      "25%      1428.000000\n",
      "50%      1499.000000\n",
      "75%      1500.000000\n",
      "max      1500.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Sample chunks:\n",
      "\n",
      "Chunk 1 from 1507.pdf:\n",
      "--------------------------------------------------------------------------------\n",
      "Client: HUD Job Number: GOV HUC C1 1175 Size: 14 x 8.5 Date: 4/09/01 Art Director: Kennedy Copywriter: --- Production: Traffic: Studio Proofreader Copywriter Art Director Art Buyer Creative Dir. Acct. Exec. Acct. Sup. THE HUD HOME BUYING GUIDE U.S. Department of Housing and Urban Development Office ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Chunk 2 from 1507.pdf:\n",
      "--------------------------------------------------------------------------------\n",
      "homeownership and have a home of their own.We do itby making homebuying easierand more affordable. One way we can do this is by selling homesHUD ownsin many communitiesthroughout the U.S.,atattractive pricesand economicalterms. So that home youve been dreaming about just may be one you buy from HUD....\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Chunk 3 from 1507.pdf:\n",
      "--------------------------------------------------------------------------------\n",
      "N YOU AFFORD? Before you start shopping for a home, you need to know what kind of home to shop for. To determine that, of course, youve got to figure out how much you can afford to pay each month. Fortunately,theresapretty simple formula forcoming up with thisnumber.Itsthe FHA formula that many mort...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T22:32:58.523080Z",
     "start_time": "2025-04-21T22:32:57.617017Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# First, install required packages\n",
    "# !pip install sentence-transformers faiss-cpu\n",
    "\n",
    "class RealEstateRAG:\n",
    "    def __init__(self, chunks_path=\"processed_chunks.pkl\", embedding_model=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system with document chunks and embedding model.\n",
    "        \n",
    "        Args:\n",
    "            chunks_path: Path to the pickle file with processed chunks\n",
    "            embedding_model: SentenceTransformer model to use for embeddings\n",
    "        \"\"\"\n",
    "        self.chunks_path = chunks_path\n",
    "        self.embedding_model_name = embedding_model\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.index = None\n",
    "        self.chunks_df = None\n",
    "        self.embeddings = None\n",
    "        \n",
    "        # Load chunks if they exist\n",
    "        if os.path.exists(chunks_path):\n",
    "            self.load_chunks()\n",
    "    \n",
    "    def load_chunks(self):\n",
    "        \"\"\"Load document chunks from pickle file.\"\"\"\n",
    "        print(f\"Loading chunks from {self.chunks_path}...\")\n",
    "        self.chunks_df = pd.read_pickle(self.chunks_path)\n",
    "        print(f\"Loaded {len(self.chunks_df)} chunks\")\n",
    "    \n",
    "    def create_embeddings(self, save_path=\"embeddings.pkl\"):\n",
    "        \"\"\"Create embeddings for all chunks.\"\"\"\n",
    "        if self.chunks_df is None:\n",
    "            self.load_chunks()\n",
    "        \n",
    "        print(f\"Creating embeddings using {self.embedding_model_name}...\")\n",
    "        texts = self.chunks_df['text'].tolist()\n",
    "        \n",
    "        # Embed in batches to avoid memory issues\n",
    "        batch_size = 64\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Creating embeddings\"):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            batch_embeddings = self.embedding_model.encode(batch_texts)\n",
    "            embeddings.append(batch_embeddings)\n",
    "        \n",
    "        self.embeddings = np.vstack(embeddings)\n",
    "        \n",
    "        # Save embeddings\n",
    "        if save_path:\n",
    "            with open(save_path, 'wb') as f:\n",
    "                pickle.dump(self.embeddings, f)\n",
    "            print(f\"Saved embeddings to {save_path}\")\n",
    "        \n",
    "        return self.embeddings\n",
    "    \n",
    "    def load_embeddings(self, embeddings_path=\"embeddings.pkl\"):\n",
    "        \"\"\"Load pre-computed embeddings.\"\"\"\n",
    "        if os.path.exists(embeddings_path):\n",
    "            print(f\"Loading embeddings from {embeddings_path}...\")\n",
    "            with open(embeddings_path, 'rb') as f:\n",
    "                self.embeddings = pickle.load(f)\n",
    "            print(f\"Loaded embeddings with shape {self.embeddings.shape}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Embeddings file {embeddings_path} not found.\")\n",
    "            return False\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"Build a FAISS index for fast similarity search.\"\"\"\n",
    "        if self.embeddings is None:\n",
    "            if not self.load_embeddings():\n",
    "                self.create_embeddings()\n",
    "        \n",
    "        print(\"Building FAISS index...\")\n",
    "        dimension = self.embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "        self.index.add(self.embeddings.astype('float32'))\n",
    "        print(f\"Built index with {self.index.ntotal} vectors\")\n",
    "    \n",
    "    def search(self, query, k=5):\n",
    "        \"\"\"\n",
    "        Search for chunks most similar to the query.\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            k: Number of results to return\n",
    "        \n",
    "        Returns:\n",
    "            List of dictionaries with chunk text and metadata\n",
    "        \"\"\"\n",
    "        if self.index is None:\n",
    "            self.build_index()\n",
    "        \n",
    "        # Embed the query\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        # Search the index\n",
    "        distances, indices = self.index.search(query_embedding.astype('float32'), k)\n",
    "        \n",
    "        # Get the results\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx < len(self.chunks_df):  # Ensure index is valid\n",
    "                chunk = self.chunks_df.iloc[idx]\n",
    "                results.append({\n",
    "                    'chunk_id': chunk['chunk_id'],\n",
    "                    'source_file': chunk['source_file'],\n",
    "                    'text': chunk['text'],\n",
    "                    'distance': distances[0][i]\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def answer_question(self, question, k=5):\n",
    "        \"\"\"\n",
    "        Answer a question using RAG.\n",
    "        \n",
    "        Args:\n",
    "            question: The question to answer\n",
    "            k: Number of chunks to retrieve\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with retrieved context and sample answer\n",
    "        \"\"\"\n",
    "        # Retrieve relevant chunks\n",
    "        relevant_chunks = self.search(question, k=k)\n",
    "        \n",
    "        # Combine context\n",
    "        context = \"\\n\\n\".join([f\"From {chunk['source_file']}:\\n{chunk['text']}\" \n",
    "                              for chunk in relevant_chunks])\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'retrieved_chunks': relevant_chunks,\n",
    "            'context': context\n",
    "        }\n",
    "\n",
    "# Initialize and run\n",
    "if __name__ == \"__main__\":\n",
    "    rag = RealEstateRAG()\n",
    "    \n",
    "    # Check if embeddings exist, if not create them\n",
    "    if not os.path.exists(\"embeddings.pkl\"):\n",
    "        rag.create_embeddings()\n",
    "    else:\n",
    "        rag.load_embeddings()\n",
    "    \n",
    "    # Build search index\n",
    "    rag.build_index()\n",
    "    \n",
    "    # Test the search\n",
    "    test_queries = [\n",
    "        \"What is an FHA loan?\",\n",
    "        \"Should I rent or buy a house?\",\n",
    "        \"What are closing costs?\",\n",
    "        \"How do I get pre-approved for a mortgage?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTesting retrieval with sample queries:\")\n",
    "    for query in test_queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        results = rag.search(query, k=2)\n",
    "        for i, result in enumerate(results):\n",
    "            print(f\"Result {i+1} from {result['source_file']}:\")\n",
    "            print(f\"Distance: {result['distance']:.4f}\")\n",
    "            print(f\"Text snippet: {result['text'][:150]}...\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chunks from processed_chunks.pkl...\n",
      "Loaded 4138 chunks\n",
      "Loading embeddings from embeddings.pkl...\n",
      "Loaded embeddings with shape (4138, 384)\n",
      "Building FAISS index...\n",
      "Built index with 4138 vectors\n",
      "\n",
      "Testing retrieval with sample queries:\n",
      "\n",
      "Query: What is an FHA loan?\n",
      "Result 1 from FHA-Reference-Guide-2023.pdf:\n",
      "Distance: 0.6199\n",
      "Text snippet: alify for than conventional mortgages. FHA loans are insured by the Federal Housing Administration. FHA does not lend money or issue cre dit, so the f...\n",
      "Result 2 from RS20530.pdf:\n",
      "Distance: 0.6410\n",
      "Text snippet: ....... ................................ ................................ ......................... 19 FHA-Insured Home Loans: An Overview Congression...\n",
      "\n",
      "Query: Should I rent or buy a house?\n",
      "Result 1 from CCL_BuyersGuide.pdf:\n",
      "Distance: 0.7296\n",
      "Text snippet: pected to rise 10 to 15 percent over the next decade, creating a continued high demand for housing. EQUITY Money paid for rent is money that youll nev...\n",
      "Result 2 from HL_Buyers_Guide_FINAL_March2019.pdf:\n",
      "Distance: 0.7623\n",
      "Text snippet: heres one reason you feel speaks especially to you, circle it with some hearts. Yknow. If you want. Any other reasons?Because Ive always wanted to own...\n",
      "\n",
      "Query: What are closing costs?\n",
      "Result 1 from Home_Buyers_Guide.pdf:\n",
      "Distance: 0.7689\n",
      "Text snippet: rices of similar properties. 8A.Home .Buyers .Glossary H O M E B U Y E R S G U I D E R E A LT O R . C O M | T O P P R O D U C E R | S T E P - B Y- S T...\n",
      "Result 2 from realestateglossary.pdf:\n",
      "Distance: 0.8004\n",
      "Text snippet: torneys. Closing Costs: The upfront fees charged in connection with a mortgage loan trans - action. Money paid by a buyer (and/or seller or other thir...\n",
      "\n",
      "Query: How do I get pre-approved for a mortgage?\n",
      "Result 1 from Home_Buyers_Guide.pdf:\n",
      "Distance: 0.6360\n",
      "Text snippet: ion the lender will provide you with a document that details how much you can borrow to buy a home. You may want to consider looking online to see wha...\n",
      "Result 2 from HL_Buyers_Guide_FINAL_March2019.pdf:\n",
      "Distance: 0.6367\n",
      "Text snippet: an effectively size up your loan options and decide which lender is best for you - and your future. (If you need help navigating the details, the Cons...\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T22:33:06.234581Z",
     "start_time": "2025-04-21T22:33:04.432821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RealEstateChatbot:\n",
    "    def __init__(self, rag_system):\n",
    "        \"\"\"\n",
    "        Initialize the chatbot with a RAG system.\n",
    "\n",
    "        Args:\n",
    "            rag_system: An initialized RealEstateRAG object\n",
    "        \"\"\"\n",
    "        self.rag = rag_system\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def answer(self, query, k=5, show_context=False):\n",
    "        \"\"\"\n",
    "        Answer a user question by retrieving relevant context.\n",
    "\n",
    "        Args:\n",
    "            query: The user's question\n",
    "            k: Number of contexts to retrieve\n",
    "            show_context: Whether to display the retrieved context\n",
    "        \"\"\"\n",
    "        # Save the user's query to conversation history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "        # Retrieve relevant information\n",
    "        retrieval_results = self.rag.answer_question(query, k=k)\n",
    "        context = retrieval_results['context']\n",
    "\n",
    "        # Show the retrieved context if requested\n",
    "        if show_context:\n",
    "            print(\"RETRIEVED CONTEXT:\")\n",
    "            print(\"-\" * 80)\n",
    "            print(context)\n",
    "            print(\"-\" * 80)\n",
    "            print()\n",
    "\n",
    "        # Get sources for citation\n",
    "        sources = []\n",
    "        for chunk in retrieval_results['retrieved_chunks']:\n",
    "            source = chunk['source_file']\n",
    "            if source not in sources:\n",
    "                sources.append(source)\n",
    "\n",
    "        # Format response with retrieved information\n",
    "        response = \"Based on the retrieved documents:\\n\\n\"\n",
    "        for i, chunk in enumerate(retrieval_results['retrieved_chunks']):\n",
    "            response += f\"From {chunk['source_file']}:\\n\"\n",
    "            response += f\"{chunk['text'][:300]}...\\n\\n\"\n",
    "\n",
    "        response += \"\\nSources: \" + \", \".join(sources)\n",
    "\n",
    "        # Add the response to conversation history\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "        return response\n",
    "\n",
    "    def chat(self):\n",
    "        \"\"\"Start an interactive chat session.\"\"\"\n",
    "        print(\"Welcome to RealEstateGPT! Ask me anything about real estate.\")\n",
    "        print(\"Type 'exit' to end the conversation.\\n\")\n",
    "\n",
    "        while True:\n",
    "            query = input(\"You: \")\n",
    "            if query.lower() in ['exit', 'quit', 'bye']:\n",
    "                print(\"RealEstateGPT: Goodbye! Hope I was helpful.\")\n",
    "                break\n",
    "\n",
    "            answer = self.answer(query, show_context=False)\n",
    "            print(f\"\\nRealEstateGPT: {answer}\\n\")\n",
    "\n",
    "# Function for Jupyter Notebook interface\n",
    "def create_chatbot_interface():\n",
    "    try:\n",
    "        import ipywidgets as widgets\n",
    "        from IPython.display import display, clear_output, Markdown\n",
    "\n",
    "        # Initialize the chatbot\n",
    "        rag = RealEstateRAG()\n",
    "        rag.load_embeddings()\n",
    "        rag.build_index()\n",
    "        chatbot = RealEstateChatbot(rag)\n",
    "\n",
    "        # Create widgets\n",
    "        output = widgets.Output()\n",
    "        text_input = widgets.Text(\n",
    "            placeholder='Type your real estate question here...',\n",
    "            layout=widgets.Layout(width='80%')\n",
    "        )\n",
    "        context_checkbox = widgets.Checkbox(\n",
    "            value=False,\n",
    "            description='Show retrieved context',\n",
    "            disabled=False\n",
    "        )\n",
    "        send_button = widgets.Button(\n",
    "            description='Send',\n",
    "            button_style='primary',\n",
    "            tooltip='Send your question'\n",
    "        )\n",
    "        clear_button = widgets.Button(\n",
    "            description='Clear',\n",
    "            tooltip='Clear the conversation'\n",
    "        )\n",
    "\n",
    "        # Layout\n",
    "        input_box = widgets.HBox([text_input, send_button, clear_button])\n",
    "        display(widgets.VBox([context_checkbox, input_box, output]))\n",
    "\n",
    "        def on_send_button_clicked(b):\n",
    "            with output:\n",
    "                query = text_input.value\n",
    "                if query.strip() == \"\":\n",
    "                    return\n",
    "\n",
    "                # Display user question\n",
    "                display(Markdown(f\"**You:** {query}\"))\n",
    "\n",
    "                # Get and display answer\n",
    "                answer = chatbot.answer(query, show_context=context_checkbox.value)\n",
    "                display(Markdown(f\"**RealEstateGPT:** {answer}\"))\n",
    "\n",
    "                # Clear input field\n",
    "                text_input.value = \"\"\n",
    "\n",
    "        def on_clear_button_clicked(b):\n",
    "            with output:\n",
    "                clear_output()\n",
    "                chatbot.conversation_history = []\n",
    "\n",
    "        # Connect events\n",
    "        send_button.on_click(on_send_button_clicked)\n",
    "        clear_button.on_click(on_clear_button_clicked)\n",
    "\n",
    "        # Also submit on enter key\n",
    "        text_input.on_submit(lambda x: on_send_button_clicked(None))\n",
    "\n",
    "        return chatbot\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"ipywidgets not available. Use chatbot.chat() instead.\")\n",
    "        return None\n",
    "\n",
    "# Initialize and run immediately\n",
    "print(\"Initializing Real Estate Chatbot...\")\n",
    "rag = RealEstateRAG()\n",
    "rag.load_embeddings()\n",
    "rag.build_index()\n",
    "chatbot = RealEstateChatbot(rag)\n",
    "\n",
    "# Demo section\n",
    "print(\"\\n=== Demo: Ask a few sample questions ===\\n\")\n",
    "\n",
    "demo_questions = [\n",
    "    \"What are the advantages of FHA loans for first-time homebuyers?\",\n",
    "    \"Is it better to rent or buy in 2024?\",\n",
    "    \"What should I know about closing costs?\"\n",
    "]\n",
    "\n",
    "for question in demo_questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    answer = chatbot.answer(question, k=3)\n",
    "    print(f\"Answer: {answer}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "print(\"\\n=== Interactive Mode ===\\n\")\n",
    "# For Jupyter Notebook interface\n",
    "create_chatbot_interface()\n",
    "\n",
    "print(\"Or start a command-line chat with: chatbot.chat()\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Real Estate Chatbot...\n",
      "Loading chunks from processed_chunks.pkl...\n",
      "Loaded 4138 chunks\n",
      "Loading embeddings from embeddings.pkl...\n",
      "Loaded embeddings with shape (4138, 384)\n",
      "Building FAISS index...\n",
      "Built index with 4138 vectors\n",
      "\n",
      "=== Demo: Ask a few sample questions ===\n",
      "\n",
      "Question: What are the advantages of FHA loans for first-time homebuyers?\n",
      "Answer: Based on the retrieved documents:\n",
      "\n",
      "From guide_firsttimehomebuying-2.pdf:\n",
      "ents can be as low as 3.5%. + Credit requirements are more accessible than many conventional loans. + Closing costs may be lower than with conventional loans.CONS: An FHA loan may not be the most cost-effective option if you have good credit and 10% or greater down payment. Mortgage insurance is req...\n",
      "\n",
      "From RS20530.pdf:\n",
      "arly large role for first -time homebuyers, low - and moderate -income households, and minorit ies. For example , nearly 85% of FHA -insured mortgages made to purchase a home (rather than to refinance an existing mortgage) in FY20 21 were ob tained by first -time homebuyers, while about one-third of...\n",
      "\n",
      "From guide_firsttimehomebuying-2.pdf:\n",
      "ny point before youve officially closed on the home - such as changing jobs, consolidating debt, closing a credit account, or opening a new line of credit.FINANCING YOUR DREAM HOME 18 Popular Financing Options for First-Time Homebuyers Now that you have a better understanding of the basics, youre re...\n",
      "\n",
      "\n",
      "Sources: guide_firsttimehomebuying-2.pdf, RS20530.pdf\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question: Is it better to rent or buy in 2024?\n",
      "Answer: Based on the retrieved documents:\n",
      "\n",
      "From 2024_Zillow_Rent-vs-Buy.pdf:\n",
      "renter is fully investing the money that would have been used to buy a home or maintain 11 Buy vs. Rent - Understanding the economic questionUnderstand Rent Versus Buy: In vestment potential Forecast consistent with rates trending down to low 6% range by year end, then historic appreciation ove r th...\n",
      "\n",
      "From 2024_Zillow_Rent-vs-Buy.pdf:\n",
      "growth will most likely return if all our thinking regarding generational underbuilding is correct. New homeowners that lock in higher mortgage rates are insulated by the future opportunity to refinance should the economy undergo future stress (mortgage rates are often counter-cyclical to general ec...\n",
      "\n",
      "From 2024_Zillow_Rent-vs-Buy.pdf:\n",
      "outbid others; todays potential buyer may still need to delay homeownership to save enough to make the mortgage payment affordable or to qualify for a loan (typically your debt-to-income ratio, including debt unrelated to housing, like student loans, has to be below 43%). UPFRONT COSTS TO RENT Ultim...\n",
      "\n",
      "\n",
      "Sources: 2024_Zillow_Rent-vs-Buy.pdf\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Question: What should I know about closing costs?\n",
      "Answer: Based on the retrieved documents:\n",
      "\n",
      "From HL_Buyers_Guide_FINAL_March2019.pdf:\n",
      "fixing the problems yourself. Worst-case scenario: Y ou have to delay closing to resolve problems. In the unlikely event that happens, your agent will help you address the issue.When Will the Final Walk-Through Happen?Some items can change by only 10% (fees paid to local government to record the mor...\n",
      "\n",
      "From guide_firsttimehomebuying-2.pdf:\n",
      "FRONT COSTS Before the sale is complete, there are a few upfront expenses youll be expected to pay. These are usually paid out of your own pocket and include: Earnest money: This is a good-faith deposit, usually made within three days of your offer being accepted, which goes toward the down payment....\n",
      "\n",
      "From TJC_ebook_fha-homeloan.pdf:\n",
      "Closing Costs When obtaining a mortgage, points and fees are terms you should know and understand. They are lumped into two major categories: Mortgage Points - Y ou may be offered to pay points to get a lower interest rate or you may be charged a point by the mortgage lender for originating your mor...\n",
      "\n",
      "\n",
      "Sources: HL_Buyers_Guide_FINAL_March2019.pdf, guide_firsttimehomebuying-2.pdf, TJC_ebook_fha-homeloan.pdf\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "=== Interactive Mode ===\n",
      "\n",
      "Loading chunks from processed_chunks.pkl...\n",
      "Loaded 4138 chunks\n",
      "Loading embeddings from embeddings.pkl...\n",
      "Loaded embeddings with shape (4138, 384)\n",
      "Building FAISS index...\n",
      "Built index with 4138 vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Checkbox(value=False, description='Show retrieved context'), HBox(children=(Text(value='', layo…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "841f0bc7a7474f019c20e53f06e0e5e5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Or start a command-line chat with: chatbot.chat()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Madhumita\\AppData\\Local\\Temp\\ipykernel_2108\\4285059954.py:133: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  text_input.on_submit(lambda x: on_send_button_clicked(None))\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T22:33:13.484030Z",
     "start_time": "2025-04-21T22:33:13.468231Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import docx\n",
    "import PyPDF2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Base class for processing different types of real estate data\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir=\"data\"):\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def save_processed_data(self, data, output_path):\n",
    "        \"\"\"Save processed data to a pickle file\"\"\"\n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        print(f\"Saved processed data to {output_path}\")\n",
    "\n",
    "class StructuredDataProcessor(DataProcessor):\n",
    "    \"\"\"Process structured data (Excel/CSV files)\"\"\"\n",
    "    \n",
    "    def process_excel(self, file_path):\n",
    "        \"\"\"Process Excel file and return structured data\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            return {\n",
    "                'filename': os.path.basename(file_path),\n",
    "                'data': df,\n",
    "                'metadata': {\n",
    "                    'columns': df.columns.tolist(),\n",
    "                    'rows': len(df),\n",
    "                    'dtypes': {col: str(dtype) for col, dtype in df.dtypes.items()}\n",
    "                }\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_csv(self, file_path):\n",
    "        \"\"\"Process CSV file and return structured data\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            return {\n",
    "                'filename': os.path.basename(file_path),\n",
    "                'data': df,\n",
    "                'metadata': {\n",
    "                    'columns': df.columns.tolist(),\n",
    "                    'rows': len(df),\n",
    "                    'dtypes': {col: str(dtype) for col, dtype in df.dtypes.items()}\n",
    "                }\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_all(self):\n",
    "        \"\"\"Process all structured data files in the data directory\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Find all Excel and CSV files\n",
    "        excel_files = [f for f in os.listdir(self.data_dir) \n",
    "                      if f.lower().endswith(('.xlsx', '.xls'))]\n",
    "        csv_files = [f for f in os.listdir(self.data_dir) \n",
    "                    if f.lower().endswith('.csv')]\n",
    "        \n",
    "        # Process Excel files\n",
    "        for file in tqdm(excel_files, desc=\"Processing Excel files\"):\n",
    "            file_path = os.path.join(self.data_dir, file)\n",
    "            result = self.process_excel(file_path)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "        \n",
    "        # Process CSV files\n",
    "        for file in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
    "            file_path = os.path.join(self.data_dir, file)\n",
    "            result = self.process_csv(file_path)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "class DocProcessor(DataProcessor):\n",
    "    \"\"\"Process Word documents\"\"\"\n",
    "    \n",
    "    def extract_text_from_docx(self, file_path):\n",
    "        \"\"\"Extract text from a Word document\"\"\"\n",
    "        try:\n",
    "            doc = docx.Document(file_path)\n",
    "            full_text = []\n",
    "            for para in doc.paragraphs:\n",
    "                full_text.append(para.text)\n",
    "            return '\\n'.join(full_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {file_path}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def process_all(self):\n",
    "        \"\"\"Process all Word documents in the data directory\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Find all Word documents\n",
    "        docx_files = [f for f in os.listdir(self.data_dir) \n",
    "                     if f.lower().endswith('.docx')]\n",
    "        \n",
    "        # Process Word documents\n",
    "        for file in tqdm(docx_files, desc=\"Processing Word documents\"):\n",
    "            file_path = os.path.join(self.data_dir, file)\n",
    "            try:\n",
    "                text = self.extract_text_from_docx(file_path)\n",
    "                results.append({\n",
    "                    'filename': file,\n",
    "                    'text': text,\n",
    "                    'size': os.path.getsize(file_path)\n",
    "                })\n",
    "                print(f\"Successfully processed {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {e}\")\n",
    "        \n",
    "        return results"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T22:33:19.610355Z",
     "start_time": "2025-04-21T22:33:19.576285Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "class EnhancedRealEstateRAG:\n",
    "    def __init__(self, \n",
    "                 text_chunks_path=\"processed_chunks.pkl\", \n",
    "                 structured_data_path=\"processed_structured_data.pkl\",\n",
    "                 embedding_model=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the enhanced RAG system with document chunks and embedding model.\n",
    "        \n",
    "        Args:\n",
    "            text_chunks_path: Path to the pickle file with processed text chunks\n",
    "            structured_data_path: Path to the pickle file with processed structured data\n",
    "            embedding_model: SentenceTransformer model to use for embeddings\n",
    "        \"\"\"\n",
    "        self.text_chunks_path = text_chunks_path\n",
    "        self.structured_data_path = structured_data_path\n",
    "        self.embedding_model_name = embedding_model\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        \n",
    "        self.text_index = None\n",
    "        self.text_chunks_df = None\n",
    "        self.text_embeddings = None\n",
    "        \n",
    "        self.structured_data = None\n",
    "        self.structured_data_descriptions = None\n",
    "        self.structured_data_index = None\n",
    "        self.structured_data_embeddings = None\n",
    "        \n",
    "        # Load data if it exists\n",
    "        self.load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load document chunks and structured data from pickle files.\"\"\"\n",
    "        # Load text chunks\n",
    "        if os.path.exists(self.text_chunks_path):\n",
    "            print(f\"Loading text chunks from {self.text_chunks_path}...\")\n",
    "            self.text_chunks_df = pd.read_pickle(self.text_chunks_path)\n",
    "            print(f\"Loaded {len(self.text_chunks_df)} text chunks\")\n",
    "        \n",
    "        # Load structured data\n",
    "        if os.path.exists(self.structured_data_path):\n",
    "            print(f\"Loading structured data from {self.structured_data_path}...\")\n",
    "            with open(self.structured_data_path, 'rb') as f:\n",
    "                self.structured_data = pickle.load(f)\n",
    "            print(f\"Loaded structured data with {len(self.structured_data)} files\")\n",
    "            \n",
    "            # Create descriptions for structured data\n",
    "            self.create_structured_data_descriptions()\n",
    "    \n",
    "    def create_structured_data_descriptions(self):\n",
    "        \"\"\"Create searchable descriptions for structured data.\"\"\"\n",
    "        if not self.structured_data:\n",
    "            return\n",
    "        \n",
    "        descriptions = []\n",
    "        for item in self.structured_data:\n",
    "            df = item['data']\n",
    "            filename = item['filename']\n",
    "            \n",
    "            # Create a description of the dataset\n",
    "            description = f\"Dataset: {filename}\\n\"\n",
    "            description += f\"Contains {len(df)} rows and {len(df.columns)} columns.\\n\"\n",
    "            description += f\"Columns: {', '.join(df.columns.tolist())}\\n\"\n",
    "            \n",
    "            # Add sample data (first few rows)\n",
    "            description += \"Sample data:\\n\"\n",
    "            sample = df.head(3).to_string()\n",
    "            description += sample\n",
    "            \n",
    "            descriptions.append({\n",
    "                'source_file': filename,\n",
    "                'text': description,\n",
    "                'type': 'structured'\n",
    "            })\n",
    "            \n",
    "            # Also add column-specific descriptions\n",
    "            for column in df.columns:\n",
    "                col_desc = f\"Column '{column}' in dataset {filename}.\\n\"\n",
    "                try:\n",
    "                    # Add statistical information if numeric\n",
    "                    if pd.api.types.is_numeric_dtype(df[column]):\n",
    "                        col_desc += f\"Min: {df[column].min()}, Max: {df[column].max()}, Mean: {df[column].mean():.2f}\\n\"\n",
    "                    # Add unique values if categorical with few values\n",
    "                    elif df[column].nunique() < 10:\n",
    "                        col_desc += f\"Values: {', '.join(map(str, df[column].unique()[:10]))}\\n\"\n",
    "                    # Add value counts for high-frequency items\n",
    "                    value_counts = df[column].value_counts().nlargest(5).to_dict()\n",
    "                    col_desc += \"Most common values: \" + \", \".join([f\"{k}: {v}\" for k, v in value_counts.items()])\n",
    "                except:\n",
    "                    # Skip if there's an error calculating stats\n",
    "                    pass\n",
    "                \n",
    "                descriptions.append({\n",
    "                    'source_file': filename,\n",
    "                    'text': col_desc,\n",
    "                    'type': 'structured_column',\n",
    "                    'column': column\n",
    "                })\n",
    "        \n",
    "        self.structured_data_descriptions = pd.DataFrame(descriptions)\n",
    "    \n",
    "    def create_embeddings(self, save_text_path=\"text_embeddings.pkl\", save_structured_path=\"structured_embeddings.pkl\"):\n",
    "        \"\"\"Create embeddings for all content.\"\"\"\n",
    "        # Create text embeddings\n",
    "        if self.text_chunks_df is not None:\n",
    "            print(f\"Creating text embeddings using {self.embedding_model_name}...\")\n",
    "            texts = self.text_chunks_df['text'].tolist()\n",
    "            \n",
    "            # Embed in batches\n",
    "            batch_size = 64\n",
    "            embeddings = []\n",
    "            \n",
    "            for i in tqdm(range(0, len(texts), batch_size), desc=\"Creating text embeddings\"):\n",
    "                batch_texts = texts[i:i + batch_size]\n",
    "                batch_embeddings = self.embedding_model.encode(batch_texts)\n",
    "                embeddings.append(batch_embeddings)\n",
    "            \n",
    "            self.text_embeddings = np.vstack(embeddings)\n",
    "            \n",
    "            # Save embeddings\n",
    "            if save_text_path:\n",
    "                with open(save_text_path, 'wb') as f:\n",
    "                    pickle.dump(self.text_embeddings, f)\n",
    "                print(f\"Saved text embeddings to {save_text_path}\")\n",
    "        \n",
    "        # Create structured data embeddings\n",
    "        if self.structured_data_descriptions is not None:\n",
    "            print(f\"Creating structured data embeddings using {self.embedding_model_name}...\")\n",
    "            descriptions = self.structured_data_descriptions['text'].tolist()\n",
    "            \n",
    "            # Embed in batches\n",
    "            batch_size = 64\n",
    "            embeddings = []\n",
    "            \n",
    "            for i in tqdm(range(0, len(descriptions), batch_size), desc=\"Creating structured data embeddings\"):\n",
    "                batch_texts = descriptions[i:i + batch_size]\n",
    "                batch_embeddings = self.embedding_model.encode(batch_texts)\n",
    "                embeddings.append(batch_embeddings)\n",
    "            \n",
    "            self.structured_data_embeddings = np.vstack(embeddings)\n",
    "            \n",
    "            # Save embeddings\n",
    "            if save_structured_path:\n",
    "                with open(save_structured_path, 'wb') as f:\n",
    "                    pickle.dump(self.structured_data_embeddings, f)\n",
    "                print(f\"Saved structured data embeddings to {save_structured_path}\")\n",
    "    \n",
    "    def load_embeddings(self, text_embeddings_path=\"text_embeddings.pkl\", \n",
    "                       structured_embeddings_path=\"structured_embeddings.pkl\"):\n",
    "        \"\"\"Load pre-computed embeddings.\"\"\"\n",
    "        # Load text embeddings\n",
    "        if os.path.exists(text_embeddings_path):\n",
    "            print(f\"Loading text embeddings from {text_embeddings_path}...\")\n",
    "            with open(text_embeddings_path, 'rb') as f:\n",
    "                self.text_embeddings = pickle.load(f)\n",
    "            print(f\"Loaded text embeddings with shape {self.text_embeddings.shape}\")\n",
    "        \n",
    "        # Load structured data embeddings\n",
    "        if os.path.exists(structured_embeddings_path) and self.structured_data_descriptions is not None:\n",
    "            print(f\"Loading structured data embeddings from {structured_embeddings_path}...\")\n",
    "            with open(structured_embeddings_path, 'rb') as f:\n",
    "                self.structured_data_embeddings = pickle.load(f)\n",
    "            print(f\"Loaded structured data embeddings with shape {self.structured_data_embeddings.shape}\")\n",
    "    \n",
    "    def build_indices(self):\n",
    "        \"\"\"Build FAISS indices for fast similarity search.\"\"\"\n",
    "        # Build text index\n",
    "        if self.text_embeddings is not None:\n",
    "            print(\"Building text FAISS index...\")\n",
    "            dimension = self.text_embeddings.shape[1]\n",
    "            self.text_index = faiss.IndexFlatL2(dimension)\n",
    "            self.text_index.add(self.text_embeddings.astype('float32'))\n",
    "            print(f\"Built text index with {self.text_index.ntotal} vectors\")\n",
    "        \n",
    "        # Build structured data index\n",
    "        if self.structured_data_embeddings is not None:\n",
    "            print(\"Building structured data FAISS index...\")\n",
    "            dimension = self.structured_data_embeddings.shape[1]\n",
    "            self.structured_data_index = faiss.IndexFlatL2(dimension)\n",
    "            self.structured_data_index.add(self.structured_data_embeddings.astype('float32'))\n",
    "            print(f\"Built structured data index with {self.structured_data_index.ntotal} vectors\")\n",
    "    \n",
    "    def classify_query(self, query):\n",
    "        \"\"\"Classify whether the query is more likely to be for text or structured data.\"\"\"\n",
    "        # Keywords that suggest structured data queries\n",
    "        structured_keywords = [\n",
    "            'how many', 'average', 'median', 'statistics', 'price range', 'data', \n",
    "            'dataset', 'spreadsheet', 'chart', 'graph', 'table', 'column', 'row',\n",
    "            'excel', 'csv', 'percentage', 'trend', 'distribution', 'count'\n",
    "        ]\n",
    "        \n",
    "        # Simple rule-based classification\n",
    "        if any(keyword in query.lower() for keyword in structured_keywords):\n",
    "            return \"structured\"\n",
    "        \n",
    "        # More complex classification could be implemented here\n",
    "        return \"text\"\n",
    "    \n",
    "    def search(self, query, k=5, query_type=None):\n",
    "        \"\"\"\n",
    "        Search for content most similar to the query.\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            k: Number of results to return\n",
    "            query_type: Force search in \"text\" or \"structured\" data, or \"both\"\n",
    "        \n",
    "        Returns:\n",
    "            List of dictionaries with content and metadata\n",
    "        \"\"\"\n",
    "        # Auto-classify query if not specified\n",
    "        if query_type is None:\n",
    "            query_type = self.classify_query(query)\n",
    "        \n",
    "        # Embed the query\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Search text index\n",
    "        if query_type in [\"text\", \"both\"] and self.text_index is not None:\n",
    "            distances, indices = self.text_index.search(query_embedding.astype('float32'), k)\n",
    "            \n",
    "            for i, idx in enumerate(indices[0]):\n",
    "                if idx < len(self.text_chunks_df):  # Ensure index is valid\n",
    "                    chunk = self.text_chunks_df.iloc[idx]\n",
    "                    results.append({\n",
    "                        'chunk_id': chunk['chunk_id'],\n",
    "                        'source_file': chunk['source_file'],\n",
    "                        'text': chunk['text'],\n",
    "                        'distance': distances[0][i],\n",
    "                        'type': 'text'\n",
    "                    })\n",
    "        \n",
    "        # Search structured data index\n",
    "        if query_type in [\"structured\", \"both\"] and self.structured_data_index is not None:\n",
    "            # Adjust k to get enough results when combining\n",
    "            s_k = k if query_type == \"structured\" else k // 2\n",
    "            \n",
    "            distances, indices = self.structured_data_index.search(query_embedding.astype('float32'), s_k)\n",
    "            \n",
    "            for i, idx in enumerate(indices[0]):\n",
    "                if idx < len(self.structured_data_descriptions):  # Ensure index is valid\n",
    "                    item = self.structured_data_descriptions.iloc[idx]\n",
    "                    results.append({\n",
    "                        'source_file': item['source_file'],\n",
    "                        'text': item['text'],\n",
    "                        'distance': distances[0][i],\n",
    "                        'type': item['type']\n",
    "                    })\n",
    "        \n",
    "        # Sort by distance\n",
    "        results.sort(key=lambda x: x['distance'])\n",
    "        \n",
    "        # Limit to k results\n",
    "        return results[:k]\n",
    "    \n",
    "    def answer_question(self, question, k=5):\n",
    "        \"\"\"\n",
    "        Answer a question using RAG.\n",
    "        \n",
    "        Args:\n",
    "            question: The question to answer\n",
    "            k: Number of chunks to retrieve\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with retrieved context and metadata\n",
    "        \"\"\"\n",
    "        # Determine if this is a structured or unstructured data question\n",
    "        query_type = self.classify_query(question)\n",
    "        \n",
    "        # Retrieve relevant content\n",
    "        relevant_content = self.search(question, k=k, query_type=query_type)\n",
    "        \n",
    "        # Combine context\n",
    "        context = \"\\n\\n\".join([f\"From {item['source_file']}:\\n{item['text']}\" \n",
    "                              for item in relevant_content])\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'retrieved_content': relevant_content,\n",
    "            'context': context,\n",
    "            'query_type': query_type\n",
    "        }\n",
    "    \n",
    "    def extract_structured_data(self, filename, column=None, filters=None):\n",
    "        \"\"\"\n",
    "        Extract specific data from structured datasets.\n",
    "        \n",
    "        Args:\n",
    "            filename: Name of the dataset file\n",
    "            column: Specific column to extract\n",
    "            filters: Dictionary of column:value pairs to filter the data\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame or Series with the extracted data\n",
    "        \"\"\"\n",
    "        if not self.structured_data:\n",
    "            return None\n",
    "        \n",
    "        # Find the dataset\n",
    "        dataset = None\n",
    "        for item in self.structured_data:\n",
    "            if item['filename'] == filename:\n",
    "                dataset = item['data']\n",
    "                break\n",
    "        \n",
    "        if dataset is None:\n",
    "            return None\n",
    "        \n",
    "        # Apply filters\n",
    "        if filters:\n",
    "            for col, value in filters.items():\n",
    "                if col in dataset.columns:\n",
    "                    dataset = dataset[dataset[col] == value]\n",
    "        \n",
    "        # Extract specific column\n",
    "        if column and column in dataset.columns:\n",
    "            return dataset[column]\n",
    "        \n",
    "        return dataset"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T22:33:30.690932Z",
     "start_time": "2025-04-21T22:33:30.674626Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "class FineTuningDataGenerator:\n",
    "    \"\"\"Generate data for fine-tuning LLMs on real estate content\"\"\"\n",
    "    \n",
    "    def __init__(self, text_chunks_path=\"processed_chunks.pkl\", output_dir=\"fine_tuning_data\"):\n",
    "        self.text_chunks_path = text_chunks_path\n",
    "        self.output_dir = output_dir\n",
    "        self.chunks_df = None\n",
    "        \n",
    "        # Load chunks data\n",
    "        if os.path.exists(text_chunks_path):\n",
    "            self.chunks_df = pd.read_pickle(text_chunks_path)\n",
    "            print(f\"Loaded {len(self.chunks_df)} text chunks for fine-tuning data generation\")\n",
    "        \n",
    "        # Create output directory if needed\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "    \n",
    "    def generate_qa_pairs(self, num_pairs=100, min_context_length=200):\n",
    "        \"\"\"\n",
    "        Generate question-answer pairs for fine-tuning based on chunks.\n",
    "        \n",
    "        This creates synthetic QA pairs from the text chunks.\n",
    "        \"\"\"\n",
    "        if self.chunks_df is None:\n",
    "            print(\"No chunks data available. Please check the path.\")\n",
    "            return\n",
    "        \n",
    "        # Filter chunks by minimum length\n",
    "        valid_chunks = self.chunks_df[self.chunks_df['text'].str.len() > min_context_length]\n",
    "        if len(valid_chunks) < num_pairs:\n",
    "            print(f\"Warning: Only {len(valid_chunks)} valid chunks available.\")\n",
    "            num_pairs = len(valid_chunks)\n",
    "        \n",
    "        # Sample random chunks\n",
    "        selected_chunks = valid_chunks.sample(num_pairs)\n",
    "        \n",
    "        qa_pairs = []\n",
    "        \n",
    "        question_templates = [\n",
    "            \"What does the document say about {topic}?\",\n",
    "            \"Can you explain {topic} based on the text?\",\n",
    "            \"What information is provided about {topic}?\",\n",
    "            \"How does the document describe {topic}?\",\n",
    "            \"What are the key points mentioned about {topic}?\"\n",
    "        ]\n",
    "        \n",
    "        # Extract topics from chunks and create QA pairs\n",
    "        for _, chunk in tqdm(selected_chunks.iterrows(), total=len(selected_chunks), \n",
    "                            desc=\"Generating QA pairs\"):\n",
    "            # Extract potential topic words (nouns) using simple heuristics\n",
    "            # In a real implementation, you might use NLP libraries for better extraction\n",
    "            text = chunk['text']\n",
    "            words = text.split()\n",
    "            \n",
    "            # Find potential topic words (longer words, likely to be meaningful)\n",
    "            potential_topics = [word for word in words \n",
    "                               if len(word) > 5 and word.isalpha()]\n",
    "            \n",
    "            if not potential_topics:\n",
    "                # If no good topics found, use generic template\n",
    "                question = \"What information is provided in this text?\"\n",
    "                answer = text\n",
    "            else:\n",
    "                # Select a random topic and question template\n",
    "                topic = np.random.choice(potential_topics)\n",
    "                question_template = np.random.choice(question_templates)\n",
    "                question = question_template.format(topic=topic)\n",
    "                answer = text\n",
    "            \n",
    "            qa_pairs.append({\n",
    "                'question': question,\n",
    "                'answer': answer,\n",
    "                'source': chunk['source_file']\n",
    "            })\n",
    "        \n",
    "        # Save the generated QA pairs\n",
    "        output_path = os.path.join(self.output_dir, \"qa_pairs.json\")\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(qa_pairs, f, indent=2)\n",
    "        \n",
    "        print(f\"Generated {len(qa_pairs)} QA pairs and saved to {output_path}\")\n",
    "        return qa_pairs\n",
    "    \n",
    "    def generate_fine_tuning_formats(self, qa_pairs=None, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Format QA pairs for different fine-tuning methods.\n",
    "        \n",
    "        Generates formats for:\n",
    "        1. OpenAI fine-tuning (JSONL)\n",
    "        2. Hugging Face fine-tuning (CSV)\n",
    "        \"\"\"\n",
    "        if qa_pairs is None:\n",
    "            # Try to load QA pairs\n",
    "            qa_path = os.path.join(self.output_dir, \"qa_pairs.json\")\n",
    "            if os.path.exists(qa_path):\n",
    "                with open(qa_path, 'r') as f:\n",
    "                    qa_pairs = json.load(f)\n",
    "            else:\n",
    "                print(\"No QA pairs provided or found. Please generate QA pairs first.\")\n",
    "                return\n",
    "        \n",
    "        # Split into train and test sets\n",
    "        train_pairs, test_pairs = train_test_split(qa_pairs, test_size=test_size, random_state=42)\n",
    "        \n",
    "        # 1. OpenAI fine-tuning format (JSONL)\n",
    "        openai_train = []\n",
    "        openai_test = []\n",
    "        \n",
    "        for pair in train_pairs:\n",
    "            openai_train.append({\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in real estate.\"},\n",
    "                    {\"role\": \"user\", \"content\": pair['question']},\n",
    "                    {\"role\": \"assistant\", \"content\": pair['answer']}\n",
    "                ]\n",
    "            })\n",
    "        \n",
    "        for pair in test_pairs:\n",
    "            openai_test.append({\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in real estate.\"},\n",
    "                    {\"role\": \"user\", \"content\": pair['question']},\n",
    "                    {\"role\": \"assistant\", \"content\": pair['answer']}\n",
    "                ]\n",
    "            })\n",
    "        \n",
    "        # Save OpenAI format\n",
    "        with open(os.path.join(self.output_dir, \"openai_train.jsonl\"), 'w') as f:\n",
    "            for item in openai_train:\n",
    "                f.write(json.dumps(item) + '\\n')\n",
    "        \n",
    "        with open(os.path.join(self.output_dir, \"openai_test.jsonl\"), 'w') as f:\n",
    "            for item in openai_test:\n",
    "                f.write(json.dumps(item) + '\\n')\n",
    "        \n",
    "        # 2. Hugging Face fine-tuning format (CSV)\n",
    "        hf_train_data = pd.DataFrame({\n",
    "            'input': [pair['question'] for pair in train_pairs],\n",
    "            'output': [pair['answer'] for pair in train_pairs]\n",
    "        })\n",
    "        \n",
    "        hf_test_data = pd.DataFrame({\n",
    "            'input': [pair['question'] for pair in test_pairs],\n",
    "            'output': [pair['answer'] for pair in test_pairs]\n",
    "        })\n",
    "        \n",
    "        # Save Hugging Face format\n",
    "        hf_train_data.to_csv(os.path.join(self.output_dir, \"hf_train.csv\"), index=False)\n",
    "        hf_test_data.to_csv(os.path.join(self.output_dir, \"hf_test.csv\"), index=False)\n",
    "        \n",
    "        print(f\"Created fine-tuning datasets in multiple formats:\")\n",
    "        print(f\"OpenAI: {len(openai_train)} training, {len(openai_test)} testing examples\")\n",
    "        print(f\"Hugging Face: {len(hf_train_data)} training, {len(hf_test_data)} testing examples\")\n",
    "        \n",
    "        return {\n",
    "            'openai': {'train': openai_train, 'test': openai_test},\n",
    "            'huggingface': {'train': hf_train_data, 'test': hf_test_data}\n",
    "        }"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T22:33:35.216151Z",
     "start_time": "2025-04-21T22:33:35.100225Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from enhanced_rag import EnhancedRealEstateRAG\n",
    "\n",
    "class RealEstateChatbot:\n",
    "    def __init__(self, rag_system=None):\n",
    "        \"\"\"\n",
    "        Initialize the chatbot with a RAG system.\n",
    "        \n",
    "        Args:\n",
    "            rag_system: An initialized EnhancedRealEstateRAG object\n",
    "        \"\"\"\n",
    "        # Initialize RAG if not provided\n",
    "        if rag_system is None:\n",
    "            print(\"Initializing new RAG system...\")\n",
    "            self.rag = EnhancedRealEstateRAG()\n",
    "            \n",
    "            # Load embeddings and build index\n",
    "            if os.path.exists(\"text_embeddings.pkl\"):\n",
    "                self.rag.load_embeddings()\n",
    "                self.rag.build_indices()\n",
    "            else:\n",
    "                print(\"No embeddings found. Please create embeddings first.\")\n",
    "                return\n",
    "        else:\n",
    "            self.rag = rag_system\n",
    "        \n",
    "        self.conversation_history = []\n",
    "        self.last_query_type = None\n",
    "        self.last_structured_data_file = None\n",
    "    \n",
    "    def answer(self, query, k=5, show_context=False, query_type=None):\n",
    "        \"\"\"\n",
    "        Answer a user question by retrieving relevant context.\n",
    "        \n",
    "        Args:\n",
    "            query: The user's question\n",
    "            k: Number of contexts to retrieve\n",
    "            show_context: Whether to display the retrieved context\n",
    "            query_type: Force a specific query type (\"text\", \"structured\", or \"both\")\n",
    "            \n",
    "        Returns:\n",
    "            A formatted response\n",
    "        \"\"\"\n",
    "        # Save the user's query to conversation history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": query})\n",
    "        \n",
    "        # Check for special commands\n",
    "        if query.lower().startswith(\"analyze \"):\n",
    "            return self._handle_analysis_command(query[8:])\n",
    "        \n",
    "        # Retrieve relevant information\n",
    "        retrieval_results = self.rag.answer_question(query, k=k)\n",
    "        context = retrieval_results['context']\n",
    "        self.last_query_type = retrieval_results['query_type']\n",
    "        \n",
    "        # Show the retrieved context if requested\n",
    "        if show_context:\n",
    "            print(\"RETRIEVED CONTEXT:\")\n",
    "            print(\"-\" * 80)\n",
    "            print(context)\n",
    "            print(\"-\" * 80)\n",
    "            print()\n",
    "        \n",
    "        # Get sources for citation\n",
    "        sources = []\n",
    "        structured_files = []\n",
    "        for item in retrieval_results['retrieved_content']:\n",
    "            source = item['source_file']\n",
    "            if source not in sources:\n",
    "                sources.append(source)\n",
    "            \n",
    "            # Track structured data files for follow-up\n",
    "            if item.get('type') in ['structured', 'structured_column']:\n",
    "                if source not in structured_files:\n",
    "                    structured_files.append(source)\n",
    "                    self.last_structured_data_file = source\n",
    "        \n",
    "        # Format response based on query type\n",
    "        if self.last_query_type == \"structured\":\n",
    "            response = self._format_structured_response(retrieval_results, structured_files)\n",
    "        else:\n",
    "            response = self._format_text_response(retrieval_results)\n",
    "        \n",
    "        # Add the response to conversation history\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _format_text_response(self, retrieval_results):\n",
    "        \"\"\"Format response for text-based queries.\"\"\"\n",
    "        response = \"Based on the real estate documents I've analyzed:\\n\\n\"\n",
    "        \n",
    "        # Group by source file for better organization\n",
    "        by_source = {}\n",
    "        for item in retrieval_results['retrieved_content']:\n",
    "            source = item['source_file']\n",
    "            if source not in by_source:\n",
    "                by_source[source] = []\n",
    "            by_source[source].append(item['text'])\n",
    "        \n",
    "        # Build response from each source\n",
    "        for source, texts in by_source.items():\n",
    "            response += f\"From {source}:\\n\"\n",
    "            \n",
    "            # Join and truncate content from this source\n",
    "            combined_text = \"\\n\".join(texts)\n",
    "            truncated = combined_text[:500] + \"...\" if len(combined_text) > 500 else combined_text\n",
    "            response += f\"{truncated}\\n\\n\"\n",
    "        \n",
    "        # Add sources\n",
    "        sources = list(by_source.keys())\n",
    "        response += f\"Sources: {', '.join(sources)}\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _format_structured_response(self, retrieval_results, structured_files):\n",
    "        \"\"\"Format response for structured data queries.\"\"\"\n",
    "        response = \"Based on the real estate data analysis:\\n\\n\"\n",
    "        \n",
    "        for item in retrieval_results['retrieved_content']:\n",
    "            if item.get('type') in ['structured', 'structured_column']:\n",
    "                response += f\"From {item['source_file']}:\\n\"\n",
    "                response += f\"{item['text']}\\n\\n\"\n",
    "        \n",
    "        # Add follow-up hint\n",
    "        if structured_files:\n",
    "            response += f\"You can ask me to analyze specific aspects of the {structured_files[0]} dataset. \"\n",
    "            response += f\"For example, try 'analyze {structured_files[0]} by price range'.\\n\\n\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _handle_analysis_command(self, command):\n",
    "        \"\"\"Handle special analysis commands for structured data.\"\"\"\n",
    "        # Parse command to extract file and analysis type\n",
    "        parts = command.strip().split()\n",
    "        if len(parts) < 2:\n",
    "            return \"Please specify what to analyze. For example: 'analyze dataset.csv by price'\"\n",
    "        \n",
    "        filename = parts[0]\n",
    "        analysis_type = \" \".join(parts[1:])\n",
    "        \n",
    "        # Find the structured dataset\n",
    "        if not self.rag.structured_data:\n",
    "            return \"No structured data is available for analysis.\"\n",
    "        \n",
    "        dataset = None\n",
    "        for item in self.rag.structured_data:\n",
    "            if item['filename'] == filename:\n",
    "                dataset = item['data']\n",
    "                break\n",
    "        \n",
    "        if dataset is None:\n",
    "            return f\"Dataset '{filename}' not found. Available datasets: \" + \\\n",
    "                   \", \".join([item['filename'] for item in self.rag.structured_data])\n",
    "        \n",
    "        # Perform different analyses based on the command\n",
    "        try:\n",
    "            if \"summary\" in analysis_type.lower():\n",
    "                return self._generate_summary_stats(dataset, filename)\n",
    "            elif \"price range\" in analysis_type.lower() or \"price distribution\" in analysis_type.lower():\n",
    "                return self._analyze_price_distribution(dataset, filename)\n",
    "            elif \"location\" in analysis_type.lower() or \"area\" in analysis_type.lower():\n",
    "                return self._analyze_by_location(dataset, filename)\n",
    "            elif \"trend\" in analysis_type.lower() or \"time\" in analysis_type.lower():\n",
    "                return self._analyze_time_trends(dataset, filename)\n",
    "            else:\n",
    "                return f\"I'm not sure how to analyze {filename} by {analysis_type}. \" + \\\n",
    "                       \"Try asking for a summary, price range, location analysis, or time trends.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error analyzing {filename}: {str(e)}\"\n",
    "    \n",
    "    def _generate_summary_stats(self, df, filename):\n",
    "        \"\"\"Generate summary statistics for a dataset.\"\"\"\n",
    "        response = f\"## Summary Statistics for {filename}\\n\\n\"\n",
    "        \n",
    "        # Basic dataset info\n",
    "        response += f\"This dataset contains {len(df)} properties with {len(df.columns)} attributes.\\n\\n\"\n",
    "        \n",
    "        # Try to identify numeric columns for statistics\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        if numeric_cols:\n",
    "            response += \"### Key Metrics\\n\\n\"\n",
    "            \n",
    "            # Look for price/value columns\n",
    "            price_cols = [col for col in numeric_cols \n",
    "                         if any(term in col.lower() for term in ['price', 'value', 'cost', 'amount'])]\n",
    "            \n",
    "            if price_cols:\n",
    "                for col in price_cols[:2]:  # Limit to first 2 price columns\n",
    "                    response += f\"**{col}**:\\n\"\n",
    "                    response += f\"- Average: ${df[col].mean():.2f}\\n\"\n",
    "                    response += f\"- Median: ${df[col].median():.2f}\\n\"\n",
    "                    response += f\"- Range: ${df[col].min():.2f} to ${df[col].max():.2f}\\n\\n\"\n",
    "            \n",
    "            # Look for size/area columns\n",
    "            size_cols = [col for col in numeric_cols \n",
    "                        if any(term in col.lower() for term in ['size', 'area', 'sqft', 'feet', 'acre'])]\n",
    "            \n",
    "            if size_cols:\n",
    "                for col in size_cols[:2]:  # Limit to first 2 size columns\n",
    "                    response += f\"**{col}**:\\n\"\n",
    "                    response += f\"- Average: {df[col].mean():.2f}\\n\"\n",
    "                    response += f\"- Median: {df[col].median():.2f}\\n\"\n",
    "                    response += f\"- Range: {df[col].min():.2f} to {df[col].max():.2f}\\n\\n\"\n",
    "        \n",
    "        # Try to identify categorical columns\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "        if categorical_cols:\n",
    "            # Look for location columns\n",
    "            location_cols = [col for col in categorical_cols \n",
    "                           if any(term in col.lower() for term in ['city', 'state', 'zip', 'location', 'address'])]\n",
    "            \n",
    "            if location_cols:\n",
    "                for col in location_cols[:1]:  # Limit to first location column\n",
    "                    top_locations = df[col].value_counts().nlargest(5)\n",
    "                    response += f\"**Top {col}**:\\n\"\n",
    "                    for loc, count in top_locations.items():\n",
    "                        response += f\"- {loc}: {count} properties ({count/len(df)*100:.1f}%)\\n\"\n",
    "                    response += \"\\n\"\n",
    "            \n",
    "            # Look for property type columns\n",
    "            type_cols = [col for col in categorical_cols \n",
    "                       if any(term in col.lower() for term in ['type', 'style', 'property', 'category'])]\n",
    "            \n",
    "            if type_cols:\n",
    "                for col in type_cols[:1]:  # Limit to first type column\n",
    "                    top_types = df[col].value_counts().nlargest(5)\n",
    "                    response += f\"**Top {col}**:\\n\"\n",
    "                    for typ, count in top_types.items():\n",
    "                        response += f\"- {typ}: {count} properties ({count/len(df)*100:.1f}%)\\n\"\n",
    "                    response += \"\\n\"\n",
    "        \n",
    "        response += \"You can ask for more specific analyses like 'analyze \" + \\\n",
    "                    f\"{filename} by price range' or 'analyze {filename} by location'\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _analyze_price_distribution(self, df, filename):\n",
    "        \"\"\"Analyze price distribution in the dataset.\"\"\"\n",
    "        # Try to find price column\n",
    "        price_cols = [col for col in df.columns \n",
    "                     if any(term in col.lower() for term in ['price', 'value', 'cost', 'amount'])]\n",
    "        \n",
    "        if not price_cols:\n",
    "            return f\"Could not identify a price column in {filename}.\"\n",
    "        \n",
    "        price_col = price_cols[0]  # Use the first price column found\n",
    "        \n",
    "        # Ensure the column is numeric\n",
    "        if not pd.api.types.is_numeric_dtype(df[price_col]):\n",
    "            return f\"The column {price_col} is not numeric, cannot analyze price distribution.\"\n",
    "        \n",
    "        response = f\"## Price Distribution Analysis for {filename}\\n\\n\"\n",
    "        \n",
    "        # Calculate price ranges\n",
    "        min_price = df[price_col].min()\n",
    "        max_price = df[price_col].max()\n",
    "        \n",
    "        response += f\"Price range from ${min_price:.2f} to ${max_price:.2f}\\n\\n\"\n",
    "        \n",
    "        # Create price brackets\n",
    "        brackets = 5\n",
    "        bracket_size = (max_price - min_price) / brackets\n",
    "        \n",
    "        response += \"### Price Brackets\\n\\n\"\n",
    "        for i in range(brackets):\n",
    "            lower = min_price + i * bracket_size\n",
    "            upper = lower + bracket_size\n",
    "            count = df[(df[price_col] >= lower) & (df[price_col] < upper)].shape[0]\n",
    "            percentage = count / len(df) * 100\n",
    "            \n",
    "            response += f\"- ${lower:.2f} to ${upper:.2f}: {count} properties ({percentage:.1f}%)\\n\"\n",
    "        \n",
    "        # Properties above the highest bracket\n",
    "        count = df[df[price_col] >= max_price].shape[0]\n",
    "        percentage = count / len(df) * 100\n",
    "        response += f\"- ${max_price:.2f} and above: {count} properties ({percentage:.1f}%)\\n\\n\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _analyze_by_location(self, df, filename):\n",
    "        \"\"\"Analyze properties by location.\"\"\"\n",
    "        # Try to find location columns\n",
    "        location_cols = [col for col in df.columns \n",
    "                       if any(term in col.lower() for term in ['city', 'state', 'zip', 'location', 'address'])]\n",
    "        \n",
    "        if not location_cols:\n",
    "            return f\"Could not identify a location column in {filename}.\"\n",
    "        \n",
    "        location_col = location_cols[0]  # Use the first location column found\n",
    "        \n",
    "        response = f\"## Location Analysis for {filename}\\n\\n\"\n",
    "        \n",
    "        # Count properties by location\n",
    "        location_counts = df[location_col].value_counts().nlargest(10)\n",
    "        \n",
    "        response += \"### Top 10 Locations\\n\\n\"\n",
    "        for location, count in location_counts.items():\n",
    "            percentage = count / len(df) * 100\n",
    "            response += f\"- {location}: {count} properties ({percentage:.1f}%)\\n\"\n",
    "        \n",
    "        # Try to find price column to analyze price by location\n",
    "        price_cols = [col for col in df.columns \n",
    "                     if any(term in col.lower() for term in ['price', 'value', 'cost', 'amount'])]\n",
    "        \n",
    "        if price_cols and pd.api.types.is_numeric_dtype(df[price_cols[0]]):\n",
    "            price_col = price_cols[0]\n",
    "            response += f\"\\n### Average Prices by Top 5 Locations\\n\\n\"\n",
    "            \n",
    "            for location in location_counts.index[:5]:\n",
    "                avg_price = df[df[location_col] == location][price_col].mean()\n",
    "                response += f\"- {location}: ${avg_price:.2f}\\n\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _analyze_time_trends(self, df, filename):\n",
    "        \"\"\"Analyze trends over time.\"\"\"\n",
    "        # Try to find date columns\n",
    "        date_cols = [col for col in df.columns \n",
    "                   if any(term in col.lower() for term in ['date', 'year', 'month', 'time'])]\n",
    "        \n",
    "        if not date_cols:\n",
    "            return f\"Could not identify a date/time column in {filename}.\"\n",
    "        \n",
    "        date_col = date_cols[0]  # Use the first date column found\n",
    "        \n",
    "        # Try to convert to datetime if not already\n",
    "        try:\n",
    "            if not pd.api.types.is_datetime64_dtype(df[date_col]):\n",
    "                df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "        except:\n",
    "            return f\"Could not convert {date_col} to a valid date format.\"\n",
    "        \n",
    "        # Drop rows with invalid dates\n",
    "        df = df.dropna(subset=[date_col])\n",
    "        \n",
    "        response = f\"## Time Trend Analysis for {filename}\\n\\n\"\n",
    "        \n",
    "        # Extract year and month\n",
    "        df['year'] = df[date_col].dt.year\n",
    "        \n",
    "        # Count by year\n",
    "        year_counts = df['year'].value_counts().sort_index()\n",
    "        \n",
    "        response += \"### Properties by Year\\n\\n\"\n",
    "        for year, count in year_counts.items():\n",
    "            percentage = count / len(df) * 100\n",
    "            response += f\"- {year}: {count} properties ({percentage:.1f}%)\\n\"\n",
    "        \n",
    "        # Try to find price column to analyze price trends\n",
    "        price_cols = [col for col in df.columns \n",
    "                     if any(term in col.lower() for term in ['price', 'value', 'cost', 'amount'])]\n",
    "        \n",
    "        if price_cols and pd.api.types.is_numeric_dtype(df[price_cols[0]]):\n",
    "            price_col = price_cols[0]\n",
    "            response += f\"\\n### Average Prices by Year\\n\\n\"\n",
    "            \n",
    "            price_by_year = df.groupby('year')[price_col].mean()\n",
    "            \n",
    "            for year, avg_price in price_by_year.items():\n",
    "                response += f\"- {year}: ${avg_price:.2f}\\n\"\n",
    "            \n",
    "            # Calculate price change between first and last year\n",
    "            if len(price_by_year) > 1:\n",
    "                first_year = price_by_year.index[0]\n",
    "                last_year = price_by_year.index[-1]\n",
    "                price_change = ((price_by_year[last_year] / price_by_year[first_year]) - 1) * 100\n",
    "                \n",
    "                response += f\"\\nPrice change from {first_year} to {last_year}: {price_change:.1f}%\\n\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def chat(self):\n",
    "        \"\"\"Start an interactive chat session.\"\"\"\n",
    "        print(\"Welcome to RealEstateGPT! Ask me anything about real estate.\")\n",
    "        print(\"Type 'exit' to end the conversation.\\n\")\n",
    "        \n",
    "        while True:\n",
    "            query = input(\"You: \")\n",
    "            if query.lower() in ['exit', 'quit', 'bye']:\n",
    "                print(\"RealEstateGPT: Goodbye! Hope I was helpful.\")\n",
    "                break\n",
    "            \n",
    "            answer = self.answer(query, show_context=False)\n",
    "            print(f\"\\nRealEstateGPT: {answer}\\n\")"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'enhanced_rag'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[34]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mre\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpickle\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01menhanced_rag\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m EnhancedRealEstateRAG\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mRealEstateChatbot\u001B[39;00m:\n\u001B[32m      9\u001B[39m     \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, rag_system=\u001B[38;5;28;01mNone\u001B[39;00m):\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'enhanced_rag'"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T22:34:03.774965Z",
     "start_time": "2025-04-21T22:34:03.707348Z"
    }
   },
   "source": [
    "#MAIN APPLICATION\n",
    "import os\n",
    "import argparse\n",
    "from data_processors import DataProcessor, StructuredDataProcessor, DocProcessor\n",
    "from enhanced_rag import EnhancedRealEstateRAG\n",
    "from real_estate_chatbot import RealEstateChatbot\n",
    "from fine_tuning import FineTuningDataGenerator\n",
    "\n",
    "def process_all_data(data_dir=\"data\", force_reprocess=False):\n",
    "    \"\"\"Process all data sources and generate necessary files.\"\"\"\n",
    "    # Check if processed files already exist\n",
    "    text_chunks_exists = os.path.exists(\"processed_chunks.pkl\")\n",
    "    structured_data_exists = os.path.exists(\"processed_structured_data.pkl\")\n",
    "    \n",
    "    if text_chunks_exists and structured_data_exists and not force_reprocess:\n",
    "        print(\"Processed data files already exist. Use --force to reprocess.\")\n",
    "        return\n",
    "    \n",
    "    # Process PDF files (using your existing code)\n",
    "    if not text_chunks_exists or force_reprocess:\n",
    "        print(\"Processing PDF files...\")\n",
    "        # You can call your existing functions here\n",
    "        # We'll assume they create processed_chunks.pkl\n",
    "    \n",
    "    # Process structured data\n",
    "    if not structured_data_exists or force_reprocess:\n",
    "        print(\"Processing structured data...\")\n",
    "        structured_processor = StructuredDataProcessor(data_dir)\n",
    "        structured_data = structured_processor.process_all()\n",
    "        structured_processor.save_processed_data(structured_data, \"processed_structured_data.pkl\")\n",
    "        print(f\"Processed {len(structured_data)} structured data files\")\n",
    "    \n",
    "    # Process Word documents\n",
    "    print(\"Processing Word documents...\")\n",
    "    doc_processor = DocProcessor(data_dir)\n",
    "    doc_data = doc_processor.process_all()\n",
    "    doc_processor.save_processed_data(doc_data, \"processed_doc_data.pkl\")\n",
    "    print(f\"Processed {len(doc_data)} Word documents\")\n",
    "\n",
    "def create_embeddings(force_recreate=False):\n",
    "    \"\"\"Create embeddings for all data sources.\"\"\"\n",
    "    text_embeddings_exists = os.path.exists(\"text_embeddings.pkl\")\n",
    "    structured_embeddings_exists = os.path.exists(\"structured_embeddings.pkl\")\n",
    "    \n",
    "    if text_embeddings_exists and structured_embeddings_exists and not force_recreate:\n",
    "        print(\"Embedding files already exist. Use --force to recreate.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize RAG system\n",
    "    rag = EnhancedRealEstateRAG()\n",
    "    \n",
    "    # Create embeddings\n",
    "    rag.create_embeddings()\n",
    "\n",
    "def prepare_fine_tuning_data(num_pairs=1000):\n",
    "    \"\"\"Prepare data for fine-tuning experiments.\"\"\"\n",
    "    print(f\"Generating fine-tuning data with {num_pairs} QA pairs...\")\n",
    "    \n",
    "    # Initialize data generator\n",
    "    generator = FineTuningDataGenerator()\n",
    "    \n",
    "    # Generate QA pairs\n",
    "    qa_pairs = generator.generate_qa_pairs(num_pairs=num_pairs)\n",
    "    \n",
    "    # Format for different fine-tuning approaches\n",
    "    generator.generate_fine_tuning_formats(qa_pairs)\n",
    "\n",
    "def start_chatbot():\n",
    "    \"\"\"Initialize and start the chatbot.\"\"\"\n",
    "    print(\"Initializing Real Estate Chatbot...\")\n",
    "    \n",
    "    # Initialize RAG system\n",
    "    rag = EnhancedRealEstateRAG()\n",
    "    rag.load_embeddings()\n",
    "    rag.build_indices()\n",
    "    \n",
    "    # Initialize chatbot\n",
    "    chatbot = RealEstateChatbot(rag)\n",
    "    \n",
    "    # Start interactive chat\n",
    "    chatbot.chat()\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='RealEstateLLM - AI-powered real estate chatbot')\n",
    "    \n",
    "    parser.add_argument('--process', action='store_true', \n",
    "                        help='Process all data sources')\n",
    "    parser.add_argument('--embeddings', action='store_true', \n",
    "                        help='Create embeddings for all data')\n",
    "    parser.add_argument('--finetune', action='store_true', \n",
    "                        help='Prepare data for fine-tuning')\n",
    "    parser.add_argument('--chat', action='store_true', \n",
    "                        help='Start the chatbot')\n",
    "    parser.add_argument('--force', action='store_true', \n",
    "                        help='Force reprocessing of existing files')\n",
    "    parser.add_argument('--qa-pairs', type=int, default=1000, \n",
    "                        help='Number of QA pairs to generate for fine-tuning')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # If no arguments, show help\n",
    "    if not any(vars(args).values()):\n",
    "        parser.print_help()\n",
    "        return\n",
    "    \n",
    "    # Process data if requested\n",
    "    if args.process:\n",
    "        process_all_data(force_reprocess=args.force)\n",
    "    \n",
    "    # Create embeddings if requested\n",
    "    if args.embeddings:\n",
    "        create_embeddings(force_recreate=args.force)\n",
    "    \n",
    "    # Prepare fine-tuning data if requested\n",
    "    if args.finetune:\n",
    "        prepare_fine_tuning_data(num_pairs=args.qa_pairs)\n",
    "    \n",
    "    # Start chatbot if requested\n",
    "    if args.chat:\n",
    "        start_chatbot()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_processors'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[35]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mos\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01margparse\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mdata_processors\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DataProcessor, StructuredDataProcessor, DocProcessor\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01menhanced_rag\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m EnhancedRealEstateRAG\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mreal_estate_chatbot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m RealEstateChatbot\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'data_processors'"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
