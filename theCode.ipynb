{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install packages if necessary\n",
    "#!pip install pandas numpy faiss-cpu PyPDF2 sentence-transformers python-docx tqdm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   4%|▍         | 1/23 [00:19<07:15, 19.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 4000.1hsghhdbk103123.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   9%|▊         | 2/23 [00:20<02:55,  8.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed CCL_BuyersGuide.pdf\n",
      "Successfully processed renting-vs-owning.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  22%|██▏       | 5/23 [00:20<00:41,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed RS20530.pdf\n",
      "Successfully processed FHA-Reference-Guide-2023.pdf\n",
      "Successfully processed ort-ss-realestatedictionary.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  30%|███       | 7/23 [00:20<00:20,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed TJC_ebook_fha-homeloan.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  35%|███▍      | 8/23 [00:21<00:17,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed GeneralGlossary.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  39%|███▉      | 9/23 [00:22<00:12,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed GLOSSARY_OF_REAL_ESTATE_TERMS.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  43%|████▎     | 10/23 [00:22<00:11,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed Home_Buyers_Guide.pdf\n",
      "Successfully processed consumer-guide-buying-your-first-home-2024-11-05.pdf\n",
      "Successfully processed FHA_loan_guidelines.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  57%|█████▋    | 13/23 [00:23<00:04,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 2024_Zillow_Rent-vs-Buy.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  61%|██████    | 14/23 [00:23<00:04,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed guide_firsttimehomebuying-2.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  65%|██████▌   | 15/23 [00:24<00:03,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 1507.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  70%|██████▉   | 16/23 [00:25<00:04,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed HL_Buyers_Guide_FINAL_March2019.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  78%|███████▊  | 18/23 [00:25<00:02,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed NAHREP-Glossary-of-Real-Estate-Industry-Terms.pdf\n",
      "Successfully processed naiop-2024-terms-and-definitions.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  87%|████████▋ | 20/23 [00:26<00:00,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed First-TIme-HomeBuyer-Guide.pdf\n",
      "Successfully processed realestateglossary.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  91%|█████████▏| 21/23 [00:26<00:00,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed home-buyers-guide-1.pdf\n",
      "Successfully processed renting-vs-buying-study-press-release.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 23/23 [00:26<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed First-TIme-HomeBuyer-Guide-2.pdf\n",
      "Processed 23 PDF files and saved to extracted_pdf_texts.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text() + \"\\n\\n\"\n",
    "        return text\n",
    "\n",
    "def process_pdfs(directory):\n",
    "    \"\"\"Process all PDFs in the given directory.\"\"\"\n",
    "    results = []\n",
    "    pdf_files = [f for f in os.listdir(directory) if f.lower().endswith('.pdf')]\n",
    "    \n",
    "    for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        file_path = os.path.join(directory, pdf_file)\n",
    "        try:\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "            results.append({\n",
    "                'filename': pdf_file,\n",
    "                'text': text,\n",
    "                'size': os.path.getsize(file_path)\n",
    "            })\n",
    "            print(f\"Successfully processed {pdf_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_file}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir = \"data\"\n",
    "    results_df = process_pdfs(data_dir)\n",
    "    results_df.to_pickle(\"extracted_pdf_texts.pkl\")\n",
    "    print(f\"Processed {len(results_df)} PDF files and saved to extracted_pdf_texts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from extracted_pdf_texts.pkl...\n",
      "\n",
      "Dataset contains 23 documents\n",
      "Columns: ['filename', 'text', 'size']\n",
      "\n",
      "Documents with empty text: 0\n",
      "\n",
      "Text length statistics:\n",
      "count    2.300000e+01\n",
      "mean     2.255588e+05\n",
      "std      8.347313e+05\n",
      "min      2.083000e+03\n",
      "25%      2.211300e+04\n",
      "50%      3.974200e+04\n",
      "75%      8.795000e+04\n",
      "max      4.047940e+06\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "3 random samples (first 500 chars):\n",
      "\n",
      "Sample 1 from 'CCL_BuyersGuide.pdf':\n",
      "--------------------------------------------------------------------------------\n",
      "Home \n",
      "Buyer’s \n",
      "Guide\n",
      "\n",
      "1\n",
      "\n",
      "2About Us\n",
      "Corcoran Classic Living is a top- performing \n",
      "residential and commercial real estate firm \n",
      "serving greater Athens, GA. Our agents \n",
      "are socially-minded and deeply entrenched \n",
      "in their communities, backed by a top-\n",
      "notch support staff, global connections, \n",
      "and innovative technologies that ensure a \n",
      "seamless experience. Our mission is to treat \n",
      "our customers and clients as the lifeblood of \n",
      "our business, keep their satisfaction our top \n",
      "priority, and strive for a ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 2 from 'HL_Buyers_Guide_FINAL_March2019.pdf':\n",
      "--------------------------------------------------------------------------------\n",
      "OPEN\n",
      "???\n",
      "FOR\n",
      "SALEA Step-by-Step\n",
      "Guide to \n",
      "Home Buying\n",
      "\n",
      "\n",
      "© Copyright 2019 NATIONAL ASSOCIATION OF REALTORS®HouseLogic.com\n",
      "2A STEP-BY-STEP GUIDE TO HOME BUYING\n",
      "Table of Contents\n",
      "3\n",
      "6\n",
      "9\n",
      "14\n",
      "16\n",
      "21\n",
      "24\n",
      "29\n",
      "33\n",
      "38\n",
      "42\n",
      "47\n",
      "50\n",
      "\n",
      "HouseLogic.com\n",
      "3THE EVERYTHING GUIDE TO\n",
      "Buying Your First Home\n",
      "How to find exactly what you want, and how to work with the experts who’ll help you get it.\n",
      "It’s a big move, literally and figuratively. Buying a house requires a serious amount of money and time. The \n",
      "journey isn’t always e...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample 3 from 'renting-vs-buying-study-press-release.pdf':\n",
      "--------------------------------------------------------------------------------\n",
      "Renting\n",
      "is\n",
      "Now\n",
      "Cheaper\n",
      "Than\n",
      "Buying\n",
      "in\n",
      "All\n",
      "50\n",
      "of\n",
      "the\n",
      "Largest\n",
      "U.S.\n",
      "Metros\n",
      "Metros\n",
      "in\n",
      "the\n",
      "West\n",
      "have\n",
      "the\n",
      "biggest\n",
      "differences\n",
      "between\n",
      "renting\n",
      "and\n",
      "buying\n",
      "costs\n",
      "NEW\n",
      "YORK\n",
      "-\n",
      "April\n",
      "29,\n",
      "2024\n",
      "-\n",
      "Renting\n",
      "a\n",
      "typical\n",
      "home\n",
      "is\n",
      "now\n",
      "cheaper\n",
      "than\n",
      "buying\n",
      "one\n",
      "in\n",
      "all\n",
      "50\n",
      "of\n",
      "the\n",
      "largest\n",
      "U.S.\n",
      "metros\n",
      "in\n",
      "2024,\n",
      "according\n",
      "to\n",
      "a\n",
      "new\n",
      "Bankrate\n",
      "study .\n",
      "Bankrate\n",
      "analyzed\n",
      "typical\n",
      "monthly\n",
      "mortgage\n",
      "payments\n",
      "and\n",
      "typical\n",
      "monthly\n",
      "rent\n",
      "for\n",
      "all\n",
      "homes\n",
      "across\n",
      "all\n",
      "50\n",
      "of\n",
      "the\n",
      "largest\n",
      "metro\n",
      "statistical\n",
      "areas\n",
      "(MSAs)\n",
      "in\n",
      "the\n",
      "United\n",
      "States\n",
      "to\n",
      "compare\n",
      "...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Checking for potential issues:\n",
      "Documents with no spaces (potential OCR issues): 0\n",
      "Documents with non-ASCII characters: 23\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "def check_extracted_data(pkl_path, num_samples=3, sample_length=500):\n",
    "    \"\"\"\n",
    "    Examine the extracted PDF data to check its quality.\n",
    "    \n",
    "    Args:\n",
    "        pkl_path: Path to the pickle file with extracted text\n",
    "        num_samples: Number of random samples to display\n",
    "        sample_length: Number of characters to display from each sample\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    if not os.path.exists(pkl_path):\n",
    "        print(f\"Error: {pkl_path} does not exist.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Loading data from {pkl_path}...\")\n",
    "    df = pd.read_pickle(pkl_path)\n",
    "    \n",
    "    # Print basic information\n",
    "    print(f\"\\nDataset contains {len(df)} documents\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Check for empty text\n",
    "    empty_texts = df[df['text'].str.strip() == ''].shape[0]\n",
    "    print(f\"\\nDocuments with empty text: {empty_texts}\")\n",
    "    \n",
    "    # Check text lengths\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    print(f\"\\nText length statistics:\")\n",
    "    print(df['text_length'].describe())\n",
    "    \n",
    "    # Show some random samples\n",
    "    print(f\"\\n{num_samples} random samples (first {sample_length} chars):\")\n",
    "    sample_indices = random.sample(range(len(df)), min(num_samples, len(df)))\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        doc = df.iloc[idx]\n",
    "        print(f\"\\nSample {i+1} from '{doc['filename']}':\")\n",
    "        print(\"-\" * 80)\n",
    "        print(doc['text'][:sample_length] + \"...\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Check for common issues\n",
    "    print(\"\\nChecking for potential issues:\")\n",
    "    \n",
    "    # Missing spaces between words (possible OCR issue)\n",
    "    no_spaces = df[~df['text'].str.contains(' ', regex=False)].shape[0]\n",
    "    print(f\"Documents with no spaces (potential OCR issues): {no_spaces}\")\n",
    "    \n",
    "    # Unusual characters (possible encoding issues)\n",
    "    unusual_chars = df[df['text'].str.contains('[^\\x00-\\x7F]', regex=True)].shape[0]\n",
    "    print(f\"Documents with non-ASCII characters: {unusual_chars}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_extracted_data(\"extracted_pdf_texts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from extracted_pdf_texts.pkl...\n",
      "Cleaning text data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 116.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking documents into smaller pieces...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking documents: 100%|██████████| 23/23 [00:00<00:00, 1913.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Large document detected: 4000.1hsghhdbk103123.pdf (3893681 chars)\n",
      "Created 4138 chunks from 23 documents\n",
      "Saved processed chunks to processed_chunks.pkl\n",
      "\n",
      "Chunk length statistics:\n",
      "count    4138.000000\n",
      "mean     1241.202272\n",
      "std       527.984130\n",
      "min        99.000000\n",
      "25%      1428.000000\n",
      "50%      1499.000000\n",
      "75%      1500.000000\n",
      "max      1500.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Sample chunks:\n",
      "\n",
      "Chunk 1 from 4000.1hsghhdbk103123.pdf:\n",
      "--------------------------------------------------------------------------------\n",
      "Special Attention of: Transmittal: Handbook 4000.1 All FHA -Approved Mortgagees Issued: October 31, 2023 All Direct Endorsement Underwriters Effective Date: April 29, 2024 All Eligible Submission Sources for Condominium Project Approvals All FHA Roster Appraisers All FHA -Approved 203(k) Consultants...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Chunk 2 from 4000.1hsghhdbk103123.pdf:\n",
      "--------------------------------------------------------------------------------\n",
      "nt of the Appraisal II.D.3 Acceptable Appraisal Reporting Forms and Protocols 2 3. Implementation : These updates do not impact previously announced effective dates for Handbook 4000.1. Below is a list of effective dates for the Handbook changes: Changes identified in previously published Mortgagee ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Chunk 3 from 4000.1hsghhdbk103123.pdf:\n",
      "--------------------------------------------------------------------------------\n",
      "led or superseded. All superseded or canceled policy documents will co ntinue to be available for informational purposes only on HUDs website. Policy documents that have been superseded in full by the Handbook can always be found on HUDs Client Information Policy Systems (HUDCLIPS) web pages, access...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text for NLP tasks.\"\"\"\n",
    "    # Replace common non-ASCII characters \n",
    "    text = text.replace('–', '-').replace('—', '-').replace(''', \"'\").replace(''', \"'\")\n",
    "    text = text.replace('\"', '\"').replace('\"', '\"').replace('…', '...')\n",
    "    \n",
    "    # Normalize unicode characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove excessive newlines\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    # Clean up page numbers and headers/footers (common in PDFs)\n",
    "    text = re.sub(r'\\n\\s*\\d+\\s*\\n', '\\n', text)  # Standalone page numbers\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=100):\n",
    "    \"\"\"Split text into overlapping chunks of approximately chunk_size characters.\"\"\"\n",
    "    if len(text) <= chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    # Add timeout protection\n",
    "    max_iterations = (len(text) // (chunk_size - overlap)) * 2  # Generous upper bound\n",
    "    iteration = 0\n",
    "    \n",
    "    while start < len(text) and iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        \n",
    "        # Limit the search window for breaking points to improve performance\n",
    "        search_start = max(start, end - 200)\n",
    "        \n",
    "        # Try to find a good breaking point (end of sentence or paragraph)\n",
    "        if end < len(text):\n",
    "            # Look for paragraph break first (limit search range)\n",
    "            paragraph_break = text.rfind('\\n\\n', search_start, end)\n",
    "            if paragraph_break != -1:\n",
    "                end = paragraph_break\n",
    "            else:\n",
    "                # Look for sentence break (use a simpler, faster approach)\n",
    "                for marker in ['. ', '! ', '? ']:\n",
    "                    sentence_break = text.rfind(marker, search_start, end)\n",
    "                    if sentence_break != -1:\n",
    "                        end = sentence_break + 2  # +2 to include the punctuation and space\n",
    "                        break\n",
    "        \n",
    "        # Make sure we're making progress\n",
    "        if end <= start:\n",
    "            end = start + chunk_size  # Force progress if no break point found\n",
    "            \n",
    "        chunks.append(text[start:end].strip())\n",
    "        start = end - overlap  # Create overlap between chunks\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_text_data(pkl_path, output_path=None, chunk_size=1500):\n",
    "    \"\"\"Clean, preprocess and chunk text data from PDFs.\"\"\"\n",
    "    # Load the data\n",
    "    print(f\"Loading data from {pkl_path}...\")\n",
    "    df = pd.read_pickle(pkl_path)\n",
    "    \n",
    "    # Clean texts\n",
    "    print(\"Cleaning text data...\")\n",
    "    df['cleaned_text'] = df['text'].progress_apply(clean_text)\n",
    "    \n",
    "    # Chunk texts\n",
    "    print(\"Chunking documents into smaller pieces...\")\n",
    "    all_chunks = []\n",
    "    \n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Chunking documents\"):\n",
    "        try:\n",
    "            # Skip extremely large docs or process them differently\n",
    "            if len(row['cleaned_text']) > 1_000_000:  # 1 million chars\n",
    "                print(f\"⚠️ Large document detected: {row['filename']} ({len(row['cleaned_text'])} chars)\")\n",
    "                # Process large documents in a simpler way (just divide by size)\n",
    "                simple_chunks = [row['cleaned_text'][j:j+chunk_size] \n",
    "                               for j in range(0, len(row['cleaned_text']), chunk_size)]\n",
    "                for j, chunk in enumerate(simple_chunks):\n",
    "                    all_chunks.append({\n",
    "                        'source_file': row['filename'],\n",
    "                        'chunk_id': f\"{row['filename']}_simple_{j}\",\n",
    "                        'text': chunk.strip()\n",
    "                    })\n",
    "                continue\n",
    "            \n",
    "            # Regular chunking for normal sized documents\n",
    "            chunks = chunk_text(row['cleaned_text'], chunk_size=chunk_size)\n",
    "            for j, chunk in enumerate(chunks):\n",
    "                all_chunks.append({\n",
    "                    'source_file': row['filename'],\n",
    "                    'chunk_id': f\"{row['filename']}_{j}\",\n",
    "                    'text': chunk\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['filename']}: {e}\")\n",
    "    \n",
    "    chunks_df = pd.DataFrame(all_chunks)\n",
    "    print(f\"Created {len(chunks_df)} chunks from {len(df)} documents\")\n",
    "    \n",
    "    # Save the processed data\n",
    "    if output_path:\n",
    "        chunks_df.to_pickle(output_path)\n",
    "        print(f\"Saved processed chunks to {output_path}\")\n",
    "    \n",
    "    return chunks_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Add tqdm to pandas operations\n",
    "    tqdm.pandas()\n",
    "    \n",
    "    # Process the data\n",
    "    processed_df = process_text_data(\"extracted_pdf_texts.pkl\", \"processed_chunks.pkl\")\n",
    "    \n",
    "    # Display some statistics\n",
    "    print(\"\\nChunk length statistics:\")\n",
    "    processed_df['text_length'] = processed_df['text'].str.len()\n",
    "    print(processed_df['text_length'].describe())\n",
    "    \n",
    "    # Print a few sample chunks\n",
    "    print(\"\\nSample chunks:\")\n",
    "    for i in range(min(3, len(processed_df))):\n",
    "        print(f\"\\nChunk {i+1} from {processed_df.iloc[i]['source_file']}:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(processed_df.iloc[i]['text'][:300] + \"...\" if len(processed_df.iloc[i]['text']) > 300 else processed_df.iloc[i]['text'])\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4ff9f0a5f54aed8168e61601b470ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820e4738472c49009f2b3e8483544a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e3fc98123a4b9b9c1a6f256ea08ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa899093f574f97a6594d968e40030d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885b4fff858c4048bbbd4664c5b468ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7821b761525c438b91e74d821c480120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9c69683ae84a0596041c4f720cc448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1be03e2df04832849a571c92827ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217de2bca1b147e0afa7f4ca431e4095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0835c269a9ff44038459a4df778a2387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af686cc0a2014b5faddf04dbcfa8d094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chunks from processed_chunks.pkl...\n",
      "Loaded 4138 chunks\n",
      "Creating embeddings using all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9981337d0bdf4e35ba085c4f5e99d07d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating embeddings:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings to embeddings.pkl\n",
      "Building FAISS index...\n",
      "Built index with 4138 vectors\n",
      "\n",
      "Testing retrieval with sample queries:\n",
      "\n",
      "Query: What is an FHA loan?\n",
      "Result 1 from FHA-Reference-Guide-2023.pdf:\n",
      "Distance: 0.6199\n",
      "Text snippet: alify for than conventional mortgages. FHA loans are insured by the Federal Housing Administration. FHA does not lend money or issue cre dit, so the f...\n",
      "Result 2 from RS20530.pdf:\n",
      "Distance: 0.6410\n",
      "Text snippet: ....... ................................ ................................ ......................... 19 FHA-Insured Home Loans: An Overview Congression...\n",
      "\n",
      "Query: Should I rent or buy a house?\n",
      "Result 1 from CCL_BuyersGuide.pdf:\n",
      "Distance: 0.7296\n",
      "Text snippet: pected to rise 10 to 15 percent over the next decade, creating a continued high demand for housing. EQUITY Money paid for rent is money that youll nev...\n",
      "Result 2 from HL_Buyers_Guide_FINAL_March2019.pdf:\n",
      "Distance: 0.7623\n",
      "Text snippet: heres one reason you feel speaks especially to you, circle it with some hearts. Yknow. If you want. Any other reasons?Because Ive always wanted to own...\n",
      "\n",
      "Query: What are closing costs?\n",
      "Result 1 from Home_Buyers_Guide.pdf:\n",
      "Distance: 0.7689\n",
      "Text snippet: rices of similar properties. 8A.Home .Buyers .Glossary H O M E B U Y E R S G U I D E R E A LT O R . C O M | T O P P R O D U C E R | S T E P - B Y- S T...\n",
      "Result 2 from realestateglossary.pdf:\n",
      "Distance: 0.8004\n",
      "Text snippet: torneys. Closing Costs: The upfront fees charged in connection with a mortgage loan trans - action. Money paid by a buyer (and/or seller or other thir...\n",
      "\n",
      "Query: How do I get pre-approved for a mortgage?\n",
      "Result 1 from Home_Buyers_Guide.pdf:\n",
      "Distance: 0.6360\n",
      "Text snippet: ion the lender will provide you with a document that details how much you can borrow to buy a home. You may want to consider looking online to see wha...\n",
      "Result 2 from HL_Buyers_Guide_FINAL_March2019.pdf:\n",
      "Distance: 0.6367\n",
      "Text snippet: an effectively size up your loan options and decide which lender is best for you - and your future. (If you need help navigating the details, the Cons...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# First, install required packages\n",
    "# !pip install sentence-transformers faiss-cpu\n",
    "\n",
    "class RealEstateRAG:\n",
    "    def __init__(self, chunks_path=\"processed_chunks.pkl\", embedding_model=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system with document chunks and embedding model.\n",
    "        \n",
    "        Args:\n",
    "            chunks_path: Path to the pickle file with processed chunks\n",
    "            embedding_model: SentenceTransformer model to use for embeddings\n",
    "        \"\"\"\n",
    "        self.chunks_path = chunks_path\n",
    "        self.embedding_model_name = embedding_model\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.index = None\n",
    "        self.chunks_df = None\n",
    "        self.embeddings = None\n",
    "        \n",
    "        # Load chunks if they exist\n",
    "        if os.path.exists(chunks_path):\n",
    "            self.load_chunks()\n",
    "    \n",
    "    def load_chunks(self):\n",
    "        \"\"\"Load document chunks from pickle file.\"\"\"\n",
    "        print(f\"Loading chunks from {self.chunks_path}...\")\n",
    "        self.chunks_df = pd.read_pickle(self.chunks_path)\n",
    "        print(f\"Loaded {len(self.chunks_df)} chunks\")\n",
    "    \n",
    "    def create_embeddings(self, save_path=\"embeddings.pkl\"):\n",
    "        \"\"\"Create embeddings for all chunks.\"\"\"\n",
    "        if self.chunks_df is None:\n",
    "            self.load_chunks()\n",
    "        \n",
    "        print(f\"Creating embeddings using {self.embedding_model_name}...\")\n",
    "        texts = self.chunks_df['text'].tolist()\n",
    "        \n",
    "        # Embed in batches to avoid memory issues\n",
    "        batch_size = 64\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Creating embeddings\"):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            batch_embeddings = self.embedding_model.encode(batch_texts)\n",
    "            embeddings.append(batch_embeddings)\n",
    "        \n",
    "        self.embeddings = np.vstack(embeddings)\n",
    "        \n",
    "        # Save embeddings\n",
    "        if save_path:\n",
    "            with open(save_path, 'wb') as f:\n",
    "                pickle.dump(self.embeddings, f)\n",
    "            print(f\"Saved embeddings to {save_path}\")\n",
    "        \n",
    "        return self.embeddings\n",
    "    \n",
    "    def load_embeddings(self, embeddings_path=\"embeddings.pkl\"):\n",
    "        \"\"\"Load pre-computed embeddings.\"\"\"\n",
    "        if os.path.exists(embeddings_path):\n",
    "            print(f\"Loading embeddings from {embeddings_path}...\")\n",
    "            with open(embeddings_path, 'rb') as f:\n",
    "                self.embeddings = pickle.load(f)\n",
    "            print(f\"Loaded embeddings with shape {self.embeddings.shape}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Embeddings file {embeddings_path} not found.\")\n",
    "            return False\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"Build a FAISS index for fast similarity search.\"\"\"\n",
    "        if self.embeddings is None:\n",
    "            if not self.load_embeddings():\n",
    "                self.create_embeddings()\n",
    "        \n",
    "        print(\"Building FAISS index...\")\n",
    "        dimension = self.embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "        self.index.add(self.embeddings.astype('float32'))\n",
    "        print(f\"Built index with {self.index.ntotal} vectors\")\n",
    "    \n",
    "    def search(self, query, k=5):\n",
    "        \"\"\"\n",
    "        Search for chunks most similar to the query.\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            k: Number of results to return\n",
    "        \n",
    "        Returns:\n",
    "            List of dictionaries with chunk text and metadata\n",
    "        \"\"\"\n",
    "        if self.index is None:\n",
    "            self.build_index()\n",
    "        \n",
    "        # Embed the query\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        # Search the index\n",
    "        distances, indices = self.index.search(query_embedding.astype('float32'), k)\n",
    "        \n",
    "        # Get the results\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx < len(self.chunks_df):  # Ensure index is valid\n",
    "                chunk = self.chunks_df.iloc[idx]\n",
    "                results.append({\n",
    "                    'chunk_id': chunk['chunk_id'],\n",
    "                    'source_file': chunk['source_file'],\n",
    "                    'text': chunk['text'],\n",
    "                    'distance': distances[0][i]\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def answer_question(self, question, k=5):\n",
    "        \"\"\"\n",
    "        Answer a question using RAG.\n",
    "        \n",
    "        Args:\n",
    "            question: The question to answer\n",
    "            k: Number of chunks to retrieve\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with retrieved context and sample answer\n",
    "        \"\"\"\n",
    "        # Retrieve relevant chunks\n",
    "        relevant_chunks = self.search(question, k=k)\n",
    "        \n",
    "        # Combine context\n",
    "        context = \"\\n\\n\".join([f\"From {chunk['source_file']}:\\n{chunk['text']}\" \n",
    "                              for chunk in relevant_chunks])\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'retrieved_chunks': relevant_chunks,\n",
    "            'context': context\n",
    "        }\n",
    "\n",
    "# Initialize and run\n",
    "if __name__ == \"__main__\":\n",
    "    rag = RealEstateRAG()\n",
    "    \n",
    "    # Check if embeddings exist, if not create them\n",
    "    if not os.path.exists(\"embeddings.pkl\"):\n",
    "        rag.create_embeddings()\n",
    "    else:\n",
    "        rag.load_embeddings()\n",
    "    \n",
    "    # Build search index\n",
    "    rag.build_index()\n",
    "    \n",
    "    # Test the search\n",
    "    test_queries = [\n",
    "        \"What is an FHA loan?\",\n",
    "        \"Should I rent or buy a house?\",\n",
    "        \"What are closing costs?\",\n",
    "        \"How do I get pre-approved for a mortgage?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTesting retrieval with sample queries:\")\n",
    "    for query in test_queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        results = rag.search(query, k=2)\n",
    "        for i, result in enumerate(results):\n",
    "            print(f\"Result {i+1} from {result['source_file']}:\")\n",
    "            print(f\"Distance: {result['distance']:.4f}\")\n",
    "            print(f\"Text snippet: {result['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealEstateChatbot:\n",
    "    def __init__(self, rag_system):\n",
    "        \"\"\"\n",
    "        Initialize the chatbot with a RAG system.\n",
    "        \n",
    "        Args:\n",
    "            rag_system: An initialized RealEstateRAG object\n",
    "        \"\"\"\n",
    "        self.rag = rag_system\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def answer(self, query, k=5, show_context=False):\n",
    "        \"\"\"\n",
    "        Answer a user question by retrieving relevant context.\n",
    "        \n",
    "        Args:\n",
    "            query: The user's question\n",
    "            k: Number of contexts to retrieve\n",
    "            show_context: Whether to display the retrieved context\n",
    "        \"\"\"\n",
    "        # Save the user's query to conversation history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": query})\n",
    "        \n",
    "        # Retrieve relevant information\n",
    "        retrieval_results = self.rag.answer_question(query, k=k)\n",
    "        context = retrieval_results['context']\n",
    "        \n",
    "        # Show the retrieved context if requested\n",
    "        if show_context:\n",
    "            print(\"RETRIEVED CONTEXT:\")\n",
    "            print(\"-\" * 80)\n",
    "            print(context)\n",
    "            print(\"-\" * 80)\n",
    "            print()\n",
    "        \n",
    "        # Get sources for citation\n",
    "        sources = []\n",
    "        for chunk in retrieval_results['retrieved_chunks']:\n",
    "            source = chunk['source_file']\n",
    "            if source not in sources:\n",
    "                sources.append(source)\n",
    "        \n",
    "        # Format response with retrieved information\n",
    "        response = \"Based on the retrieved documents:\\n\\n\"\n",
    "        for i, chunk in enumerate(retrieval_results['retrieved_chunks']):\n",
    "            response += f\"From {chunk['source_file']}:\\n\"\n",
    "            response += f\"{chunk['text'][:300]}...\\n\\n\"\n",
    "        \n",
    "        response += \"\\nSources: \" + \", \".join(sources)\n",
    "        \n",
    "        # Add the response to conversation history\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def chat(self):\n",
    "        \"\"\"Start an interactive chat session.\"\"\"\n",
    "        print(\"Welcome to RealEstateGPT! Ask me anything about real estate.\")\n",
    "        print(\"Type 'exit' to end the conversation.\\n\")\n",
    "        \n",
    "        while True:\n",
    "            query = input(\"You: \")\n",
    "            if query.lower() in ['exit', 'quit', 'bye']:\n",
    "                print(\"RealEstateGPT: Goodbye! Hope I was helpful.\")\n",
    "                break\n",
    "            \n",
    "            answer = self.answer(query, show_context=False)\n",
    "            print(f\"\\nRealEstateGPT: {answer}\\n\")\n",
    "\n",
    "# Function for Jupyter Notebook interface\n",
    "def create_chatbot_interface():\n",
    "    try:\n",
    "        import ipywidgets as widgets\n",
    "        from IPython.display import display, clear_output, Markdown\n",
    "        \n",
    "        # Initialize the chatbot\n",
    "        rag = RealEstateRAG()\n",
    "        rag.load_embeddings()\n",
    "        rag.build_index()\n",
    "        chatbot = RealEstateChatbot(rag)\n",
    "        \n",
    "        # Create widgets\n",
    "        output = widgets.Output()\n",
    "        text_input = widgets.Text(\n",
    "            placeholder='Type your real estate question here...',\n",
    "            layout=widgets.Layout(width='80%')\n",
    "        )\n",
    "        context_checkbox = widgets.Checkbox(\n",
    "            value=False,\n",
    "            description='Show retrieved context',\n",
    "            disabled=False\n",
    "        )\n",
    "        send_button = widgets.Button(\n",
    "            description='Send',\n",
    "            button_style='primary',\n",
    "            tooltip='Send your question'\n",
    "        )\n",
    "        clear_button = widgets.Button(\n",
    "            description='Clear',\n",
    "            tooltip='Clear the conversation'\n",
    "        )\n",
    "        \n",
    "        # Layout\n",
    "        input_box = widgets.HBox([text_input, send_button, clear_button])\n",
    "        display(widgets.VBox([context_checkbox, input_box, output]))\n",
    "        \n",
    "        def on_send_button_clicked(b):\n",
    "            with output:\n",
    "                query = text_input.value\n",
    "                if query.strip() == \"\":\n",
    "                    return\n",
    "                \n",
    "                # Display user question\n",
    "                display(Markdown(f\"**You:** {query}\"))\n",
    "                \n",
    "                # Get and display answer\n",
    "                answer = chatbot.answer(query, show_context=context_checkbox.value)\n",
    "                display(Markdown(f\"**RealEstateGPT:** {answer}\"))\n",
    "                \n",
    "                # Clear input field\n",
    "                text_input.value = \"\"\n",
    "        \n",
    "        def on_clear_button_clicked(b):\n",
    "            with output:\n",
    "                clear_output()\n",
    "                chatbot.conversation_history = []\n",
    "        \n",
    "        # Connect events\n",
    "        send_button.on_click(on_send_button_clicked)\n",
    "        clear_button.on_click(on_clear_button_clicked)\n",
    "        \n",
    "        # Also submit on enter key\n",
    "        text_input.on_submit(lambda x: on_send_button_clicked(None))\n",
    "        \n",
    "        return chatbot\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"ipywidgets not available. Use chatbot.chat() instead.\")\n",
    "        return None\n",
    "\n",
    "# Initialize and run immediately\n",
    "print(\"Initializing Real Estate Chatbot...\")\n",
    "rag = RealEstateRAG()\n",
    "rag.load_embeddings()\n",
    "rag.build_index()\n",
    "chatbot = RealEstateChatbot(rag)\n",
    "\n",
    "# Demo section\n",
    "print(\"\\n=== Demo: Ask a few sample questions ===\\n\")\n",
    "\n",
    "demo_questions = [\n",
    "    \"What are the advantages of FHA loans for first-time homebuyers?\",\n",
    "    \"Is it better to rent or buy in 2024?\",\n",
    "    \"What should I know about closing costs?\"\n",
    "]\n",
    "\n",
    "for question in demo_questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    answer = chatbot.answer(question, k=3)\n",
    "    print(f\"Answer: {answer}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "print(\"\\n=== Interactive Mode ===\\n\")\n",
    "# For Jupyter Notebook interface\n",
    "create_chatbot_interface()\n",
    "\n",
    "print(\"Or start a command-line chat with: chatbot.chat()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import docx\n",
    "import PyPDF2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Base class for processing different types of real estate data\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir=\"data\"):\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def save_processed_data(self, data, output_path):\n",
    "        \"\"\"Save processed data to a pickle file\"\"\"\n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        print(f\"Saved processed data to {output_path}\")\n",
    "\n",
    "class StructuredDataProcessor(DataProcessor):\n",
    "    \"\"\"Process structured data (Excel/CSV files)\"\"\"\n",
    "    \n",
    "    def process_excel(self, file_path):\n",
    "        \"\"\"Process Excel file and return structured data\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            return {\n",
    "                'filename': os.path.basename(file_path),\n",
    "                'data': df,\n",
    "                'metadata': {\n",
    "                    'columns': df.columns.tolist(),\n",
    "                    'rows': len(df),\n",
    "                    'dtypes': {col: str(dtype) for col, dtype in df.dtypes.items()}\n",
    "                }\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_csv(self, file_path):\n",
    "        \"\"\"Process CSV file and return structured data\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            return {\n",
    "                'filename': os.path.basename(file_path),\n",
    "                'data': df,\n",
    "                'metadata': {\n",
    "                    'columns': df.columns.tolist(),\n",
    "                    'rows': len(df),\n",
    "                    'dtypes': {col: str(dtype) for col, dtype in df.dtypes.items()}\n",
    "                }\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_all(self):\n",
    "        \"\"\"Process all structured data files in the data directory\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Find all Excel and CSV files\n",
    "        excel_files = [f for f in os.listdir(self.data_dir) \n",
    "                      if f.lower().endswith(('.xlsx', '.xls'))]\n",
    "        csv_files = [f for f in os.listdir(self.data_dir) \n",
    "                    if f.lower().endswith('.csv')]\n",
    "        \n",
    "        # Process Excel files\n",
    "        for file in tqdm(excel_files, desc=\"Processing Excel files\"):\n",
    "            file_path = os.path.join(self.data_dir, file)\n",
    "            result = self.process_excel(file_path)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "        \n",
    "        # Process CSV files\n",
    "        for file in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
    "            file_path = os.path.join(self.data_dir, file)\n",
    "            result = self.process_csv(file_path)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "class DocProcessor(DataProcessor):\n",
    "    \"\"\"Process Word documents\"\"\"\n",
    "    \n",
    "    def extract_text_from_docx(self, file_path):\n",
    "        \"\"\"Extract text from a Word document\"\"\"\n",
    "        try:\n",
    "            doc = docx.Document(file_path)\n",
    "            full_text = []\n",
    "            for para in doc.paragraphs:\n",
    "                full_text.append(para.text)\n",
    "            return '\\n'.join(full_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {file_path}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def process_all(self):\n",
    "        \"\"\"Process all Word documents in the data directory\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Find all Word documents\n",
    "        docx_files = [f for f in os.listdir(self.data_dir) \n",
    "                     if f.lower().endswith('.docx')]\n",
    "        \n",
    "        # Process Word documents\n",
    "        for file in tqdm(docx_files, desc=\"Processing Word documents\"):\n",
    "            file_path = os.path.join(self.data_dir, file)\n",
    "            try:\n",
    "                text = self.extract_text_from_docx(file_path)\n",
    "                results.append({\n",
    "                    'filename': file,\n",
    "                    'text': text,\n",
    "                    'size': os.path.getsize(file_path)\n",
    "                })\n",
    "                print(f\"Successfully processed {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {e}\")\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "class EnhancedRealEstateRAG:\n",
    "    def __init__(self, \n",
    "                 text_chunks_path=\"processed_chunks.pkl\", \n",
    "                 structured_data_path=\"processed_structured_data.pkl\",\n",
    "                 embedding_model=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the enhanced RAG system with document chunks and embedding model.\n",
    "        \n",
    "        Args:\n",
    "            text_chunks_path: Path to the pickle file with processed text chunks\n",
    "            structured_data_path: Path to the pickle file with processed structured data\n",
    "            embedding_model: SentenceTransformer model to use for embeddings\n",
    "        \"\"\"\n",
    "        self.text_chunks_path = text_chunks_path\n",
    "        self.structured_data_path = structured_data_path\n",
    "        self.embedding_model_name = embedding_model\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        \n",
    "        self.text_index = None\n",
    "        self.text_chunks_df = None\n",
    "        self.text_embeddings = None\n",
    "        \n",
    "        self.structured_data = None\n",
    "        self.structured_data_descriptions = None\n",
    "        self.structured_data_index = None\n",
    "        self.structured_data_embeddings = None\n",
    "        \n",
    "        # Load data if it exists\n",
    "        self.load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load document chunks and structured data from pickle files.\"\"\"\n",
    "        # Load text chunks\n",
    "        if os.path.exists(self.text_chunks_path):\n",
    "            print(f\"Loading text chunks from {self.text_chunks_path}...\")\n",
    "            self.text_chunks_df = pd.read_pickle(self.text_chunks_path)\n",
    "            print(f\"Loaded {len(self.text_chunks_df)} text chunks\")\n",
    "        \n",
    "        # Load structured data\n",
    "        if os.path.exists(self.structured_data_path):\n",
    "            print(f\"Loading structured data from {self.structured_data_path}...\")\n",
    "            with open(self.structured_data_path, 'rb') as f:\n",
    "                self.structured_data = pickle.load(f)\n",
    "            print(f\"Loaded structured data with {len(self.structured_data)} files\")\n",
    "            \n",
    "            # Create descriptions for structured data\n",
    "            self.create_structured_data_descriptions()\n",
    "    \n",
    "    def create_structured_data_descriptions(self):\n",
    "        \"\"\"Create searchable descriptions for structured data.\"\"\"\n",
    "        if not self.structured_data:\n",
    "            return\n",
    "        \n",
    "        descriptions = []\n",
    "        for item in self.structured_data:\n",
    "            df = item['data']\n",
    "            filename = item['filename']\n",
    "            \n",
    "            # Create a description of the dataset\n",
    "            description = f\"Dataset: {filename}\\n\"\n",
    "            description += f\"Contains {len(df)} rows and {len(df.columns)} columns.\\n\"\n",
    "            description += f\"Columns: {', '.join(df.columns.tolist())}\\n\"\n",
    "            \n",
    "            # Add sample data (first few rows)\n",
    "            description += \"Sample data:\\n\"\n",
    "            sample = df.head(3).to_string()\n",
    "            description += sample\n",
    "            \n",
    "            descriptions.append({\n",
    "                'source_file': filename,\n",
    "                'text': description,\n",
    "                'type': 'structured'\n",
    "            })\n",
    "            \n",
    "            # Also add column-specific descriptions\n",
    "            for column in df.columns:\n",
    "                col_desc = f\"Column '{column}' in dataset {filename}.\\n\"\n",
    "                try:\n",
    "                    # Add statistical information if numeric\n",
    "                    if pd.api.types.is_numeric_dtype(df[column]):\n",
    "                        col_desc += f\"Min: {df[column].min()}, Max: {df[column].max()}, Mean: {df[column].mean():.2f}\\n\"\n",
    "                    # Add unique values if categorical with few values\n",
    "                    elif df[column].nunique() < 10:\n",
    "                        col_desc += f\"Values: {', '.join(map(str, df[column].unique()[:10]))}\\n\"\n",
    "                    # Add value counts for high-frequency items\n",
    "                    value_counts = df[column].value_counts().nlargest(5).to_dict()\n",
    "                    col_desc += \"Most common values: \" + \", \".join([f\"{k}: {v}\" for k, v in value_counts.items()])\n",
    "                except:\n",
    "                    # Skip if there's an error calculating stats\n",
    "                    pass\n",
    "                \n",
    "                descriptions.append({\n",
    "                    'source_file': filename,\n",
    "                    'text': col_desc,\n",
    "                    'type': 'structured_column',\n",
    "                    'column': column\n",
    "                })\n",
    "        \n",
    "        self.structured_data_descriptions = pd.DataFrame(descriptions)\n",
    "    \n",
    "    def create_embeddings(self, save_text_path=\"text_embeddings.pkl\", save_structured_path=\"structured_embeddings.pkl\"):\n",
    "        \"\"\"Create embeddings for all content.\"\"\"\n",
    "        # Create text embeddings\n",
    "        if self.text_chunks_df is not None:\n",
    "            print(f\"Creating text embeddings using {self.embedding_model_name}...\")\n",
    "            texts = self.text_chunks_df['text'].tolist()\n",
    "            \n",
    "            # Embed in batches\n",
    "            batch_size = 64\n",
    "            embeddings = []\n",
    "            \n",
    "            for i in tqdm(range(0, len(texts), batch_size), desc=\"Creating text embeddings\"):\n",
    "                batch_texts = texts[i:i + batch_size]\n",
    "                batch_embeddings = self.embedding_model.encode(batch_texts)\n",
    "                embeddings.append(batch_embeddings)\n",
    "            \n",
    "            self.text_embeddings = np.vstack(embeddings)\n",
    "            \n",
    "            # Save embeddings\n",
    "            if save_text_path:\n",
    "                with open(save_text_path, 'wb') as f:\n",
    "                    pickle.dump(self.text_embeddings, f)\n",
    "                print(f\"Saved text embeddings to {save_text_path}\")\n",
    "        \n",
    "        # Create structured data embeddings\n",
    "        if self.structured_data_descriptions is not None:\n",
    "            print(f\"Creating structured data embeddings using {self.embedding_model_name}...\")\n",
    "            descriptions = self.structured_data_descriptions['text'].tolist()\n",
    "            \n",
    "            # Embed in batches\n",
    "            batch_size = 64\n",
    "            embeddings = []\n",
    "            \n",
    "            for i in tqdm(range(0, len(descriptions), batch_size), desc=\"Creating structured data embeddings\"):\n",
    "                batch_texts = descriptions[i:i + batch_size]\n",
    "                batch_embeddings = self.embedding_model.encode(batch_texts)\n",
    "                embeddings.append(batch_embeddings)\n",
    "            \n",
    "            self.structured_data_embeddings = np.vstack(embeddings)\n",
    "            \n",
    "            # Save embeddings\n",
    "            if save_structured_path:\n",
    "                with open(save_structured_path, 'wb') as f:\n",
    "                    pickle.dump(self.structured_data_embeddings, f)\n",
    "                print(f\"Saved structured data embeddings to {save_structured_path}\")\n",
    "    \n",
    "    def load_embeddings(self, text_embeddings_path=\"text_embeddings.pkl\", \n",
    "                       structured_embeddings_path=\"structured_embeddings.pkl\"):\n",
    "        \"\"\"Load pre-computed embeddings.\"\"\"\n",
    "        # Load text embeddings\n",
    "        if os.path.exists(text_embeddings_path):\n",
    "            print(f\"Loading text embeddings from {text_embeddings_path}...\")\n",
    "            with open(text_embeddings_path, 'rb') as f:\n",
    "                self.text_embeddings = pickle.load(f)\n",
    "            print(f\"Loaded text embeddings with shape {self.text_embeddings.shape}\")\n",
    "        \n",
    "        # Load structured data embeddings\n",
    "        if os.path.exists(structured_embeddings_path) and self.structured_data_descriptions is not None:\n",
    "            print(f\"Loading structured data embeddings from {structured_embeddings_path}...\")\n",
    "            with open(structured_embeddings_path, 'rb') as f:\n",
    "                self.structured_data_embeddings = pickle.load(f)\n",
    "            print(f\"Loaded structured data embeddings with shape {self.structured_data_embeddings.shape}\")\n",
    "    \n",
    "    def build_indices(self):\n",
    "        \"\"\"Build FAISS indices for fast similarity search.\"\"\"\n",
    "        # Build text index\n",
    "        if self.text_embeddings is not None:\n",
    "            print(\"Building text FAISS index...\")\n",
    "            dimension = self.text_embeddings.shape[1]\n",
    "            self.text_index = faiss.IndexFlatL2(dimension)\n",
    "            self.text_index.add(self.text_embeddings.astype('float32'))\n",
    "            print(f\"Built text index with {self.text_index.ntotal} vectors\")\n",
    "        \n",
    "        # Build structured data index\n",
    "        if self.structured_data_embeddings is not None:\n",
    "            print(\"Building structured data FAISS index...\")\n",
    "            dimension = self.structured_data_embeddings.shape[1]\n",
    "            self.structured_data_index = faiss.IndexFlatL2(dimension)\n",
    "            self.structured_data_index.add(self.structured_data_embeddings.astype('float32'))\n",
    "            print(f\"Built structured data index with {self.structured_data_index.ntotal} vectors\")\n",
    "    \n",
    "    def classify_query(self, query):\n",
    "        \"\"\"Classify whether the query is more likely to be for text or structured data.\"\"\"\n",
    "        # Keywords that suggest structured data queries\n",
    "        structured_keywords = [\n",
    "            'how many', 'average', 'median', 'statistics', 'price range', 'data', \n",
    "            'dataset', 'spreadsheet', 'chart', 'graph', 'table', 'column', 'row',\n",
    "            'excel', 'csv', 'percentage', 'trend', 'distribution', 'count'\n",
    "        ]\n",
    "        \n",
    "        # Simple rule-based classification\n",
    "        if any(keyword in query.lower() for keyword in structured_keywords):\n",
    "            return \"structured\"\n",
    "        \n",
    "        # More complex classification could be implemented here\n",
    "        return \"text\"\n",
    "    \n",
    "    def search(self, query, k=5, query_type=None):\n",
    "        \"\"\"\n",
    "        Search for content most similar to the query.\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            k: Number of results to return\n",
    "            query_type: Force search in \"text\" or \"structured\" data, or \"both\"\n",
    "        \n",
    "        Returns:\n",
    "            List of dictionaries with content and metadata\n",
    "        \"\"\"\n",
    "        # Auto-classify query if not specified\n",
    "        if query_type is None:\n",
    "            query_type = self.classify_query(query)\n",
    "        \n",
    "        # Embed the query\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Search text index\n",
    "        if query_type in [\"text\", \"both\"] and self.text_index is not None:\n",
    "            distances, indices = self.text_index.search(query_embedding.astype('float32'), k)\n",
    "            \n",
    "            for i, idx in enumerate(indices[0]):\n",
    "                if idx < len(self.text_chunks_df):  # Ensure index is valid\n",
    "                    chunk = self.text_chunks_df.iloc[idx]\n",
    "                    results.append({\n",
    "                        'chunk_id': chunk['chunk_id'],\n",
    "                        'source_file': chunk['source_file'],\n",
    "                        'text': chunk['text'],\n",
    "                        'distance': distances[0][i],\n",
    "                        'type': 'text'\n",
    "                    })\n",
    "        \n",
    "        # Search structured data index\n",
    "        if query_type in [\"structured\", \"both\"] and self.structured_data_index is not None:\n",
    "            # Adjust k to get enough results when combining\n",
    "            s_k = k if query_type == \"structured\" else k // 2\n",
    "            \n",
    "            distances, indices = self.structured_data_index.search(query_embedding.astype('float32'), s_k)\n",
    "            \n",
    "            for i, idx in enumerate(indices[0]):\n",
    "                if idx < len(self.structured_data_descriptions):  # Ensure index is valid\n",
    "                    item = self.structured_data_descriptions.iloc[idx]\n",
    "                    results.append({\n",
    "                        'source_file': item['source_file'],\n",
    "                        'text': item['text'],\n",
    "                        'distance': distances[0][i],\n",
    "                        'type': item['type']\n",
    "                    })\n",
    "        \n",
    "        # Sort by distance\n",
    "        results.sort(key=lambda x: x['distance'])\n",
    "        \n",
    "        # Limit to k results\n",
    "        return results[:k]\n",
    "    \n",
    "    def answer_question(self, question, k=5):\n",
    "        \"\"\"\n",
    "        Answer a question using RAG.\n",
    "        \n",
    "        Args:\n",
    "            question: The question to answer\n",
    "            k: Number of chunks to retrieve\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with retrieved context and metadata\n",
    "        \"\"\"\n",
    "        # Determine if this is a structured or unstructured data question\n",
    "        query_type = self.classify_query(question)\n",
    "        \n",
    "        # Retrieve relevant content\n",
    "        relevant_content = self.search(question, k=k, query_type=query_type)\n",
    "        \n",
    "        # Combine context\n",
    "        context = \"\\n\\n\".join([f\"From {item['source_file']}:\\n{item['text']}\" \n",
    "                              for item in relevant_content])\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'retrieved_content': relevant_content,\n",
    "            'context': context,\n",
    "            'query_type': query_type\n",
    "        }\n",
    "    \n",
    "    def extract_structured_data(self, filename, column=None, filters=None):\n",
    "        \"\"\"\n",
    "        Extract specific data from structured datasets.\n",
    "        \n",
    "        Args:\n",
    "            filename: Name of the dataset file\n",
    "            column: Specific column to extract\n",
    "            filters: Dictionary of column:value pairs to filter the data\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame or Series with the extracted data\n",
    "        \"\"\"\n",
    "        if not self.structured_data:\n",
    "            return None\n",
    "        \n",
    "        # Find the dataset\n",
    "        dataset = None\n",
    "        for item in self.structured_data:\n",
    "            if item['filename'] == filename:\n",
    "                dataset = item['data']\n",
    "                break\n",
    "        \n",
    "        if dataset is None:\n",
    "            return None\n",
    "        \n",
    "        # Apply filters\n",
    "        if filters:\n",
    "            for col, value in filters.items():\n",
    "                if col in dataset.columns:\n",
    "                    dataset = dataset[dataset[col] == value]\n",
    "        \n",
    "        # Extract specific column\n",
    "        if column and column in dataset.columns:\n",
    "            return dataset[column]\n",
    "        \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "class FineTuningDataGenerator:\n",
    "    \"\"\"Generate data for fine-tuning LLMs on real estate content\"\"\"\n",
    "    \n",
    "    def __init__(self, text_chunks_path=\"processed_chunks.pkl\", output_dir=\"fine_tuning_data\"):\n",
    "        self.text_chunks_path = text_chunks_path\n",
    "        self.output_dir = output_dir\n",
    "        self.chunks_df = None\n",
    "        \n",
    "        # Load chunks data\n",
    "        if os.path.exists(text_chunks_path):\n",
    "            self.chunks_df = pd.read_pickle(text_chunks_path)\n",
    "            print(f\"Loaded {len(self.chunks_df)} text chunks for fine-tuning data generation\")\n",
    "        \n",
    "        # Create output directory if needed\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "    \n",
    "    def generate_qa_pairs(self, num_pairs=100, min_context_length=200):\n",
    "        \"\"\"\n",
    "        Generate question-answer pairs for fine-tuning based on chunks.\n",
    "        \n",
    "        This creates synthetic QA pairs from the text chunks.\n",
    "        \"\"\"\n",
    "        if self.chunks_df is None:\n",
    "            print(\"No chunks data available. Please check the path.\")\n",
    "            return\n",
    "        \n",
    "        # Filter chunks by minimum length\n",
    "        valid_chunks = self.chunks_df[self.chunks_df['text'].str.len() > min_context_length]\n",
    "        if len(valid_chunks) < num_pairs:\n",
    "            print(f\"Warning: Only {len(valid_chunks)} valid chunks available.\")\n",
    "            num_pairs = len(valid_chunks)\n",
    "        \n",
    "        # Sample random chunks\n",
    "        selected_chunks = valid_chunks.sample(num_pairs)\n",
    "        \n",
    "        qa_pairs = []\n",
    "        \n",
    "        question_templates = [\n",
    "            \"What does the document say about {topic}?\",\n",
    "            \"Can you explain {topic} based on the text?\",\n",
    "            \"What information is provided about {topic}?\",\n",
    "            \"How does the document describe {topic}?\",\n",
    "            \"What are the key points mentioned about {topic}?\"\n",
    "        ]\n",
    "        \n",
    "        # Extract topics from chunks and create QA pairs\n",
    "        for _, chunk in tqdm(selected_chunks.iterrows(), total=len(selected_chunks), \n",
    "                            desc=\"Generating QA pairs\"):\n",
    "            # Extract potential topic words (nouns) using simple heuristics\n",
    "            # In a real implementation, you might use NLP libraries for better extraction\n",
    "            text = chunk['text']\n",
    "            words = text.split()\n",
    "            \n",
    "            # Find potential topic words (longer words, likely to be meaningful)\n",
    "            potential_topics = [word for word in words \n",
    "                               if len(word) > 5 and word.isalpha()]\n",
    "            \n",
    "            if not potential_topics:\n",
    "                # If no good topics found, use generic template\n",
    "                question = \"What information is provided in this text?\"\n",
    "                answer = text\n",
    "            else:\n",
    "                # Select a random topic and question template\n",
    "                topic = np.random.choice(potential_topics)\n",
    "                question_template = np.random.choice(question_templates)\n",
    "                question = question_template.format(topic=topic)\n",
    "                answer = text\n",
    "            \n",
    "            qa_pairs.append({\n",
    "                'question': question,\n",
    "                'answer': answer,\n",
    "                'source': chunk['source_file']\n",
    "            })\n",
    "        \n",
    "        # Save the generated QA pairs\n",
    "        output_path = os.path.join(self.output_dir, \"qa_pairs.json\")\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(qa_pairs, f, indent=2)\n",
    "        \n",
    "        print(f\"Generated {len(qa_pairs)} QA pairs and saved to {output_path}\")\n",
    "        return qa_pairs\n",
    "    \n",
    "    def generate_fine_tuning_formats(self, qa_pairs=None, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Format QA pairs for different fine-tuning methods.\n",
    "        \n",
    "        Generates formats for:\n",
    "        1. OpenAI fine-tuning (JSONL)\n",
    "        2. Hugging Face fine-tuning (CSV)\n",
    "        \"\"\"\n",
    "        if qa_pairs is None:\n",
    "            # Try to load QA pairs\n",
    "            qa_path = os.path.join(self.output_dir, \"qa_pairs.json\")\n",
    "            if os.path.exists(qa_path):\n",
    "                with open(qa_path, 'r') as f:\n",
    "                    qa_pairs = json.load(f)\n",
    "            else:\n",
    "                print(\"No QA pairs provided or found. Please generate QA pairs first.\")\n",
    "                return\n",
    "        \n",
    "        # Split into train and test sets\n",
    "        train_pairs, test_pairs = train_test_split(qa_pairs, test_size=test_size, random_state=42)\n",
    "        \n",
    "        # 1. OpenAI fine-tuning format (JSONL)\n",
    "        openai_train = []\n",
    "        openai_test = []\n",
    "        \n",
    "        for pair in train_pairs:\n",
    "            openai_train.append({\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in real estate.\"},\n",
    "                    {\"role\": \"user\", \"content\": pair['question']},\n",
    "                    {\"role\": \"assistant\", \"content\": pair['answer']}\n",
    "                ]\n",
    "            })\n",
    "        \n",
    "        for pair in test_pairs:\n",
    "            openai_test.append({\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in real estate.\"},\n",
    "                    {\"role\": \"user\", \"content\": pair['question']},\n",
    "                    {\"role\": \"assistant\", \"content\": pair['answer']}\n",
    "                ]\n",
    "            })\n",
    "        \n",
    "        # Save OpenAI format\n",
    "        with open(os.path.join(self.output_dir, \"openai_train.jsonl\"), 'w') as f:\n",
    "            for item in openai_train:\n",
    "                f.write(json.dumps(item) + '\\n')\n",
    "        \n",
    "        with open(os.path.join(self.output_dir, \"openai_test.jsonl\"), 'w') as f:\n",
    "            for item in openai_test:\n",
    "                f.write(json.dumps(item) + '\\n')\n",
    "        \n",
    "        # 2. Hugging Face fine-tuning format (CSV)\n",
    "        hf_train_data = pd.DataFrame({\n",
    "            'input': [pair['question'] for pair in train_pairs],\n",
    "            'output': [pair['answer'] for pair in train_pairs]\n",
    "        })\n",
    "        \n",
    "        hf_test_data = pd.DataFrame({\n",
    "            'input': [pair['question'] for pair in test_pairs],\n",
    "            'output': [pair['answer'] for pair in test_pairs]\n",
    "        })\n",
    "        \n",
    "        # Save Hugging Face format\n",
    "        hf_train_data.to_csv(os.path.join(self.output_dir, \"hf_train.csv\"), index=False)\n",
    "        hf_test_data.to_csv(os.path.join(self.output_dir, \"hf_test.csv\"), index=False)\n",
    "        \n",
    "        print(f\"Created fine-tuning datasets in multiple formats:\")\n",
    "        print(f\"OpenAI: {len(openai_train)} training, {len(openai_test)} testing examples\")\n",
    "        print(f\"Hugging Face: {len(hf_train_data)} training, {len(hf_test_data)} testing examples\")\n",
    "        \n",
    "        return {\n",
    "            'openai': {'train': openai_train, 'test': openai_test},\n",
    "            'huggingface': {'train': hf_train_data, 'test': hf_test_data}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from enhanced_rag import EnhancedRealEstateRAG\n",
    "\n",
    "class RealEstateChatbot:\n",
    "    def __init__(self, rag_system=None):\n",
    "        \"\"\"\n",
    "        Initialize the chatbot with a RAG system.\n",
    "        \n",
    "        Args:\n",
    "            rag_system: An initialized EnhancedRealEstateRAG object\n",
    "        \"\"\"\n",
    "        # Initialize RAG if not provided\n",
    "        if rag_system is None:\n",
    "            print(\"Initializing new RAG system...\")\n",
    "            self.rag = EnhancedRealEstateRAG()\n",
    "            \n",
    "            # Load embeddings and build index\n",
    "            if os.path.exists(\"text_embeddings.pkl\"):\n",
    "                self.rag.load_embeddings()\n",
    "                self.rag.build_indices()\n",
    "            else:\n",
    "                print(\"No embeddings found. Please create embeddings first.\")\n",
    "                return\n",
    "        else:\n",
    "            self.rag = rag_system\n",
    "        \n",
    "        self.conversation_history = []\n",
    "        self.last_query_type = None\n",
    "        self.last_structured_data_file = None\n",
    "    \n",
    "    def answer(self, query, k=5, show_context=False, query_type=None):\n",
    "        \"\"\"\n",
    "        Answer a user question by retrieving relevant context.\n",
    "        \n",
    "        Args:\n",
    "            query: The user's question\n",
    "            k: Number of contexts to retrieve\n",
    "            show_context: Whether to display the retrieved context\n",
    "            query_type: Force a specific query type (\"text\", \"structured\", or \"both\")\n",
    "            \n",
    "        Returns:\n",
    "            A formatted response\n",
    "        \"\"\"\n",
    "        # Save the user's query to conversation history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": query})\n",
    "        \n",
    "        # Check for special commands\n",
    "        if query.lower().startswith(\"analyze \"):\n",
    "            return self._handle_analysis_command(query[8:])\n",
    "        \n",
    "        # Retrieve relevant information\n",
    "        retrieval_results = self.rag.answer_question(query, k=k)\n",
    "        context = retrieval_results['context']\n",
    "        self.last_query_type = retrieval_results['query_type']\n",
    "        \n",
    "        # Show the retrieved context if requested\n",
    "        if show_context:\n",
    "            print(\"RETRIEVED CONTEXT:\")\n",
    "            print(\"-\" * 80)\n",
    "            print(context)\n",
    "            print(\"-\" * 80)\n",
    "            print()\n",
    "        \n",
    "        # Get sources for citation\n",
    "        sources = []\n",
    "        structured_files = []\n",
    "        for item in retrieval_results['retrieved_content']:\n",
    "            source = item['source_file']\n",
    "            if source not in sources:\n",
    "                sources.append(source)\n",
    "            \n",
    "            # Track structured data files for follow-up\n",
    "            if item.get('type') in ['structured', 'structured_column']:\n",
    "                if source not in structured_files:\n",
    "                    structured_files.append(source)\n",
    "                    self.last_structured_data_file = source\n",
    "        \n",
    "        # Format response based on query type\n",
    "        if self.last_query_type == \"structured\":\n",
    "            response = self._format_structured_response(retrieval_results, structured_files)\n",
    "        else:\n",
    "            response = self._format_text_response(retrieval_results)\n",
    "        \n",
    "        # Add the response to conversation history\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _format_text_response(self, retrieval_results):\n",
    "        \"\"\"Format response for text-based queries.\"\"\"\n",
    "        response = \"Based on the real estate documents I've analyzed:\\n\\n\"\n",
    "        \n",
    "        # Group by source file for better organization\n",
    "        by_source = {}\n",
    "        for item in retrieval_results['retrieved_content']:\n",
    "            source = item['source_file']\n",
    "            if source not in by_source:\n",
    "                by_source[source] = []\n",
    "            by_source[source].append(item['text'])\n",
    "        \n",
    "        # Build response from each source\n",
    "        for source, texts in by_source.items():\n",
    "            response += f\"From {source}:\\n\"\n",
    "            \n",
    "            # Join and truncate content from this source\n",
    "            combined_text = \"\\n\".join(texts)\n",
    "            truncated = combined_text[:500] + \"...\" if len(combined_text) > 500 else combined_text\n",
    "            response += f\"{truncated}\\n\\n\"\n",
    "        \n",
    "        # Add sources\n",
    "        sources = list(by_source.keys())\n",
    "        response += f\"Sources: {', '.join(sources)}\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _format_structured_response(self, retrieval_results, structured_files):\n",
    "        \"\"\"Format response for structured data queries.\"\"\"\n",
    "        response = \"Based on the real estate data analysis:\\n\\n\"\n",
    "        \n",
    "        for item in retrieval_results['retrieved_content']:\n",
    "            if item.get('type') in ['structured', 'structured_column']:\n",
    "                response += f\"From {item['source_file']}:\\n\"\n",
    "                response += f\"{item['text']}\\n\\n\"\n",
    "        \n",
    "        # Add follow-up hint\n",
    "        if structured_files:\n",
    "            response += f\"You can ask me to analyze specific aspects of the {structured_files[0]} dataset. \"\n",
    "            response += f\"For example, try 'analyze {structured_files[0]} by price range'.\\n\\n\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _handle_analysis_command(self, command):\n",
    "        \"\"\"Handle special analysis commands for structured data.\"\"\"\n",
    "        # Parse command to extract file and analysis type\n",
    "        parts = command.strip().split()\n",
    "        if len(parts) < 2:\n",
    "            return \"Please specify what to analyze. For example: 'analyze dataset.csv by price'\"\n",
    "        \n",
    "        filename = parts[0]\n",
    "        analysis_type = \" \".join(parts[1:])\n",
    "        \n",
    "        # Find the structured dataset\n",
    "        if not self.rag.structured_data:\n",
    "            return \"No structured data is available for analysis.\"\n",
    "        \n",
    "        dataset = None\n",
    "        for item in self.rag.structured_data:\n",
    "            if item['filename'] == filename:\n",
    "                dataset = item['data']\n",
    "                break\n",
    "        \n",
    "        if dataset is None:\n",
    "            return f\"Dataset '{filename}' not found. Available datasets: \" + \\\n",
    "                   \", \".join([item['filename'] for item in self.rag.structured_data])\n",
    "        \n",
    "        # Perform different analyses based on the command\n",
    "        try:\n",
    "            if \"summary\" in analysis_type.lower():\n",
    "                return self._generate_summary_stats(dataset, filename)\n",
    "            elif \"price range\" in analysis_type.lower() or \"price distribution\" in analysis_type.lower():\n",
    "                return self._analyze_price_distribution(dataset, filename)\n",
    "            elif \"location\" in analysis_type.lower() or \"area\" in analysis_type.lower():\n",
    "                return self._analyze_by_location(dataset, filename)\n",
    "            elif \"trend\" in analysis_type.lower() or \"time\" in analysis_type.lower():\n",
    "                return self._analyze_time_trends(dataset, filename)\n",
    "            else:\n",
    "                return f\"I'm not sure how to analyze {filename} by {analysis_type}. \" + \\\n",
    "                       \"Try asking for a summary, price range, location analysis, or time trends.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error analyzing {filename}: {str(e)}\"\n",
    "    \n",
    "    def _generate_summary_stats(self, df, filename):\n",
    "        \"\"\"Generate summary statistics for a dataset.\"\"\"\n",
    "        response = f\"## Summary Statistics for {filename}\\n\\n\"\n",
    "        \n",
    "        # Basic dataset info\n",
    "        response += f\"This dataset contains {len(df)} properties with {len(df.columns)} attributes.\\n\\n\"\n",
    "        \n",
    "        # Try to identify numeric columns for statistics\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        if numeric_cols:\n",
    "            response += \"### Key Metrics\\n\\n\"\n",
    "            \n",
    "            # Look for price/value columns\n",
    "            price_cols = [col for col in numeric_cols \n",
    "                         if any(term in col.lower() for term in ['price', 'value', 'cost', 'amount'])]\n",
    "            \n",
    "            if price_cols:\n",
    "                for col in price_cols[:2]:  # Limit to first 2 price columns\n",
    "                    response += f\"**{col}**:\\n\"\n",
    "                    response += f\"- Average: ${df[col].mean():.2f}\\n\"\n",
    "                    response += f\"- Median: ${df[col].median():.2f}\\n\"\n",
    "                    response += f\"- Range: ${df[col].min():.2f} to ${df[col].max():.2f}\\n\\n\"\n",
    "            \n",
    "            # Look for size/area columns\n",
    "            size_cols = [col for col in numeric_cols \n",
    "                        if any(term in col.lower() for term in ['size', 'area', 'sqft', 'feet', 'acre'])]\n",
    "            \n",
    "            if size_cols:\n",
    "                for col in size_cols[:2]:  # Limit to first 2 size columns\n",
    "                    response += f\"**{col}**:\\n\"\n",
    "                    response += f\"- Average: {df[col].mean():.2f}\\n\"\n",
    "                    response += f\"- Median: {df[col].median():.2f}\\n\"\n",
    "                    response += f\"- Range: {df[col].min():.2f} to {df[col].max():.2f}\\n\\n\"\n",
    "        \n",
    "        # Try to identify categorical columns\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "        if categorical_cols:\n",
    "            # Look for location columns\n",
    "            location_cols = [col for col in categorical_cols \n",
    "                           if any(term in col.lower() for term in ['city', 'state', 'zip', 'location', 'address'])]\n",
    "            \n",
    "            if location_cols:\n",
    "                for col in location_cols[:1]:  # Limit to first location column\n",
    "                    top_locations = df[col].value_counts().nlargest(5)\n",
    "                    response += f\"**Top {col}**:\\n\"\n",
    "                    for loc, count in top_locations.items():\n",
    "                        response += f\"- {loc}: {count} properties ({count/len(df)*100:.1f}%)\\n\"\n",
    "                    response += \"\\n\"\n",
    "            \n",
    "            # Look for property type columns\n",
    "            type_cols = [col for col in categorical_cols \n",
    "                       if any(term in col.lower() for term in ['type', 'style', 'property', 'category'])]\n",
    "            \n",
    "            if type_cols:\n",
    "                for col in type_cols[:1]:  # Limit to first type column\n",
    "                    top_types = df[col].value_counts().nlargest(5)\n",
    "                    response += f\"**Top {col}**:\\n\"\n",
    "                    for typ, count in top_types.items():\n",
    "                        response += f\"- {typ}: {count} properties ({count/len(df)*100:.1f}%)\\n\"\n",
    "                    response += \"\\n\"\n",
    "        \n",
    "        response += \"You can ask for more specific analyses like 'analyze \" + \\\n",
    "                    f\"{filename} by price range' or 'analyze {filename} by location'\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _analyze_price_distribution(self, df, filename):\n",
    "        \"\"\"Analyze price distribution in the dataset.\"\"\"\n",
    "        # Try to find price column\n",
    "        price_cols = [col for col in df.columns \n",
    "                     if any(term in col.lower() for term in ['price', 'value', 'cost', 'amount'])]\n",
    "        \n",
    "        if not price_cols:\n",
    "            return f\"Could not identify a price column in {filename}.\"\n",
    "        \n",
    "        price_col = price_cols[0]  # Use the first price column found\n",
    "        \n",
    "        # Ensure the column is numeric\n",
    "        if not pd.api.types.is_numeric_dtype(df[price_col]):\n",
    "            return f\"The column {price_col} is not numeric, cannot analyze price distribution.\"\n",
    "        \n",
    "        response = f\"## Price Distribution Analysis for {filename}\\n\\n\"\n",
    "        \n",
    "        # Calculate price ranges\n",
    "        min_price = df[price_col].min()\n",
    "        max_price = df[price_col].max()\n",
    "        \n",
    "        response += f\"Price range from ${min_price:.2f} to ${max_price:.2f}\\n\\n\"\n",
    "        \n",
    "        # Create price brackets\n",
    "        brackets = 5\n",
    "        bracket_size = (max_price - min_price) / brackets\n",
    "        \n",
    "        response += \"### Price Brackets\\n\\n\"\n",
    "        for i in range(brackets):\n",
    "            lower = min_price + i * bracket_size\n",
    "            upper = lower + bracket_size\n",
    "            count = df[(df[price_col] >= lower) & (df[price_col] < upper)].shape[0]\n",
    "            percentage = count / len(df) * 100\n",
    "            \n",
    "            response += f\"- ${lower:.2f} to ${upper:.2f}: {count} properties ({percentage:.1f}%)\\n\"\n",
    "        \n",
    "        # Properties above the highest bracket\n",
    "        count = df[df[price_col] >= max_price].shape[0]\n",
    "        percentage = count / len(df) * 100\n",
    "        response += f\"- ${max_price:.2f} and above: {count} properties ({percentage:.1f}%)\\n\\n\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _analyze_by_location(self, df, filename):\n",
    "        \"\"\"Analyze properties by location.\"\"\"\n",
    "        # Try to find location columns\n",
    "        location_cols = [col for col in df.columns \n",
    "                       if any(term in col.lower() for term in ['city', 'state', 'zip', 'location', 'address'])]\n",
    "        \n",
    "        if not location_cols:\n",
    "            return f\"Could not identify a location column in {filename}.\"\n",
    "        \n",
    "        location_col = location_cols[0]  # Use the first location column found\n",
    "        \n",
    "        response = f\"## Location Analysis for {filename}\\n\\n\"\n",
    "        \n",
    "        # Count properties by location\n",
    "        location_counts = df[location_col].value_counts().nlargest(10)\n",
    "        \n",
    "        response += \"### Top 10 Locations\\n\\n\"\n",
    "        for location, count in location_counts.items():\n",
    "            percentage = count / len(df) * 100\n",
    "            response += f\"- {location}: {count} properties ({percentage:.1f}%)\\n\"\n",
    "        \n",
    "        # Try to find price column to analyze price by location\n",
    "        price_cols = [col for col in df.columns \n",
    "                     if any(term in col.lower() for term in ['price', 'value', 'cost', 'amount'])]\n",
    "        \n",
    "        if price_cols and pd.api.types.is_numeric_dtype(df[price_cols[0]]):\n",
    "            price_col = price_cols[0]\n",
    "            response += f\"\\n### Average Prices by Top 5 Locations\\n\\n\"\n",
    "            \n",
    "            for location in location_counts.index[:5]:\n",
    "                avg_price = df[df[location_col] == location][price_col].mean()\n",
    "                response += f\"- {location}: ${avg_price:.2f}\\n\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _analyze_time_trends(self, df, filename):\n",
    "        \"\"\"Analyze trends over time.\"\"\"\n",
    "        # Try to find date columns\n",
    "        date_cols = [col for col in df.columns \n",
    "                   if any(term in col.lower() for term in ['date', 'year', 'month', 'time'])]\n",
    "        \n",
    "        if not date_cols:\n",
    "            return f\"Could not identify a date/time column in {filename}.\"\n",
    "        \n",
    "        date_col = date_cols[0]  # Use the first date column found\n",
    "        \n",
    "        # Try to convert to datetime if not already\n",
    "        try:\n",
    "            if not pd.api.types.is_datetime64_dtype(df[date_col]):\n",
    "                df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "        except:\n",
    "            return f\"Could not convert {date_col} to a valid date format.\"\n",
    "        \n",
    "        # Drop rows with invalid dates\n",
    "        df = df.dropna(subset=[date_col])\n",
    "        \n",
    "        response = f\"## Time Trend Analysis for {filename}\\n\\n\"\n",
    "        \n",
    "        # Extract year and month\n",
    "        df['year'] = df[date_col].dt.year\n",
    "        \n",
    "        # Count by year\n",
    "        year_counts = df['year'].value_counts().sort_index()\n",
    "        \n",
    "        response += \"### Properties by Year\\n\\n\"\n",
    "        for year, count in year_counts.items():\n",
    "            percentage = count / len(df) * 100\n",
    "            response += f\"- {year}: {count} properties ({percentage:.1f}%)\\n\"\n",
    "        \n",
    "        # Try to find price column to analyze price trends\n",
    "        price_cols = [col for col in df.columns \n",
    "                     if any(term in col.lower() for term in ['price', 'value', 'cost', 'amount'])]\n",
    "        \n",
    "        if price_cols and pd.api.types.is_numeric_dtype(df[price_cols[0]]):\n",
    "            price_col = price_cols[0]\n",
    "            response += f\"\\n### Average Prices by Year\\n\\n\"\n",
    "            \n",
    "            price_by_year = df.groupby('year')[price_col].mean()\n",
    "            \n",
    "            for year, avg_price in price_by_year.items():\n",
    "                response += f\"- {year}: ${avg_price:.2f}\\n\"\n",
    "            \n",
    "            # Calculate price change between first and last year\n",
    "            if len(price_by_year) > 1:\n",
    "                first_year = price_by_year.index[0]\n",
    "                last_year = price_by_year.index[-1]\n",
    "                price_change = ((price_by_year[last_year] / price_by_year[first_year]) - 1) * 100\n",
    "                \n",
    "                response += f\"\\nPrice change from {first_year} to {last_year}: {price_change:.1f}%\\n\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def chat(self):\n",
    "        \"\"\"Start an interactive chat session.\"\"\"\n",
    "        print(\"Welcome to RealEstateGPT! Ask me anything about real estate.\")\n",
    "        print(\"Type 'exit' to end the conversation.\\n\")\n",
    "        \n",
    "        while True:\n",
    "            query = input(\"You: \")\n",
    "            if query.lower() in ['exit', 'quit', 'bye']:\n",
    "                print(\"RealEstateGPT: Goodbye! Hope I was helpful.\")\n",
    "                break\n",
    "            \n",
    "            answer = self.answer(query, show_context=False)\n",
    "            print(f\"\\nRealEstateGPT: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN APPLICATION\n",
    "import os\n",
    "import argparse\n",
    "from data_processors import DataProcessor, StructuredDataProcessor, DocProcessor\n",
    "from enhanced_rag import EnhancedRealEstateRAG\n",
    "from real_estate_chatbot import RealEstateChatbot\n",
    "from fine_tuning import FineTuningDataGenerator\n",
    "\n",
    "def process_all_data(data_dir=\"data\", force_reprocess=False):\n",
    "    \"\"\"Process all data sources and generate necessary files.\"\"\"\n",
    "    # Check if processed files already exist\n",
    "    text_chunks_exists = os.path.exists(\"processed_chunks.pkl\")\n",
    "    structured_data_exists = os.path.exists(\"processed_structured_data.pkl\")\n",
    "    \n",
    "    if text_chunks_exists and structured_data_exists and not force_reprocess:\n",
    "        print(\"Processed data files already exist. Use --force to reprocess.\")\n",
    "        return\n",
    "    \n",
    "    # Process PDF files (using your existing code)\n",
    "    if not text_chunks_exists or force_reprocess:\n",
    "        print(\"Processing PDF files...\")\n",
    "        # You can call your existing functions here\n",
    "        # We'll assume they create processed_chunks.pkl\n",
    "    \n",
    "    # Process structured data\n",
    "    if not structured_data_exists or force_reprocess:\n",
    "        print(\"Processing structured data...\")\n",
    "        structured_processor = StructuredDataProcessor(data_dir)\n",
    "        structured_data = structured_processor.process_all()\n",
    "        structured_processor.save_processed_data(structured_data, \"processed_structured_data.pkl\")\n",
    "        print(f\"Processed {len(structured_data)} structured data files\")\n",
    "    \n",
    "    # Process Word documents\n",
    "    print(\"Processing Word documents...\")\n",
    "    doc_processor = DocProcessor(data_dir)\n",
    "    doc_data = doc_processor.process_all()\n",
    "    doc_processor.save_processed_data(doc_data, \"processed_doc_data.pkl\")\n",
    "    print(f\"Processed {len(doc_data)} Word documents\")\n",
    "\n",
    "def create_embeddings(force_recreate=False):\n",
    "    \"\"\"Create embeddings for all data sources.\"\"\"\n",
    "    text_embeddings_exists = os.path.exists(\"text_embeddings.pkl\")\n",
    "    structured_embeddings_exists = os.path.exists(\"structured_embeddings.pkl\")\n",
    "    \n",
    "    if text_embeddings_exists and structured_embeddings_exists and not force_recreate:\n",
    "        print(\"Embedding files already exist. Use --force to recreate.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize RAG system\n",
    "    rag = EnhancedRealEstateRAG()\n",
    "    \n",
    "    # Create embeddings\n",
    "    rag.create_embeddings()\n",
    "\n",
    "def prepare_fine_tuning_data(num_pairs=1000):\n",
    "    \"\"\"Prepare data for fine-tuning experiments.\"\"\"\n",
    "    print(f\"Generating fine-tuning data with {num_pairs} QA pairs...\")\n",
    "    \n",
    "    # Initialize data generator\n",
    "    generator = FineTuningDataGenerator()\n",
    "    \n",
    "    # Generate QA pairs\n",
    "    qa_pairs = generator.generate_qa_pairs(num_pairs=num_pairs)\n",
    "    \n",
    "    # Format for different fine-tuning approaches\n",
    "    generator.generate_fine_tuning_formats(qa_pairs)\n",
    "\n",
    "def start_chatbot():\n",
    "    \"\"\"Initialize and start the chatbot.\"\"\"\n",
    "    print(\"Initializing Real Estate Chatbot...\")\n",
    "    \n",
    "    # Initialize RAG system\n",
    "    rag = EnhancedRealEstateRAG()\n",
    "    rag.load_embeddings()\n",
    "    rag.build_indices()\n",
    "    \n",
    "    # Initialize chatbot\n",
    "    chatbot = RealEstateChatbot(rag)\n",
    "    \n",
    "    # Start interactive chat\n",
    "    chatbot.chat()\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='RealEstateLLM - AI-powered real estate chatbot')\n",
    "    \n",
    "    parser.add_argument('--process', action='store_true', \n",
    "                        help='Process all data sources')\n",
    "    parser.add_argument('--embeddings', action='store_true', \n",
    "                        help='Create embeddings for all data')\n",
    "    parser.add_argument('--finetune', action='store_true', \n",
    "                        help='Prepare data for fine-tuning')\n",
    "    parser.add_argument('--chat', action='store_true', \n",
    "                        help='Start the chatbot')\n",
    "    parser.add_argument('--force', action='store_true', \n",
    "                        help='Force reprocessing of existing files')\n",
    "    parser.add_argument('--qa-pairs', type=int, default=1000, \n",
    "                        help='Number of QA pairs to generate for fine-tuning')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # If no arguments, show help\n",
    "    if not any(vars(args).values()):\n",
    "        parser.print_help()\n",
    "        return\n",
    "    \n",
    "    # Process data if requested\n",
    "    if args.process:\n",
    "        process_all_data(force_reprocess=args.force)\n",
    "    \n",
    "    # Create embeddings if requested\n",
    "    if args.embeddings:\n",
    "        create_embeddings(force_recreate=args.force)\n",
    "    \n",
    "    # Prepare fine-tuning data if requested\n",
    "    if args.finetune:\n",
    "        prepare_fine_tuning_data(num_pairs=args.qa_pairs)\n",
    "    \n",
    "    # Start chatbot if requested\n",
    "    if args.chat:\n",
    "        start_chatbot()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
